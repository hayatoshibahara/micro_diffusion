{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add57ae6",
   "metadata": {},
   "source": [
    "# micro diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf5321",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/JourneyDB/JourneyDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ðŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ðŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ðŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ðŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ðŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    \"mosaicml[tensorboard, wandb]\" \\\n",
    "    accelerate \\\n",
    "    beautifulsoup4 \\\n",
    "    datasets \\\n",
    "    diffusers \\\n",
    "    easydict \\\n",
    "    einops \\\n",
    "    fastparquet \\\n",
    "    huggingface_hub \\\n",
    "    hydra-core \\\n",
    "    mosaicml-streaming \\\n",
    "    omegaconf \\\n",
    "    open_clip_torch \\\n",
    "    pandas \\\n",
    "    timm \\\n",
    "    torchmetrics \\\n",
    "    tqdm \\\n",
    "    transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from collections.abc import Iterable\n",
    "from composer.algorithms import GradientClipping\n",
    "from composer.algorithms.low_precision_layernorm import apply_low_precision_layernorm\n",
    "from composer.core import Precision\n",
    "from composer.utils import dist, reproducibility\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers.models.modeling_outputs import AutoencoderKLOutput\n",
    "from einops import rearrange\n",
    "from glob import glob\n",
    "from huggingface_hub import hf_hub_download\n",
    "from hydra import compose, initialize_config_dir\n",
    "from itertools import repeat\n",
    "from micro_diffusion.models.utils import text_encoder_embedding_format\n",
    "from multiprocessing import Pool\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from streaming import MDSWriter\n",
    "from streaming import Stream, StreamingDataset\n",
    "from streaming.base import MDSWriter\n",
    "from streaming.base.util import merge_index\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5Tokenizer\n",
    "from typing import Callable, Dict, List, Optional, Sequence, Union\n",
    "from typing import Optional, Tuple, Dict, Union, List, Any\n",
    "import hydra\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "# cuDNN(CUDA Deep Neural Network libraryï¼‰ã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–\n",
    "# 3-5%ã®é€Ÿåº¦å‘ä¸ŠãŒè¦‹è¾¼ã¾ã‚Œã‚‹\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "logger.info(f\"PyTorch {torch.__version__}\")\n",
    "logger.info(f\"Transformers {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9446431",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ROOT = os.path.expanduser(\"~\")\n",
    "CACHE_DIR = os.path.join(USER_ROOT, \".cache\", \"micro_diffusion\")\n",
    "\n",
    "DATA_DIR = os.path.join(CACHE_DIR, \"data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "logger.info(f\"{DATA_DIR=}\")\n",
    "\n",
    "MODEL_DIR = os.path.join(CACHE_DIR, \"models\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "logger.info(f\"{MODEL_DIR=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_metadata():\n",
    "    # Only using a single process for downloading metadata\n",
    "    metadata_files = [\n",
    "        ('data/train', 'train_anno.jsonl.tgz'),\n",
    "        ('data/train', 'train_anno_realease_repath.jsonl.tgz'),\n",
    "        ('data/valid', 'valid_anno_repath.jsonl.tgz'),\n",
    "        ('data/test', 'test_questions.jsonl.tgz'),\n",
    "        ('data/test', 'imgs.tgz'),\n",
    "    ]\n",
    "\n",
    "    for subfolder, filename in metadata_files:\n",
    "        hf_hub_download(\n",
    "            repo_id=\"JourneyDB/JourneyDB\",\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=subfolder,\n",
    "            filename=filename,\n",
    "            local_dir=COMPRESSED_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "\n",
    "    metadata_tars = [\n",
    "        os.path.join(dir, fname) for (dir, fname) in metadata_files\n",
    "    ]\n",
    "\n",
    "    for tar_file in metadata_tars:\n",
    "        subprocess.call(\n",
    "            f'tar -xvzf {os.path.join(COMPRESSED_DIR, tar_file)} '\n",
    "            f'-C {os.path.join(COMPRESSED_DIR, os.path.dirname(tar_file))}',\n",
    "            shell=True,\n",
    "        )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/train/train_anno_realease_repath.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"train/train_anno_realease_repath.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/valid/valid_anno_repath.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"valid/valid_anno_repath.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/test/test_questions.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"test/test_questions.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.move(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/test/imgs\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"test/\")}',\n",
    "    )\n",
    "\n",
    "COMPRESSED_DIR = os.path.join(DATA_DIR, 'compressed')\n",
    "logger.info(f\"{COMPRESSED_DIR=}\")\n",
    "\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "logger.info(f\"{RAW_DIR=}\")\n",
    "\n",
    "if not os.path.exists(COMPRESSED_DIR):\n",
    "    os.makedirs(COMPRESSED_DIR, exist_ok=True)\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "    download_and_process_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df247d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_uncompress_resize(\n",
    "    valid_ids: list,\n",
    "    max_image_size: int,\n",
    "    min_image_size: int,\n",
    "    split: str,\n",
    "    idx: int,\n",
    "):\n",
    "    \"\"\"Download, uncompress, and resize images for a given archive index.\"\"\"\n",
    "    assert split in ('train', 'valid')\n",
    "    assert idx in valid_ids\n",
    "\n",
    "    print(f\"Downloading idx: {idx}\")\n",
    "    if not os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/'):\n",
    "        hf_hub_download(\n",
    "            repo_id=\"JourneyDB/JourneyDB\",\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=f'data/{split}/imgs',\n",
    "            filename=f'{idx:>03}.tgz',\n",
    "            local_dir=COMPRESSED_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "    print(f\"Downloaded idx: {idx}\")\n",
    "\n",
    "    print(f\"Extracting idx: {idx}\")\n",
    "    if not os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/'):\n",
    "        subprocess.call(\n",
    "            f'tar -xzf {COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz '\n",
    "            f'-C {COMPRESSED_DIR}/data/{split}/imgs/',\n",
    "            shell=True,\n",
    "        )\n",
    "    print(f\"Extracted idx: {idx}\")\n",
    "\n",
    "    print(f\"Removing idx: {idx}\")\n",
    "    if os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz'):\n",
    "        os.remove(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz')\n",
    "    print(f\"Removed idx: {idx}\")\n",
    "\n",
    "    # add bicubic downsize\n",
    "    downsize = transforms.Resize(\n",
    "        max_image_size,\n",
    "        antialias=True,\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    )\n",
    "\n",
    "    print(f\"Downsizing idx: {idx}\")\n",
    "    os.makedirs(\n",
    "        f'{RAW_DIR}/{split}/imgs/{idx:>03}/',\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    for f in iglob(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/*'):\n",
    "        save_path = f'{RAW_DIR}/{split}/imgs/{idx:>03}/{os.path.basename(f)}'\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "            try:\n",
    "                img = Image.open(f)\n",
    "                w, h = img.size\n",
    "                if min(w, h) > max_image_size:\n",
    "                    img = downsize(img)\n",
    "                if min(w, h) < min_image_size:\n",
    "                    print(\n",
    "                        f'Skipping image with resolution ({h}, {w}) - '\n",
    "                        f'Since at least one side has resolution below {min_image_size}'\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                img.save(save_path)\n",
    "                os.remove(f)\n",
    "            except (UnidentifiedImageError, OSError) as e:\n",
    "                print(f\"Error {e}, File: {f}\")\n",
    "    print(f'Downsized idx: {idx}')\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(RAW_DIR, 'train/imgs')):\n",
    "\n",
    "    valid_ids = list(np.arange(1))\n",
    "    pool_args = [('train', i) for i in valid_ids] + [('valid', i) for i in valid_ids]\n",
    "    max_image_size = 512\n",
    "    min_image_size = 64\n",
    "\n",
    "    with Pool(processes=4) as pool:\n",
    "        pool.starmap(\n",
    "            download_uncompress_resize,\n",
    "            [(\n",
    "                valid_ids,\n",
    "                max_image_size,\n",
    "                min_image_size,\n",
    "                split,\n",
    "                idx\n",
    "            ) for split, idx in pool_args]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221dffcd",
   "metadata": {},
   "source": [
    "## Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50319bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mds(\n",
    "    images_dir: str,\n",
    "    captions_jsonl: str,\n",
    "    local_mds_dir: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    JourneyDBã‚’MDSå½¢å¼ã«å¤‰æ›ã™ã‚‹\n",
    "    MDSï¼ˆMosaic Data Storageï¼‰ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®é«˜é€Ÿã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ\n",
    "\n",
    "    Args:\n",
    "        images_dir: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "        captions_jsonl: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "        local_mds_dir: å‡ºåŠ›MDSãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    columns = {\n",
    "        'width': 'int32',\n",
    "        'height': 'int32',\n",
    "        'jpg': 'jpeg',\n",
    "        'caption': 'str',\n",
    "    }\n",
    "    \n",
    "    writer = MDSWriter(\n",
    "        out=local_mds_dir,\n",
    "        columns=columns,\n",
    "        compression=None,\n",
    "        size_limit=256 * (2**20),\n",
    "        max_workers=64,\n",
    "    )\n",
    "    \n",
    "    # Retrieving achieve indies, in case only a subset of the data is downloaded\n",
    "    valid_archieve_idx = [\n",
    "        os.path.basename(p) for p in glob(os.path.join(images_dir, '*'))\n",
    "    ]\n",
    "    \n",
    "    metadata = list(open(captions_jsonl, 'r'))\n",
    "\n",
    "    for f in tqdm(metadata):\n",
    "        d = json.loads(f)\n",
    "        cap, p = d['prompt'], d['img_path'].strip('./')\n",
    "        \n",
    "        if os.path.dirname(p) not in valid_archieve_idx:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            img = Image.open(os.path.join(images_dir, p))\n",
    "            w, h = img.size\n",
    "            mds_sample = {\n",
    "                'jpg': img,\n",
    "                'caption': cap,\n",
    "                'width': w,\n",
    "                'height': h,\n",
    "            }\n",
    "            writer.write(mds_sample)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Something went wrong in reading caption, \"\n",
    "                f\"skipping writing this sample in mds. Error: {e}\"\n",
    "            )\n",
    "\n",
    "    writer.finish()\n",
    "\n",
    "MDS_DIR = os.path.join(DATA_DIR, 'jdb', 'mds')\n",
    "logger.info(f\"{MDS_DIR=}\")\n",
    "\n",
    "if not os.path.exists(MDS_DIR):\n",
    "    convert_to_mds(\n",
    "        images_dir=os.path.join(RAW_DIR, 'train', 'imgs'),\n",
    "        captions_jsonl=os.path.join(RAW_DIR, 'train', 'train_anno_realease_repath.jsonl'),\n",
    "        local_mds_dir=os.path.join(MDS_DIR, 'train'),\n",
    "    )\n",
    "\n",
    "    convert_to_mds(\n",
    "        images_dir=os.path.join(RAW_DIR, 'valid', 'imgs'),\n",
    "        captions_jsonl=os.path.join(RAW_DIR, 'valid', 'valid_anno_repath.jsonl'),\n",
    "        local_mds_dir=os.path.join(MDS_DIR, 'valid'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c74de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd micro_diffusion/micro_diffusion/datasets/prepare/jdb && \\\n",
    "    # accelerate launch --num_processes 8 precompute.py --datadir $DATADIR/jdb/mds/train/ --savedir $DATADIR/jdb/mds_latents_sdxl1_dfnclipH14/train/ --vae stabilityai/stable-diffusion-xl-base-1.0 --text_encoder openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378 --batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912aa78",
   "metadata": {},
   "source": [
    "### Precompute Latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_2_hf_tokenizer_wrapper:\n",
    "    \"\"\"Simple wrapper to make OpenCLIP tokenizer match HuggingFace interface.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (Any): OpenCLIP tokenizer instance\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: Any):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_max_length = self.tokenizer.context_length\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        caption: str,\n",
    "        padding: str = 'max_length',\n",
    "        max_length: Optional[int] = None,\n",
    "        truncation: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        return {'input_ids': self.tokenizer(caption, context_length=max_length)}\n",
    "\n",
    "\n",
    "class UniversalTokenizer:\n",
    "    \"\"\"Universal tokenizer supporting multiple model types.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Name/path of the tokenizer to load\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        s, d = text_encoder_embedding_format(name)\n",
    "        if self.name.startswith(\"openclip:\"):\n",
    "            self.tokenizer = simple_2_hf_tokenizer_wrapper(\n",
    "                open_clip.get_tokenizer(name.lstrip('openclip:'))\n",
    "            )\n",
    "            assert s == self.tokenizer.model_max_length, \"simply check of text_encoder_embedding_format\"\n",
    "        elif self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(name) # for t5 we would use a smaller than max_seq_length\n",
    "        else:\n",
    "            self.tokenizer = CLIPTokenizer.from_pretrained(name, subfolder='tokenizer')\n",
    "            assert s == self.tokenizer.model_max_length, \"simply check of text_encoder_embedding_format\"\n",
    "        self.model_max_length = s\n",
    "        \n",
    "    def tokenize(self, captions: Union[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
    "        if self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            text_tokens_and_mask = self.tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                max_length=self.model_max_length,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': text_tokens_and_mask['input_ids'],\n",
    "                'attention_mask': text_tokens_and_mask['attention_mask']\n",
    "            }\n",
    "        else:\n",
    "            # Avoid attention mask for CLIP tokenizers as they are not used\n",
    "            tokenized_caption = self.tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )['input_ids']\n",
    "            return {'input_ids': tokenized_caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingJdbDatasetForPreCompute(StreamingDataset):\n",
    "    \"\"\"Streaming dataset that resizes images to user-provided resolutions and tokenizes captions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        streams: Sequence[Stream],\n",
    "        transforms_list: List[Callable],\n",
    "        batch_size: int,\n",
    "        tokenizer_name: str,\n",
    "        shuffle: bool = False,\n",
    "        caption_key: str = 'caption',\n",
    "    ):\n",
    "        super().__init__(\n",
    "            streams=streams,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        self.transforms_list = transforms_list\n",
    "        self.caption_key = caption_key\n",
    "        self.tokenizer = UniversalTokenizer(tokenizer_name)\n",
    "        print(\"Created tokenizer: \", tokenizer_name)\n",
    "        assert self.transforms_list is not None, 'Must provide transforms to resize and center crop images'\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict:\n",
    "        sample = super().__getitem__(index)\n",
    "        ret = {}\n",
    "\n",
    "        out = self.tokenizer.tokenize(sample[self.caption_key])\n",
    "        ret[self.caption_key] = out['input_ids'].clone().detach()\n",
    "        if 'attention_mask' in out:\n",
    "            ret[f'{self.caption_key}_attention_mask'] = out['attention_mask'].clone().detach()\n",
    "\n",
    "        for i, tr in enumerate(self.transforms_list):\n",
    "            img = sample['jpg']\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img = tr(img)\n",
    "            ret[f'image_{i}'] = img\n",
    "\n",
    "        ret['sample'] = sample\n",
    "        return ret\n",
    "\n",
    "\n",
    "def build_streaming_jdb_precompute_dataloader(\n",
    "    datadir: Union[List[str], str],\n",
    "    batch_size: int,\n",
    "    resize_sizes: Optional[List[int]] = None,\n",
    "    drop_last: bool = False,\n",
    "    shuffle: bool = True,\n",
    "    caption_key: Optional[str] = None,\n",
    "    tokenizer_name: Optional[str] = None,\n",
    "    **dataloader_kwargs,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Builds a streaming mds dataloader returning multiple image sizes and text captions.\"\"\"\n",
    "    assert resize_sizes is not None, 'Must provide target resolution for image resizing'\n",
    "    datadir = [datadir] if isinstance(datadir, str) else datadir\n",
    "    streams = [Stream(remote=None, local=l) for l in datadir]\n",
    "\n",
    "    transforms_list = []\n",
    "    for resize in resize_sizes:\n",
    "        transforms_list.append(\n",
    "            transforms.Compose([\n",
    "                transforms.Resize(\n",
    "                    resize,\n",
    "                    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "                ),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    dataset = StreamingJdbDatasetForPreCompute(\n",
    "        streams=streams,\n",
    "        shuffle=shuffle,\n",
    "        transforms_list=transforms_list,\n",
    "        batch_size=batch_size,\n",
    "        caption_key=caption_key,\n",
    "        tokenizer_name=tokenizer_name,\n",
    "    )\n",
    "\n",
    "    def custom_collate(list_of_dict: List[Dict]) -> Dict:\n",
    "        out = {k: [] for k in list_of_dict[0].keys()}\n",
    "        for d in list_of_dict:\n",
    "            for k, v in d.items():\n",
    "                out[k].append(v)\n",
    "        return out\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=custom_collate,\n",
    "        **dataloader_kwargs,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder_embedding_format(enc: str) -> Tuple[int, int]:\n",
    "    \"\"\"Returns sequence length and token embedding dimension for text encoder.\"\"\"\n",
    "    if enc in [\n",
    "        'stabilityai/stable-diffusion-2-base',\n",
    "        'runwayml/stable-diffusion-v1-5',\n",
    "        'CompVis/stable-diffusion-v1-4'\n",
    "    ]:\n",
    "        return 77, 1024\n",
    "    if enc in ['openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378']:\n",
    "        return 77, 1024\n",
    "    if enc in [\"DeepFloyd/t5-v1_1-xxl\"]:\n",
    "        return 120, 4096\n",
    "    raise ValueError(f'Please specifcy the sequence and embedding size of {enc} encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalTextEncoder(torch.nn.Module):\n",
    "    \"\"\"Universal text encoder supporting multiple model types.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Name/path of the model to load\n",
    "        dtype (str): Data type for model weights\n",
    "        pretrained (bool, True): Whether to load pretrained weights\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, dtype: str, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        if self.name.startswith(\"openclip:\"):\n",
    "            assert pretrained, 'Load default pretrained model from openclip'\n",
    "            self.encoder = openclip_text_encoder(\n",
    "                open_clip.create_model_and_transforms(name.lstrip('openclip:'))[0],\n",
    "                torch_dtype=DATA_TYPES[dtype]\n",
    "            )\n",
    "        elif self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            self.encoder = T5EncoderModel.from_pretrained(\n",
    "                name,\n",
    "                torch_dtype=DATA_TYPES[dtype],\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = CLIPTextModel.from_pretrained(\n",
    "                name,\n",
    "                subfolder='text_encoder',\n",
    "                torch_dtype=DATA_TYPES[dtype],\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "\n",
    "    def encode(self, tokenized_caption: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        if self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            out = self.encoder(\n",
    "                tokenized_caption,\n",
    "                attention_mask=attention_mask\n",
    "            )['last_hidden_state']\n",
    "            out = out.unsqueeze(dim=1)\n",
    "            return out, None\n",
    "        else:\n",
    "            return self.encoder(tokenized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = {\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16,\n",
    "    'float32': torch.float32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "class openclip_text_encoder(torch.nn.Module):\n",
    "    \"\"\"OpenCLIP text encoder wrapper.\n",
    "    \n",
    "    Args:\n",
    "        clip_model (Any): OpenCLIP model instance\n",
    "        dtype (torch.dtype, torch.float32): Data type for model weights\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model: Any, dtype: torch.dtype = torch.float32, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.device = None\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward_fxn(self, text: torch.Tensor) -> Tuple[torch.Tensor, None]:\n",
    "        cast_dtype = self.clip_model.transformer.get_cast_dtype()\n",
    "        x = self.clip_model.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + self.clip_model.positional_embedding.to(cast_dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒžã‚¹ã‚¯ã‚’å¤–ã™\n",
    "        # x = self.clip_model.transformer(x, attn_mask=self.clip_model.attn_mask)\n",
    "        x = self.clip_model.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip_model.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        x = x.unsqueeze(dim=1) # [batch_size, 1, n_ctx, transformer.width] expected for text_emb\n",
    "        return x, None # HF encoders expected to return multiple values with first being text emb\n",
    "\n",
    "    def forward(self, text: torch.Tensor, **kwargs) -> Tuple[torch.Tensor, None]:\n",
    "        with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "            return self.forward_fxn(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute(\n",
    "    datadir: str,\n",
    "    savedir: str = \"\",\n",
    "    image_resolutions: list = [256, 512],\n",
    "    save_images: bool = False,\n",
    "    model_dtype: str = \"bfloat16\",\n",
    "    save_dtype: str = \"float16\",\n",
    "    vae: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    text_encoder: str = \"openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\",\n",
    "    batch_size: int = 32,\n",
    "    seed: int = 2024,\n",
    "):\n",
    "    \"\"\"Precompute image and text latents and store them in MDS format.\n",
    "\n",
    "    By default, we only save the image latents for 256x256 and 512x512 image\n",
    "    resolutions (using center crop).\n",
    "\n",
    "    Note that the image latents will be scaled by the vae_scaling_factor.\n",
    "    \"\"\"\n",
    "    cap_key = 'caption'  # Hardcoding the image caption key to 'caption' in MDS dataset\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    device_idx = int(accelerator.process_index)\n",
    "\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(device_idx + seed)\n",
    "    torch.cuda.manual_seed(device_idx + seed)\n",
    "    np.random.seed(device_idx + seed)\n",
    "\n",
    "    dataloader = build_streaming_jdb_precompute_dataloader(\n",
    "        datadir=[datadir],\n",
    "        batch_size=batch_size,\n",
    "        resize_sizes=image_resolutions,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        caption_key=cap_key,\n",
    "        tokenizer_name=text_encoder,\n",
    "        # prefetch_factor=2,\n",
    "        # num_workers=2,\n",
    "        # persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f'Device: {device_idx}, Dataloader sample count: {len(dataloader.dataset)}')\n",
    "\n",
    "    # print(\n",
    "    #     f\"MP variable -> world size: {os.environ['WORLD_SIZE']}, \"\n",
    "    #     f\"RANK: {os.environ['RANK']}, {device}\"\n",
    "    # )\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        vae,\n",
    "        subfolder='vae',  # Change subfolder to appropriate one in hf_hub, if needed\n",
    "        torch_dtype=DATA_TYPES[model_dtype],\n",
    "    )\n",
    "    print(\"Created VAE: \", vae)\n",
    "    assert isinstance(vae, AutoencoderKL)\n",
    "\n",
    "    text_encoder = UniversalTextEncoder(\n",
    "        text_encoder,\n",
    "        dtype=model_dtype,\n",
    "        pretrained=True,\n",
    "    )\n",
    "    print(\"Created text encoder: \", text_encoder)\n",
    "\n",
    "    vae = vae.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "    columns = {\n",
    "        cap_key: 'str',\n",
    "        f'{cap_key}_latents': 'bytes',\n",
    "        'latents_256': 'bytes',\n",
    "        'latents_512': 'bytes',\n",
    "    }\n",
    "    if save_images:\n",
    "        columns['jpg'] = 'jpeg'\n",
    "\n",
    "    remote_upload = os.path.join(savedir, str(accelerator.process_index))\n",
    "\n",
    "    writer = MDSWriter(\n",
    "        out=remote_upload,\n",
    "        columns=columns,\n",
    "        compression=None,\n",
    "        size_limit=256 * (2**20),\n",
    "        max_workers=64,\n",
    "    )\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        image_256 = torch.stack(batch['image_0']).to(device)\n",
    "        image_512 = torch.stack(batch['image_1']).to(device)\n",
    "        captions = torch.stack(batch[cap_key]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type='cuda', dtype=DATA_TYPES[model_dtype]):\n",
    "                latent_dist_256 = vae.encode(image_256)\n",
    "                assert isinstance(latent_dist_256, AutoencoderKLOutput)\n",
    "                latents_256 = (\n",
    "                    latent_dist_256['latent_dist'].sample().data * vae.config.scaling_factor\n",
    "                ).to(DATA_TYPES[save_dtype])\n",
    "\n",
    "                latent_dist_512 = vae.encode(image_512)\n",
    "                assert isinstance(latent_dist_512, AutoencoderKLOutput)\n",
    "                latents_512 = (\n",
    "                    latent_dist_512['latent_dist'].sample().data * vae.config.scaling_factor\n",
    "                ).to(DATA_TYPES[save_dtype])\n",
    "\n",
    "                attention_mask = None\n",
    "\n",
    "                if f'{cap_key}_attention_mask' in batch:\n",
    "                    attention_mask = torch.stack(\n",
    "                        batch[f'{cap_key}_attention_mask']\n",
    "                    ).to(device)\n",
    "\n",
    "                conditioning = text_encoder.encode(\n",
    "                    captions.view(-1, captions.shape[-1]),\n",
    "                    attention_mask=attention_mask,\n",
    "                )[0].to(DATA_TYPES[save_dtype])\n",
    "\n",
    "        try:\n",
    "            if isinstance(latents_256, torch.Tensor) and isinstance(\n",
    "                latents_512, torch.Tensor\n",
    "            ):\n",
    "                latents_256 = latents_256.detach().cpu().numpy()\n",
    "                latents_512 = latents_512.detach().cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if isinstance(conditioning, torch.Tensor):\n",
    "                conditioning = conditioning.detach().cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Write the batch to the MDS file\n",
    "            for i in range(latents_256.shape[0]):\n",
    "                mds_sample = {\n",
    "                    cap_key: batch['sample'][i][cap_key],\n",
    "                    f'{cap_key}_latents': np.reshape(conditioning[i], -1).tobytes(),\n",
    "                    'latents_256': latents_256[i].tobytes(),\n",
    "                    'latents_512': latents_512[i].tobytes(),\n",
    "                }\n",
    "                if save_images:\n",
    "                    mds_sample['jpg'] = batch['sample'][i]['jpg']\n",
    "                writer.write(mds_sample)\n",
    "        except RuntimeError:\n",
    "            print('Runtime error CUDA, skipping this batch')\n",
    "\n",
    "    writer.finish()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    accelerator.wait_for_everyone()\n",
    "    print(f'Process {accelerator.process_index} finished')\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Merge the mds shards created by each device (only do on main process)\n",
    "    if accelerator.is_main_process:\n",
    "        shards_metadata = [\n",
    "            os.path.join(savedir, str(i), 'index.json')\n",
    "            for i in range(accelerator.num_processes)\n",
    "        ]\n",
    "        merge_index(shards_metadata, out=savedir, keep_local=True)\n",
    "\n",
    "PRECOMPUTE_DIR = os.path.join(DATA_DIR, 'mds_latents_sdxl1_dfnclipH14',)\n",
    "logger.info(f\"{PRECOMPUTE_DIR=}\")\n",
    "\n",
    "if not os.path.exists(PRECOMPUTE_DIR):\n",
    "    precompute(\n",
    "        datadir=os.path.join(DATA_DIR, 'jdb', 'mds', 'train'),\n",
    "        savedir=os.path.join(PRECOMPUTE_DIR, 'train'),\n",
    "        image_resolutions=[256, 512],\n",
    "        save_images=False,\n",
    "        model_dtype=\"bfloat16\",\n",
    "        save_dtype=\"float16\",\n",
    "        vae=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        text_encoder=\"openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\",\n",
    "        batch_size=16,\n",
    "        seed=2024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f75d5",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg: DictConfig) -> None:\n",
    "    \"\"\"Train a micro-diffusion model using the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): Configuration object loaded from yaml file.\n",
    "    \"\"\"\n",
    "    if not cfg:\n",
    "        raise ValueError('Config not specified. Please provide --config-path and --config-name, respectively.')\n",
    "    reproducibility.seed_all(cfg['seed'])\n",
    "\n",
    "    assert cfg.model.precomputed_latents, \"For microbudget training, we assume that latents are already precomputed for all datasets\"\n",
    "    model = hydra.utils.instantiate(cfg.model)\n",
    "\n",
    "    # Set up optimizer with special handling for MoE parameters\n",
    "    moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "    rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "    if len(moe_params) > 0:\n",
    "        print('Reducing learning rate of MoE parameters by 1/2')\n",
    "        opt_dict = dict(cfg.optimizer)\n",
    "        opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "        del opt_dict['_target_']\n",
    "        optimizer = getattr(torch.optim, opt_name)(\n",
    "            params=[{'params': rest_params}, {'params': moe_params, 'lr': cfg.optimizer.lr / 2}], **opt_dict)\n",
    "    else:\n",
    "        optimizer = hydra.utils.instantiate(cfg.optimizer, params=model.dit.parameters())\n",
    "\n",
    "    # Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "    for p in optimizer.param_groups:\n",
    "        p['betas'] = list(p['betas'])\n",
    "\n",
    "    # Set up data loaders\n",
    "    cap_seq_size, cap_emb_dim = text_encoder_embedding_format(cfg.model.text_encoder_name)\n",
    "    train_loader = hydra.utils.instantiate(\n",
    "        cfg.dataset.train,\n",
    "        image_size=cfg.dataset.image_size,\n",
    "        batch_size=cfg.dataset.train_batch_size // dist.get_world_size(),\n",
    "        cap_seq_size=cap_seq_size,\n",
    "        cap_emb_dim=cap_emb_dim,\n",
    "        cap_drop_prob=cfg.dataset.cap_drop_prob)\n",
    "    print(f\"Found {len(train_loader.dataset)*dist.get_world_size()} images in the training dataset\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    eval_loader = hydra.utils.instantiate(\n",
    "        cfg.dataset.eval,\n",
    "        image_size=cfg.dataset.image_size,\n",
    "        batch_size=cfg.dataset.eval_batch_size // dist.get_world_size(),\n",
    "        cap_seq_size=cap_seq_size,\n",
    "        cap_emb_dim=cap_emb_dim)\n",
    "    print(f\"Found {len(eval_loader.dataset)*dist.get_world_size()} images in the eval dataset\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Initialize training components\n",
    "    logger, callbacks, algorithms = [], [], []\n",
    "\n",
    "    # Set up loggers\n",
    "    for log, log_conf in cfg.logger.items():\n",
    "        if '_target_' in log_conf:\n",
    "            if log == 'wandb':\n",
    "                wandb_logger = hydra.utils.instantiate(log_conf, _partial_=True)\n",
    "                logger.append(wandb_logger(init_kwargs={'config': OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)}))\n",
    "            else:\n",
    "                logger.append(hydra.utils.instantiate(log_conf))\n",
    "\n",
    "    # Configure algorithms\n",
    "    if 'algorithms' in cfg:\n",
    "        for alg_name, alg_conf in cfg.algorithms.items():\n",
    "            if alg_name == 'low_precision_layernorm':\n",
    "                apply_low_precision_layernorm(model=model.dit,\n",
    "                                              precision=Precision(alg_conf['precision']),\n",
    "                                              optimizers=optimizer)\n",
    "            elif alg_name == 'gradient_clipping':\n",
    "                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=alg_conf['clip_norm']))\n",
    "            else:\n",
    "                print(f'Algorithm {alg_name} not supported.')\n",
    "\n",
    "    # Set up callbacks\n",
    "    if 'callbacks' in cfg:\n",
    "        for _, call_conf in cfg.callbacks.items():\n",
    "            if '_target_' in call_conf:\n",
    "                print(f'Instantiating callbacks: {call_conf._target_}')\n",
    "                callbacks.append(hydra.utils.instantiate(call_conf))\n",
    "\n",
    "    scheduler = hydra.utils.instantiate(cfg.scheduler)\n",
    "\n",
    "    # disable online evals if using torch.compile\n",
    "    if cfg.misc.compile:\n",
    "        cfg.trainer.eval_interval = 0\n",
    "        \n",
    "    trainer = hydra.utils.instantiate(\n",
    "        cfg.trainer,\n",
    "        train_dataloader=train_loader,\n",
    "        eval_dataloader=eval_loader,\n",
    "        optimizers=optimizer,\n",
    "        model=model,\n",
    "        loggers=logger,\n",
    "        algorithms=algorithms,\n",
    "        schedulers=scheduler,\n",
    "        callbacks=callbacks,\n",
    "        precision='amp_bf16' if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "        python_log_level='debug',\n",
    "        compile_config={} if cfg.misc.compile else None  # it enables torch.compile (~15% speedup)\n",
    "    )\n",
    "\n",
    "    # Ensure models are on correct device\n",
    "    device = next(model.dit.parameters()).device\n",
    "    model.vae.to(device)\n",
    "    model.text_encoder.to(device)\n",
    "\n",
    "    return trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = os.path.abspath(\"micro_diffusion/configs\")\n",
    "\n",
    "with initialize_config_dir(config_dir=config_dir):\n",
    "    cfg = compose(\n",
    "        config_name=\"res_256_pretrain_test.yaml\",\n",
    "        overrides=[\n",
    "            \"exp_name=MicroDiTXL_mask_75_res_512_pretrain\",\n",
    "            \"model.train_mask_ratio=0.75\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
