{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add57ae6",
   "metadata": {},
   "source": [
    "# micro diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf5321",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/JourneyDB/JourneyDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi # CUDA Version 13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version # 3.12.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \\\n",
    "    accelerate \\\n",
    "    diffusers \\\n",
    "    huggingface_hub \\\n",
    "    torch==2.7.0 \\\n",
    "    torchvision \\\n",
    "    transformers \\\n",
    "    timm \\\n",
    "    open_clip_torch \\\n",
    "    easydict \\\n",
    "    einops \\\n",
    "    mosaicml-streaming \\\n",
    "    torchmetrics \\\n",
    "    tqdm \\\n",
    "    pandas \\\n",
    "    fastparquet \\\n",
    "    omegaconf \\\n",
    "    datasets \\\n",
    "    hydra-core \\\n",
    "    beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"mosaicml[tensorboard, wandb]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9446431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# パスを指定\n",
    "USER_ROOT = os.path.expanduser(\"~\")\n",
    "CACHE_DIR = os.path.join(USER_ROOT, \".cache\", \"micro_diffusion\")\n",
    "DATA_DIR = os.path.join(CACHE_DIR, \"data\")\n",
    "MODEL_DIR = os.path.join(CACHE_DIR, \"models\")\n",
    "\n",
    "COMPRESSED_DIR = os.path.join(DATA_DIR, 'compressed')\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "\n",
    "TRAIN_IMGS_DIR = os.path.join(RAW_DIR, 'train', 'imgs')\n",
    "VALID_IMGS_DIR = os.path.join(RAW_DIR, 'valid', 'imgs')\n",
    "TEST_DIR = os.path.join(RAW_DIR, 'test')\n",
    "\n",
    "# ディレクトリ作成\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(COMPRESSED_DIR, exist_ok=True)\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_IMGS_DIR, exist_ok=True)\n",
    "os.makedirs(VALID_IMGS_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from glob import iglob\n",
    "from multiprocessing import Pool\n",
    "from torchvision import transforms\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image, UnidentifiedImageError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_metadata():\n",
    "    # Only using a single process for downloading metadata\n",
    "    metadata_files = [\n",
    "        ('data/train', 'train_anno.jsonl.tgz'),\n",
    "        ('data/train', 'train_anno_realease_repath.jsonl.tgz'),\n",
    "        ('data/valid', 'valid_anno_repath.jsonl.tgz'),\n",
    "        ('data/test', 'test_questions.jsonl.tgz'),\n",
    "        ('data/test', 'imgs.tgz'),\n",
    "    ]\n",
    "\n",
    "    for subfolder, filename in metadata_files:\n",
    "        hf_hub_download(\n",
    "            repo_id=\"JourneyDB/JourneyDB\",\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=subfolder,\n",
    "            filename=filename,\n",
    "            local_dir=COMPRESSED_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "\n",
    "    metadata_tars = [\n",
    "        os.path.join(dir, fname) for (dir, fname) in metadata_files\n",
    "    ]\n",
    "\n",
    "    for tar_file in metadata_tars:\n",
    "        subprocess.call(\n",
    "            f'tar -xvzf {os.path.join(COMPRESSED_DIR, tar_file)} '\n",
    "            f'-C {os.path.join(COMPRESSED_DIR, os.path.dirname(tar_file))}',\n",
    "            shell=True,\n",
    "        )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/train/train_anno_realease_repath.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"train/train_anno_realease_repath.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/valid/valid_anno_repath.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"valid/valid_anno_repath.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/test/test_questions.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"test/test_questions.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.move(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/test/imgs\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"test/\")}',\n",
    "    )\n",
    "\n",
    "if not os.path.exists(os.path.join(RAW_DIR, 'train', 'train_anno_realease_repath.jsonl')):\n",
    "    download_and_process_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ids = list(np.arange(1))\n",
    "\n",
    "pool_args = [('train', i) for i in valid_ids] + [('valid', i) for i in valid_ids]\n",
    "pool_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_image_size = 512\n",
    "min_image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df247d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_uncompress_resize(\n",
    "    valid_ids: list,\n",
    "    max_image_size: int,\n",
    "    min_image_size: int,\n",
    "    split: str,\n",
    "    idx: int,\n",
    "):\n",
    "    \"\"\"Download, uncompress, and resize images for a given archive index.\"\"\"\n",
    "    assert split in ('train', 'valid')\n",
    "    assert idx in valid_ids\n",
    "\n",
    "    print(f\"Downloading idx: {idx}\")\n",
    "    if not os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/'):\n",
    "        hf_hub_download(\n",
    "            repo_id=\"JourneyDB/JourneyDB\",\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=f'data/{split}/imgs',\n",
    "            filename=f'{idx:>03}.tgz',\n",
    "            local_dir=COMPRESSED_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "    print(f\"Downloaded idx: {idx}\")\n",
    "\n",
    "    print(f\"Extracting idx: {idx}\")\n",
    "    if not os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/'):\n",
    "        subprocess.call(\n",
    "            f'tar -xzf {COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz '\n",
    "            f'-C {COMPRESSED_DIR}/data/{split}/imgs/',\n",
    "            shell=True,\n",
    "        )\n",
    "    print(f\"Extracted idx: {idx}\")\n",
    "\n",
    "    print(f\"Removing idx: {idx}\")\n",
    "    if os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz'):\n",
    "        os.remove(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz')\n",
    "    print(f\"Removed idx: {idx}\")\n",
    "\n",
    "    # add bicubic downsize\n",
    "    downsize = transforms.Resize(\n",
    "        max_image_size,\n",
    "        antialias=True,\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    )\n",
    "\n",
    "    print(f\"Downsizing idx: {idx}\")\n",
    "    os.makedirs(\n",
    "        f'{RAW_DIR}/{split}/imgs/{idx:>03}/',\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    for f in iglob(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/*'):\n",
    "        save_path = f'{RAW_DIR}/{split}/imgs/{idx:>03}/{os.path.basename(f)}'\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "            try:\n",
    "                img = Image.open(f)\n",
    "                w, h = img.size\n",
    "                if min(w, h) > max_image_size:\n",
    "                    img = downsize(img)\n",
    "                if min(w, h) < min_image_size:\n",
    "                    print(\n",
    "                        f'Skipping image with resolution ({h}, {w}) - '\n",
    "                        f'Since at least one side has resolution below {min_image_size}'\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                img.save(save_path)\n",
    "                os.remove(f)\n",
    "            except (UnidentifiedImageError, OSError) as e:\n",
    "                print(f\"Error {e}, File: {f}\")\n",
    "    print(f'Downsized idx: {idx}')\n",
    "\n",
    "\n",
    "num_proc = 4\n",
    "\n",
    "with Pool(processes=num_proc) as pool:\n",
    "    pool.starmap(\n",
    "        download_uncompress_resize,\n",
    "        [(valid_ids, max_image_size, min_image_size, split, idx) \\\n",
    "            for split, idx in pool_args])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d595ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd micro_diffusion/micro_diffusion/datasets/prepare/jdb && python download.py --datadir $DATA_DIR --max_image_size 512 --min_image_size 256 --valid_ids 0 1 --num_proc 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221dffcd",
   "metadata": {},
   "source": [
    "## Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from argparse import ArgumentParser\n",
    "from PIL import Image\n",
    "from streaming.base import MDSWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50319bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mds(\n",
    "    images_dir: str,\n",
    "    captions_jsonl: str,\n",
    "    local_mds_dir: str,\n",
    "):\n",
    "    \"\"\"Converts JourneyDB dataset to mds format.\"\"\"\n",
    "    columns = {\n",
    "        'width': 'int32',\n",
    "        'height': 'int32',\n",
    "        'jpg': 'jpeg',\n",
    "        'caption': 'str',\n",
    "    }\n",
    "    \n",
    "    writer = MDSWriter(\n",
    "        out=local_mds_dir,\n",
    "        columns=columns,\n",
    "        compression=None,\n",
    "        size_limit=256 * (2**20),\n",
    "        max_workers=64,\n",
    "    )\n",
    "    \n",
    "    # Retrieving achieve indies, in case only a subset of the data is downloaded\n",
    "    valid_archieve_idx = [\n",
    "        os.path.basename(p) for p in glob(os.path.join(images_dir, '*'))\n",
    "    ]\n",
    "    \n",
    "    metadata = list(open(captions_jsonl, 'r'))\n",
    "    for f in tqdm(metadata):\n",
    "        d = json.loads(f)\n",
    "        cap, p = d['prompt'], d['img_path'].strip('./')\n",
    "        \n",
    "        if os.path.dirname(p) not in valid_archieve_idx:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            img = Image.open(os.path.join(images_dir, p))\n",
    "            w, h = img.size\n",
    "            mds_sample = {\n",
    "                'jpg': img,\n",
    "                'caption': cap,\n",
    "                'width': w,\n",
    "                'height': h,\n",
    "            }\n",
    "            writer.write(mds_sample)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Something went wrong in reading caption, \"\n",
    "                f\"skipping writing this sample in mds. Error: {e}\"\n",
    "            )\n",
    "\n",
    "    writer.finish()\n",
    "\n",
    "\n",
    "# python convert.py --images_dir ./datadir/jdb/raw/train/imgs/ --captions_jsonl ./datadir/jdb/raw/train/train_anno_realease_repath.jsonl --local_mds_dir ./datadir/jdb/mds/train/\n",
    "# python convert.py --images_dir ./datadir/jdb/raw/valid/imgs/ --captions_jsonl ./datadir/jdb/raw/valid/valid_anno_repath.jsonl --local_mds_dir ./datadir/jdb/mds/valid/\n",
    "\n",
    "TRAIN_IMAGES_DIR = os.path.join(RAW_DIR, 'train', 'imgs')\n",
    "TRAIN_CAPTIONS_JSONL = os.path.join(RAW_DIR, 'train', 'train_anno_realease_repath.jsonl')\n",
    "TRAIN_LOCAL_MDS_DIR = os.path.join(DATA_DIR, 'mds', 'train')\n",
    "\n",
    "if not os.path.exists(TRAIN_LOCAL_MDS_DIR):\n",
    "    convert_to_mds(\n",
    "        images_dir=TRAIN_IMAGES_DIR,\n",
    "        captions_jsonl=TRAIN_CAPTIONS_JSONL,\n",
    "        local_mds_dir=TRAIN_LOCAL_MDS_DIR,\n",
    "    )\n",
    "\n",
    "VALID_IMAGES_DIR = os.path.join(RAW_DIR, 'valid', 'imgs')\n",
    "VALID_CAPTIONS_JSONL = os.path.join(RAW_DIR, 'valid', 'valid_anno_repath.jsonl')\n",
    "VALID_LOCAL_MDS_DIR = os.path.join(DATA_DIR, 'mds', 'valid')\n",
    "\n",
    "if not os.path.exists(VALID_LOCAL_MDS_DIR):\n",
    "    convert_to_mds(\n",
    "        images_dir=VALID_IMAGES_DIR,\n",
    "        captions_jsonl=VALID_CAPTIONS_JSONL,\n",
    "        local_mds_dir=VALID_LOCAL_MDS_DIR,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c74de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd micro_diffusion/micro_diffusion/datasets/prepare/jdb && \\\n",
    "    # accelerate launch --num_processes 8 precompute.py --datadir $DATADIR/jdb/mds/train/ --savedir $DATADIR/jdb/mds_latents_sdxl1_dfnclipH14/train/ --vae stabilityai/stable-diffusion-xl-base-1.0 --text_encoder openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378 --batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912aa78",
   "metadata": {},
   "source": [
    "### Precompute Latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers.models.modeling_outputs import AutoencoderKLOutput\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.util import merge_index\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Iterable\n",
    "from itertools import repeat\n",
    "from typing import Optional, Tuple, Dict, Union, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torchmetrics import Metric\n",
    "\n",
    "import open_clip\n",
    "from transformers import (\n",
    "    CLIPTextModel,\n",
    "    CLIPTokenizer,\n",
    "    T5EncoderModel, \n",
    "    T5Tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "class simple_2_hf_tokenizer_wrapper:\n",
    "    \"\"\"Simple wrapper to make OpenCLIP tokenizer match HuggingFace interface.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (Any): OpenCLIP tokenizer instance\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: Any):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_max_length = self.tokenizer.context_length\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        caption: str,\n",
    "        padding: str = 'max_length',\n",
    "        max_length: Optional[int] = None,\n",
    "        truncation: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        return {'input_ids': self.tokenizer(caption, context_length=max_length)}\n",
    "\n",
    "\n",
    "class UniversalTokenizer:\n",
    "    \"\"\"Universal tokenizer supporting multiple model types.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Name/path of the tokenizer to load\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        s, d = text_encoder_embedding_format(name)\n",
    "        if self.name.startswith(\"openclip:\"):\n",
    "            self.tokenizer = simple_2_hf_tokenizer_wrapper(\n",
    "                open_clip.get_tokenizer(name.lstrip('openclip:'))\n",
    "            )\n",
    "            assert s == self.tokenizer.model_max_length, \"simply check of text_encoder_embedding_format\"\n",
    "        elif self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(name) # for t5 we would use a smaller than max_seq_length\n",
    "        else:\n",
    "            self.tokenizer = CLIPTokenizer.from_pretrained(name, subfolder='tokenizer')\n",
    "            assert s == self.tokenizer.model_max_length, \"simply check of text_encoder_embedding_format\"\n",
    "        self.model_max_length = s\n",
    "        \n",
    "    def tokenize(self, captions: Union[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
    "        if self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            text_tokens_and_mask = self.tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                max_length=self.model_max_length,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': text_tokens_and_mask['input_ids'],\n",
    "                'attention_mask': text_tokens_and_mask['attention_mask']\n",
    "            }\n",
    "        else:\n",
    "            # Avoid attention mask for CLIP tokenizers as they are not used\n",
    "            tokenized_caption = self.tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )['input_ids']\n",
    "            return {'input_ids': tokenized_caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Sequence, Union\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from streaming import Stream, StreamingDataset\n",
    "\n",
    "# from micro_diffusion.models.utils import UniversalTokenizer\n",
    "\n",
    "class StreamingJdbDatasetForPreCompute(StreamingDataset):\n",
    "    \"\"\"Streaming dataset that resizes images to user-provided resolutions and tokenizes captions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        streams: Sequence[Stream],\n",
    "        transforms_list: List[Callable],\n",
    "        batch_size: int,\n",
    "        tokenizer_name: str,\n",
    "        shuffle: bool = False,\n",
    "        caption_key: str = 'caption',\n",
    "    ):\n",
    "        super().__init__(\n",
    "            streams=streams,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        self.transforms_list = transforms_list\n",
    "        self.caption_key = caption_key\n",
    "        self.tokenizer = UniversalTokenizer(tokenizer_name)\n",
    "        print(\"Created tokenizer: \", tokenizer_name)\n",
    "        assert self.transforms_list is not None, 'Must provide transforms to resize and center crop images'\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict:\n",
    "        sample = super().__getitem__(index)\n",
    "        ret = {}\n",
    "\n",
    "        out = self.tokenizer.tokenize(sample[self.caption_key])\n",
    "        ret[self.caption_key] = out['input_ids'].clone().detach()\n",
    "        if 'attention_mask' in out:\n",
    "            ret[f'{self.caption_key}_attention_mask'] = out['attention_mask'].clone().detach()\n",
    "\n",
    "        for i, tr in enumerate(self.transforms_list):\n",
    "            img = sample['jpg']\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img = tr(img)\n",
    "            ret[f'image_{i}'] = img\n",
    "\n",
    "        ret['sample'] = sample\n",
    "        return ret\n",
    "\n",
    "\n",
    "def build_streaming_jdb_precompute_dataloader(\n",
    "    datadir: Union[List[str], str],\n",
    "    batch_size: int,\n",
    "    resize_sizes: Optional[List[int]] = None,\n",
    "    drop_last: bool = False,\n",
    "    shuffle: bool = True,\n",
    "    caption_key: Optional[str] = None,\n",
    "    tokenizer_name: Optional[str] = None,\n",
    "    **dataloader_kwargs,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Builds a streaming mds dataloader returning multiple image sizes and text captions.\"\"\"\n",
    "    assert resize_sizes is not None, 'Must provide target resolution for image resizing'\n",
    "    datadir = [datadir] if isinstance(datadir, str) else datadir\n",
    "    streams = [Stream(remote=None, local=l) for l in datadir]\n",
    "\n",
    "    transforms_list = []\n",
    "    for resize in resize_sizes:\n",
    "        transforms_list.append(\n",
    "            transforms.Compose([\n",
    "                transforms.Resize(\n",
    "                    resize,\n",
    "                    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "                ),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    dataset = StreamingJdbDatasetForPreCompute(\n",
    "        streams=streams,\n",
    "        shuffle=shuffle,\n",
    "        transforms_list=transforms_list,\n",
    "        batch_size=batch_size,\n",
    "        caption_key=caption_key,\n",
    "        tokenizer_name=tokenizer_name,\n",
    "    )\n",
    "\n",
    "    def custom_collate(list_of_dict: List[Dict]) -> Dict:\n",
    "        out = {k: [] for k in list_of_dict[0].keys()}\n",
    "        for d in list_of_dict:\n",
    "            for k, v in d.items():\n",
    "                out[k].append(v)\n",
    "        return out\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=custom_collate,\n",
    "        **dataloader_kwargs,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def text_encoder_embedding_format(enc: str) -> Tuple[int, int]:\n",
    "    \"\"\"Returns sequence length and token embedding dimension for text encoder.\"\"\"\n",
    "    if enc in [\n",
    "        'stabilityai/stable-diffusion-2-base',\n",
    "        'runwayml/stable-diffusion-v1-5',\n",
    "        'CompVis/stable-diffusion-v1-4'\n",
    "    ]:\n",
    "        return 77, 1024\n",
    "    if enc in ['openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378']:\n",
    "        return 77, 1024\n",
    "    if enc in [\"DeepFloyd/t5-v1_1-xxl\"]:\n",
    "        return 120, 4096\n",
    "    raise ValueError(f'Please specifcy the sequence and embedding size of {enc} encoder')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UniversalTextEncoder(torch.nn.Module):\n",
    "    \"\"\"Universal text encoder supporting multiple model types.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Name/path of the model to load\n",
    "        dtype (str): Data type for model weights\n",
    "        pretrained (bool, True): Whether to load pretrained weights\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, dtype: str, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        if self.name.startswith(\"openclip:\"):\n",
    "            assert pretrained, 'Load default pretrained model from openclip'\n",
    "            self.encoder = openclip_text_encoder(\n",
    "                open_clip.create_model_and_transforms(name.lstrip('openclip:'))[0],\n",
    "                torch_dtype=DATA_TYPES[dtype]\n",
    "            )\n",
    "        elif self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            self.encoder = T5EncoderModel.from_pretrained(\n",
    "                name,\n",
    "                torch_dtype=DATA_TYPES[dtype],\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = CLIPTextModel.from_pretrained(\n",
    "                name,\n",
    "                subfolder='text_encoder',\n",
    "                torch_dtype=DATA_TYPES[dtype],\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "\n",
    "    def encode(self, tokenized_caption: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        if self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            out = self.encoder(\n",
    "                tokenized_caption,\n",
    "                attention_mask=attention_mask\n",
    "            )['last_hidden_state']\n",
    "            out = out.unsqueeze(dim=1)\n",
    "            return out, None\n",
    "        else:\n",
    "            return self.encoder(tokenized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = {\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16,\n",
    "    'float32': torch.float32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "class openclip_text_encoder(torch.nn.Module):\n",
    "    \"\"\"OpenCLIP text encoder wrapper.\n",
    "    \n",
    "    Args:\n",
    "        clip_model (Any): OpenCLIP model instance\n",
    "        dtype (torch.dtype, torch.float32): Data type for model weights\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model: Any, dtype: torch.dtype = torch.float32, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.device = None\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward_fxn(self, text: torch.Tensor) -> Tuple[torch.Tensor, None]:\n",
    "        cast_dtype = self.clip_model.transformer.get_cast_dtype()\n",
    "        x = self.clip_model.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + self.clip_model.positional_embedding.to(cast_dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.clip_model.transformer(x, attn_mask=self.clip_model.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip_model.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        x = x.unsqueeze(dim=1) # [batch_size, 1, n_ctx, transformer.width] expected for text_emb\n",
    "        return x, None # HF encoders expected to return multiple values with first being text emb\n",
    "\n",
    "    def forward(self, text: torch.Tensor, **kwargs) -> Tuple[torch.Tensor, None]:\n",
    "        with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "            return self.forward_fxn(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    datadir: str,\n",
    "    savedir: str = \"\",\n",
    "    image_resolutions: list = [256, 512],\n",
    "    save_images: bool = False,\n",
    "    model_dtype: str = \"bfloat16\",\n",
    "    save_dtype: str = \"float16\",\n",
    "    vae: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    text_encoder: str = \"openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\",\n",
    "    batch_size: int = 32,\n",
    "    seed: int = 2024,\n",
    "):\n",
    "    \"\"\"Precompute image and text latents and store them in MDS format.\n",
    "\n",
    "    By default, we only save the image latents for 256x256 and 512x512 image\n",
    "    resolutions (using center crop).\n",
    "\n",
    "    Note that the image latents will be scaled by the vae_scaling_factor.\n",
    "    \"\"\"\n",
    "    cap_key = 'caption'  # Hardcoding the image caption key to 'caption' in MDS dataset\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    device_idx = int(accelerator.process_index)\n",
    "\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(device_idx + seed)\n",
    "    torch.cuda.manual_seed(device_idx + seed)\n",
    "    np.random.seed(device_idx + seed)\n",
    "\n",
    "    dataloader = build_streaming_jdb_precompute_dataloader(\n",
    "        datadir=[datadir],\n",
    "        batch_size=batch_size,\n",
    "        resize_sizes=image_resolutions,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        caption_key=cap_key,\n",
    "        tokenizer_name=text_encoder,\n",
    "        # prefetch_factor=2,\n",
    "        # num_workers=2,\n",
    "        # persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f'Device: {device_idx}, Dataloader sample count: {len(dataloader.dataset)}')\n",
    "\n",
    "    # print(\n",
    "    #     f\"MP variable -> world size: {os.environ['WORLD_SIZE']}, \"\n",
    "    #     f\"RANK: {os.environ['RANK']}, {device}\"\n",
    "    # )\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        vae,\n",
    "        subfolder='vae',  # Change subfolder to appropriate one in hf_hub, if needed\n",
    "        torch_dtype=DATA_TYPES[model_dtype],\n",
    "    )\n",
    "    print(\"Created VAE: \", vae)\n",
    "    assert isinstance(vae, AutoencoderKL)\n",
    "\n",
    "    text_encoder = UniversalTextEncoder(\n",
    "        text_encoder,\n",
    "        dtype=model_dtype,\n",
    "        pretrained=True,\n",
    "    )\n",
    "    print(\"Created text encoder: \", text_encoder)\n",
    "\n",
    "    vae = vae.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "    columns = {\n",
    "        cap_key: 'str',\n",
    "        f'{cap_key}_latents': 'bytes',\n",
    "        'latents_256': 'bytes',\n",
    "        'latents_512': 'bytes',\n",
    "    }\n",
    "    if save_images:\n",
    "        columns['jpg'] = 'jpeg'\n",
    "\n",
    "    remote_upload = os.path.join(savedir, str(accelerator.process_index))\n",
    "    writer = MDSWriter(\n",
    "        out=remote_upload,\n",
    "        columns=columns,\n",
    "        compression=None,\n",
    "        size_limit=256 * (2**20),\n",
    "        max_workers=64,\n",
    "    )\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        image_256 = torch.stack(batch['image_0']).to(device)\n",
    "        image_512 = torch.stack(batch['image_1']).to(device)\n",
    "        captions = torch.stack(batch[cap_key]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type='cuda', dtype=DATA_TYPES[model_dtype]):\n",
    "                latent_dist_256 = vae.encode(image_256)\n",
    "                assert isinstance(latent_dist_256, AutoencoderKLOutput)\n",
    "                latents_256 = (\n",
    "                    latent_dist_256['latent_dist'].sample().data * vae.config.scaling_factor\n",
    "                ).to(DATA_TYPES[save_dtype])\n",
    "\n",
    "                latent_dist_512 = vae.encode(image_512)\n",
    "                assert isinstance(latent_dist_512, AutoencoderKLOutput)\n",
    "                latents_512 = (\n",
    "                    latent_dist_512['latent_dist'].sample().data * vae.config.scaling_factor\n",
    "                ).to(DATA_TYPES[save_dtype])\n",
    "\n",
    "                attention_mask = None\n",
    "                if f'{cap_key}_attention_mask' in batch:\n",
    "                    attention_mask = torch.stack(\n",
    "                        batch[f'{cap_key}_attention_mask']\n",
    "                    ).to(device)\n",
    "\n",
    "                conditioning = text_encoder.encode(\n",
    "                    captions.view(-1, captions.shape[-1]),\n",
    "                    attention_mask=attention_mask,\n",
    "                )[0].to(DATA_TYPES[save_dtype])\n",
    "\n",
    "        try:\n",
    "            if isinstance(latents_256, torch.Tensor) and isinstance(\n",
    "                latents_512, torch.Tensor\n",
    "            ):\n",
    "                latents_256 = latents_256.detach().cpu().numpy()\n",
    "                latents_512 = latents_512.detach().cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if isinstance(conditioning, torch.Tensor):\n",
    "                conditioning = conditioning.detach().cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Write the batch to the MDS file\n",
    "            for i in range(latents_256.shape[0]):\n",
    "                mds_sample = {\n",
    "                    cap_key: batch['sample'][i][cap_key],\n",
    "                    f'{cap_key}_latents': np.reshape(conditioning[i], -1).tobytes(),\n",
    "                    'latents_256': latents_256[i].tobytes(),\n",
    "                    'latents_512': latents_512[i].tobytes(),\n",
    "                }\n",
    "                if save_images:\n",
    "                    mds_sample['jpg'] = batch['sample'][i]['jpg']\n",
    "                writer.write(mds_sample)\n",
    "        except RuntimeError:\n",
    "            print('Runtime error CUDA, skipping this batch')\n",
    "\n",
    "    writer.finish()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    accelerator.wait_for_everyone()\n",
    "    print(f'Process {accelerator.process_index} finished')\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Merge the mds shards created by each device (only do on main process)\n",
    "    if accelerator.is_main_process:\n",
    "        shards_metadata = [\n",
    "            os.path.join(savedir, str(i), 'index.json')\n",
    "            for i in range(accelerator.num_processes)\n",
    "        ]\n",
    "        merge_index(shards_metadata, out=savedir, keep_local=True)\n",
    "\n",
    "# for split in train valid; do\n",
    "#     accelerate launch --multi_gpu --num_processes 8 precompute.py --datadir ./datadir/jdb/mds/$split/ --savedir ./datadir/jdb/mds_latents_sdxl1_dfnclipH14/$split/ --vae stabilityai/stable-diffusion-xl-base-1.0 --text_encoder openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378 --batch_size 32\n",
    "# done\n",
    "\n",
    "main(\n",
    "    datadir=os.path.join(DATA_DIR, 'mds', 'train'),\n",
    "    savedir=os.path.join(DATA_DIR, 'mds_latents_sdxl1_dfnclipH14', 'train'),\n",
    "    image_resolutions=[256, 512],\n",
    "    save_images=False,\n",
    "    model_dtype=\"bfloat16\",\n",
    "    save_dtype=\"float16\",\n",
    "    vae=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    text_encoder=\"openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\",\n",
    "    batch_size=32,\n",
    "    seed=2024,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
