{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add57ae6",
   "metadata": {},
   "source": [
    "# micro diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf5321",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/JourneyDB/JourneyDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16b488",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041e148",
   "metadata": {},
   "source": [
    "### ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ðŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ðŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ðŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ðŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ðŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    \"mosaicml[tensorboard, wandb]\" \\\n",
    "    accelerate \\\n",
    "    beautifulsoup4 \\\n",
    "    datasets \\\n",
    "    diffusers \\\n",
    "    easydict \\\n",
    "    einops \\\n",
    "    fastparquet \\\n",
    "    huggingface_hub \\\n",
    "    hydra-core \\\n",
    "    mosaicml-streaming \\\n",
    "    omegaconf \\\n",
    "    open_clip_torch \\\n",
    "    pandas \\\n",
    "    timm \\\n",
    "    torchmetrics \\\n",
    "    tqdm \\\n",
    "    transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from collections.abc import Iterable\n",
    "from composer import Trainer\n",
    "from composer.algorithms import GradientClipping\n",
    "from composer.algorithms.low_precision_layernorm import apply_low_precision_layernorm\n",
    "from composer.callbacks import LRMonitor\n",
    "from composer.callbacks import OptimizerMonitor\n",
    "from composer.callbacks import RuntimeEstimator\n",
    "from composer.callbacks import SpeedMonitor\n",
    "from composer.core import Precision\n",
    "from composer.loggers import TensorboardLogger\n",
    "from composer.loggers.wandb_logger import WandBLogger\n",
    "from composer.models import ComposerModel\n",
    "from composer.optim import CosineAnnealingWithWarmupScheduler\n",
    "from composer.optim import ConstantScheduler\n",
    "from composer.utils import dist, reproducibility\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers.models.modeling_outputs import AutoencoderKLOutput\n",
    "from easydict import EasyDict\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from huggingface_hub import hf_hub_download\n",
    "from hydra import compose, initialize_config_dir\n",
    "from itertools import repeat\n",
    "from micro_diffusion.models.callbacks import LogDiffusionImages\n",
    "from micro_diffusion.models.callbacks import NaNCatcher\n",
    "from micro_diffusion.models.utils import text_encoder_embedding_format\n",
    "from multiprocessing import Pool\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from streaming import MDSWriter\n",
    "from streaming import Stream, StreamingDataset\n",
    "from streaming.base import MDSWriter\n",
    "from streaming.base.util import merge_index\n",
    "from timm.models.vision_transformer import PatchEmbed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5Tokenizer\n",
    "from typing import Callable, Dict, List, Optional, Sequence, Union\n",
    "from typing import List, Dict, Union, Optional\n",
    "from typing import List, Optional\n",
    "from typing import Optional, Tuple, Dict, Union, List, Any\n",
    "import hydra\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "# cuDNN(CUDA Deep Neural Network libraryï¼‰ã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–\n",
    "# 3-5%ã®é€Ÿåº¦å‘ä¸ŠãŒè¦‹è¾¼ã¾ã‚Œã‚‹\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®è¨­å®š\n",
    "seed = 18\n",
    "reproducibility.seed_all(seed)\n",
    "\n",
    "logger.info(f\"PyTorch {torch.__version__}\")\n",
    "logger.info(f\"Transformers {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9446431",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ROOT = os.path.expanduser(\"~\")\n",
    "CACHE_DIR = os.path.join(USER_ROOT, \".cache\", \"micro_diffusion\")\n",
    "\n",
    "DATA_DIR = os.path.join(CACHE_DIR, \"data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "logger.info(f\"{DATA_DIR=}\")\n",
    "\n",
    "MODEL_DIR = os.path.join(CACHE_DIR, \"models\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "logger.info(f\"{MODEL_DIR=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = {\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16,\n",
    "    'float32': torch.float32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82559f70",
   "metadata": {},
   "source": [
    "## DiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c6c486",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulate(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Applies modulation to input tensor using shift and scale factors.\"\"\"\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_norm(norm_type: str, dim: int, eps: float = 1e-6) -> nn.Module:\n",
    "    \"\"\"Creates a normalization layer based on the specified type.\"\"\"\n",
    "    if norm_type == \"layernorm\":\n",
    "        return nn.LayerNorm(dim, eps=eps, bias=False)\n",
    "    elif norm_type == \"np_layernorm\":\n",
    "        return nn.LayerNorm(dim, eps=eps, elementwise_affine=False, bias=False)\n",
    "    else:\n",
    "        raise ValueError('Norm type not supported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa5473",
   "metadata": {},
   "source": [
    "### SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self attention layer.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Input and output tensor dimension\n",
    "        num_heads (int): Number of attention heads\n",
    "        qkv_bias (bool, True): Whether to use bias in QKV linear layers\n",
    "        norm_eps (float, 1e-6): Epsilon for normalization layers\n",
    "        hidden_dim (Optional[int], None): Dimension for qkv space. If None, same as input dim\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_eps: float = 1e-6,\n",
    "        hidden_dim: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim\n",
    "        assert hidden_dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        self.qkv = nn.Linear(dim, hidden_dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(hidden_dim, dim, bias=qkv_bias)\n",
    "\n",
    "        self.ln_q = create_norm('np_layernorm', dim=hidden_dim, eps=norm_eps)\n",
    "        self.ln_k = create_norm('np_layernorm', dim=hidden_dim, eps=norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)\n",
    "\n",
    "        q = self.ln_q(q.view(B, N, self.num_heads * self.head_dim)).view(\n",
    "            B, N, self.num_heads, self.head_dim).to(q.dtype)\n",
    "        k = self.ln_k(k.view(B, N, self.num_heads * self.head_dim)).view(\n",
    "            B, N, self.num_heads, self.head_dim).to(k.dtype)\n",
    "\n",
    "        x = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q.transpose(1, 2),\n",
    "            k.transpose(1, 2),\n",
    "            v.transpose(1, 2),\n",
    "            is_causal=False\n",
    "        ).transpose(1, 2).contiguous()\n",
    "        \n",
    "        x = x.view(B, N, self.num_heads * self.head_dim)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "    def custom_init(self, init_std: float) -> None:\n",
    "        nn.init.trunc_normal_(self.qkv.weight, mean=0.0, std=0.02)\n",
    "        nn.init.trunc_normal_(self.proj.weight, mean=0.0, std=init_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93796f",
   "metadata": {},
   "source": [
    "### CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross attention layer.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Input and output tensor dimension\n",
    "        num_heads (int): Number of attention heads\n",
    "        qkv_bias (bool, True): Whether to use bias in QKV linear layers\n",
    "        norm_eps (float, 1e-6): Epsilon for normalization layers\n",
    "        hidden_dim (Optional[int], None): Dimension for qkv space. If None, same as input dim\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_eps: float = 1e-6,\n",
    "        hidden_dim: Optional[int] = None\n",
    "    ):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim\n",
    "        assert hidden_dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        self.q_linear = nn.Linear(dim, hidden_dim, bias=qkv_bias)\n",
    "        self.kv_linear = nn.Linear(dim, hidden_dim*2, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(hidden_dim, dim, bias=qkv_bias)\n",
    "\n",
    "        self.ln_q = create_norm('np_layernorm', dim=hidden_dim, eps=norm_eps)\n",
    "        self.ln_k = create_norm('np_layernorm', dim=hidden_dim, eps=norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        q = self.q_linear(x).reshape(B, N, self.num_heads, self.head_dim)\n",
    "        kv = self.kv_linear(cond).reshape(B, -1, 2, self.num_heads, self.head_dim)\n",
    "        k, v = kv.unbind(2)\n",
    "\n",
    "        q = self.ln_q(q.view(B, N, self.num_heads * self.head_dim)).view(\n",
    "            B, N, self.num_heads, self.head_dim).to(q.dtype)\n",
    "        k = self.ln_k(k.view(B, -1, self.num_heads * self.head_dim)).view(\n",
    "            B, -1, self.num_heads, self.head_dim).to(k.dtype)\n",
    "\n",
    "        x = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q.transpose(1, 2),\n",
    "            k.transpose(1, 2),\n",
    "            v.transpose(1, 2),\n",
    "            is_causal=False\n",
    "        ).transpose(1, 2).contiguous()\n",
    "        \n",
    "        x = x.view(B, -1, self.num_heads * self.head_dim)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "    def custom_init(self, init_std: float) -> None:\n",
    "        for linear in (self.q_linear, self.kv_linear):\n",
    "            nn.init.trunc_normal_(linear.weight, mean=0.0, std=0.02)\n",
    "        nn.init.trunc_normal_(self.proj.weight, mean=0.0, std=init_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c782ffe",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/mlp.py\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"MLP implementation from timm (without the dropout layers)\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): Input tensor dimension\n",
    "        hidden_features (Optional[int], None): Number of hidden features. If None, same as in_features\n",
    "        out_features (Optional[int], None): Number of output features. If None, same as in_features\n",
    "        act_layer (Any, nn.GELU): Activation layer constructor. Defaults to GELU with tanh approximation\n",
    "        norm_layer (Optional[Any], None): Normalization layer constructor. If None, uses Identity\n",
    "        bias (bool, True): Whether to use bias in linear layers\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        act_layer: Any = lambda: nn.GELU(approximate=\"tanh\"),\n",
    "        norm_layer: Optional[Any] = None,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.norm = norm_layer if norm_layer is not None else nn.Identity()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward block with SiLU activation.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Input and output dimension\n",
    "        hidden_dim (int): Hidden dimension betwen the two linear layers\n",
    "        multiple_of (int): Round hidden dimension up to nearest multiple of this value\n",
    "        use_bias (bool): Whether to use bias terms in linear layers\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        use_bias: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        self.hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(dim, self.hidden_dim, bias=use_bias)\n",
    "        self.w2 = nn.Linear(dim, self.hidden_dim, bias=use_bias)\n",
    "        self.w3 = nn.Linear(self.hidden_dim, dim, bias=use_bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
    "\n",
    "    def custom_init(self, init_std: float) -> None:\n",
    "        nn.init.trunc_normal_(self.w1.weight, mean=0.0, std=0.02)\n",
    "        for linear in (self.w2, self.w3):\n",
    "            nn.init.trunc_normal_(linear.weight, mean=0.0, std=init_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc12771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardECMoe(nn.Module):\n",
    "    \"\"\"Expert-Choice style Mixture of Experts feed-forward layer with GELU activation.\n",
    "    \n",
    "    Args:\n",
    "        num_experts (int): Number of experts in the layer\n",
    "        expert_capacity (float): Capacity factor determining tokens per expert\n",
    "        dim (int): Input and output dimension\n",
    "        hidden_dim (int): Hidden dimension between the two linear layers\n",
    "        multiple_of (int): Round hidden dimension up to nearest multiple of this value\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_experts: int,\n",
    "        expert_capacity: float,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_capacity = expert_capacity\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Parameter(torch.ones(num_experts, dim, self.hidden_dim))\n",
    "        self.w2 = nn.Parameter(torch.ones(num_experts, self.hidden_dim, dim))\n",
    "        self.gate = nn.Linear(dim, num_experts, bias=False)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert len(x.shape) == 3\n",
    "        n, t, d = x.shape\n",
    "        tokens_per_expert = int(self.expert_capacity * t / self.num_experts)\n",
    "\n",
    "        scores = self.gate(x)  # [n, t, e]\n",
    "        probs = F.softmax(scores, dim=-1)  # [n, t, e]\n",
    "        g, m = torch.topk(probs.permute(0, 2, 1), tokens_per_expert, dim=-1)  # [n, e, k], [n, e, k]\n",
    "        p = F.one_hot(m, num_classes=t).float()  # [n, e, k, t]\n",
    "\n",
    "        xin = torch.einsum('nekt, ntd -> nekd', p, x)  # [n, e, k, d]\n",
    "        h = torch.einsum('nekd, edf -> nekf', xin, self.w1)  # [n, e, k, 4d]\n",
    "        h = self.gelu(h)\n",
    "        h = torch.einsum('nekf, efd -> nekd', h, self.w2)  # [n, e, k, d]\n",
    "\n",
    "        out = g.unsqueeze(dim=-1) * h  # [n, e, k, d]\n",
    "        out = torch.einsum('nekt, nekd -> ntd', p, out)\n",
    "        return out\n",
    "    \n",
    "    def custom_init(self, init_std: float):\n",
    "        nn.init.trunc_normal_(self.gate.weight, mean=0.0, std=0.02)\n",
    "        nn.init.trunc_normal_(self.w1, mean=0.0, std=0.02)\n",
    "        nn.init.trunc_normal_(self.w2, mean=0.0, std=init_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c5e99d",
   "metadata": {},
   "source": [
    "### TImestepEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestepEmbedder(nn.Module):\n",
    "    \"\"\"Embeds scalar timesteps into vector representations.\n",
    "    \n",
    "    Args:\n",
    "        hidden_size (int): Size of hidden dimension\n",
    "        act_layer (Any): Activation layer constructor\n",
    "        frequency_embedding_size (int, 512): Size of frequency embedding\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        act_layer: Any,\n",
    "        frequency_embedding_size: int = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
    "            act_layer(),\n",
    "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
    "        )\n",
    "        self.frequency_embedding_size = frequency_embedding_size\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(t: torch.Tensor, dim: int, max_period: int = 10000) -> torch.Tensor:\n",
    "        \"\"\"Create sinusoidal timestep embeddings.\"\"\"\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(\n",
    "                start=0,\n",
    "                end=half,\n",
    "                dtype=torch.float32,\n",
    "                device=t.device\n",
    "            ) / half\n",
    "        )\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size).to(self.dtype)\n",
    "        return self.mlp(t_freq)\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25999ac",
   "metadata": {},
   "source": [
    "### CaptionProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionProjection(nn.Module):\n",
    "    \"\"\"Projects caption embeddings to model dimension.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        hidden_size (int): Size of hidden dimension\n",
    "        act_layer (Any): Activation layer constructor\n",
    "        norm_layer (Any): Normalization layer constructor\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_size: int,\n",
    "        act_layer: Any,\n",
    "        norm_layer: Any\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.y_proj = Mlp(\n",
    "            in_features=in_channels,\n",
    "            hidden_features=hidden_size,\n",
    "            out_features=hidden_size,\n",
    "            act_layer=act_layer,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, caption: torch.Tensor) -> torch.Tensor:\n",
    "        return self.y_proj(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bd18b",
   "metadata": {},
   "source": [
    "### AttentionBlockPromptEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlockPromptEmbedding(nn.Module):\n",
    "    \"\"\"Attention block specifically for processing prompt embeddings.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Input and output dimension\n",
    "        head_dim (int): Dimension size per attention head\n",
    "        mlp_ratio (float): Multiplier for feed-forward network hidden dimension w.r.t input dim\n",
    "        multiple_of (int): Round feed-forward network hidden dimension up to nearest multiple of this value\n",
    "        norm_eps (float): Epsilon value for layer normalization\n",
    "        use_bias (bool): Whether to use bias terms in linear layers\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        head_dim: int, \n",
    "        mlp_ratio: float,\n",
    "        multiple_of: int,\n",
    "        norm_eps: float,\n",
    "        use_bias: bool,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert dim % head_dim == 0, 'Hidden dimension must be divisible by head dim'\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = dim // head_dim\n",
    "        \n",
    "        self.norm1 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "        self.attn = SelfAttention(\n",
    "            dim=dim,\n",
    "            num_heads=self.num_heads,\n",
    "            qkv_bias=use_bias,\n",
    "            norm_eps=norm_eps,\n",
    "        )\n",
    "        self.norm2 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "        self.mlp = FeedForward(\n",
    "            dim=dim,\n",
    "            hidden_dim=int(dim * mlp_ratio),\n",
    "            multiple_of=multiple_of,\n",
    "            use_bias=use_bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "    def custom_init(self, init_std: float = 0.02) -> None:\n",
    "        self.attn.custom_init(init_std)\n",
    "        self.mlp.custom_init(init_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcda204",
   "metadata": {},
   "source": [
    "### DiTBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiTBlock(nn.Module):\n",
    "    \"\"\"DiT transformer block comprising Attention and MLP blocks. It supports choosing between \n",
    "     dense feed-forward and expert-choice style Mixture-of-Experts feed-forward blocks.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Input and output dimension of the block\n",
    "        head_dim (int): Dimension of each attention head\n",
    "        mlp_ratio (float): Ratio for hidden dimension between linear layers in MLP block\n",
    "        qkv_ratio (float): Ratio for dimension in qkv layers in attention block\n",
    "        multiple_of (int): Round hidden dimensions up to nearest multiple of this value in MLP block\n",
    "        pooled_emb_dim (int): Dimension of pooled caption embeddings\n",
    "        norm_eps (float): Epsilon for layer normalization\n",
    "        depth_init (bool): Whether to initialize weights of the last layer in MLP/Attention block based on block index\n",
    "        layer_id (int): Index of this block in the dit model\n",
    "        num_layers (int): Total number of blocks in the dit model\n",
    "        compress_xattn (bool): Whether to scale cross-attention qkv dimension using qkv_ratio \n",
    "        use_bias (bool): Whether to use bias in linear layers\n",
    "        moe_block (bool): Whether to use mixture of experts for MLP block\n",
    "        num_experts (int): Number of experts if using MoE block\n",
    "        expert_capacity (float): Capacity factor for each expert if using MoE block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        head_dim: int,\n",
    "        mlp_ratio: float,\n",
    "        qkv_ratio: float,\n",
    "        multiple_of: int,\n",
    "        pooled_emb_dim: int,\n",
    "        norm_eps: float,\n",
    "        depth_init: bool,\n",
    "        layer_id: int,\n",
    "        num_layers: int,\n",
    "        compress_xattn: bool,\n",
    "        use_bias: bool,\n",
    "        moe_block: bool,\n",
    "        num_experts: int,\n",
    "        expert_capacity: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        qkv_hidden_dim = (\n",
    "            (head_dim * 2) * ((int(dim * qkv_ratio) + head_dim * 2 - 1) // (head_dim * 2))\n",
    "            if qkv_ratio != 1 else dim\n",
    "        )\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "\n",
    "        self.norm1 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "        self.attn = SelfAttention(\n",
    "            dim=dim,\n",
    "            num_heads=qkv_hidden_dim // head_dim,\n",
    "            qkv_bias=use_bias,\n",
    "            norm_eps=norm_eps,\n",
    "            hidden_dim=qkv_hidden_dim,\n",
    "        )\n",
    "        self.cross_attn = CrossAttention(\n",
    "            dim=dim,\n",
    "            num_heads=qkv_hidden_dim // head_dim if compress_xattn else dim // head_dim,\n",
    "            qkv_bias=use_bias,\n",
    "            norm_eps=norm_eps,\n",
    "            hidden_dim=qkv_hidden_dim if compress_xattn else dim,\n",
    "        )\n",
    "        self.norm2 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "        self.norm3 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "        \n",
    "        self.mlp = (\n",
    "            FeedForwardECMoe(num_experts, expert_capacity, dim, mlp_hidden_dim, multiple_of)\n",
    "            if moe_block else\n",
    "            FeedForward(dim, mlp_hidden_dim, multiple_of, use_bias)\n",
    "        )\n",
    "\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(pooled_emb_dim, 6 * dim, bias=True),\n",
    "        )\n",
    "        \n",
    "        self.weight_init_std = (\n",
    "            0.02 / (2 * (layer_id + 1)) ** 0.5 if depth_init else\n",
    "            0.02 / (2 * num_layers) ** 0.5\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, c: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (\n",
    "            self.adaLN_modulation(c).chunk(6, dim=1)\n",
    "        )\n",
    "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
    "        x = x + self.cross_attn(self.norm2(x), y)\n",
    "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm3(x), shift_mlp, scale_mlp))\n",
    "        return x\n",
    "\n",
    "    def custom_init(self):\n",
    "        for norm in (self.norm1, self.norm2, self.norm3):\n",
    "            norm.reset_parameters()\n",
    "        self.attn.custom_init(self.weight_init_std)\n",
    "        self.cross_attn.custom_init(self.weight_init_std)\n",
    "        self.mlp.custom_init(self.weight_init_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca184e",
   "metadata": {},
   "source": [
    "### T2IFinalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T2IFinalLayer(nn.Module):\n",
    "    \"\"\"The final layer of DiT architecture.\n",
    "    \n",
    "    Args:\n",
    "        hidden_size (int): Size of hidden dimension\n",
    "        time_emb_dim (int): Dimension of timestep embeddings\n",
    "        patch_size (int): Size of image patches \n",
    "        out_channels (int): Number of output channels\n",
    "        act_layer (Any): Activation layer constructor\n",
    "        norm_final (nn.Module): Final normalization layer\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        time_emb_dim: int,\n",
    "        patch_size: int,\n",
    "        out_channels: int,\n",
    "        act_layer: Any,\n",
    "        norm_final: nn.Module\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(\n",
    "            hidden_size,\n",
    "            patch_size * patch_size * out_channels,\n",
    "            bias=True\n",
    "        )\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            act_layer(),\n",
    "            nn.Linear(time_emb_dim, 2 * hidden_size, bias=True)\n",
    "        )\n",
    "        self.norm_final = norm_final\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
    "        x = modulate(self.norm_final(x), shift, scale)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de00f8f",
   "metadata": {},
   "source": [
    "### DiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2637c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Get 1D sinusoidal positional embeddings from grid.\"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    return np.concatenate([emb_sin, emb_cos], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbeb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Get 2D sinusoidal positional embeddings from grid.\"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])\n",
    "\n",
    "    return np.concatenate([emb_h, emb_w], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f671ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntuple(n: int):\n",
    "    \"\"\"Converts input into n-tuple.\"\"\"\n",
    "    def parse(x):\n",
    "        if isinstance(x, Iterable) and not isinstance(x, str):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef991cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(\n",
    "    embed_dim: int,\n",
    "    grid_size: Union[int, Tuple[int, int]],\n",
    "    cls_token: bool = False,\n",
    "    extra_tokens: int = 0,\n",
    "    pos_interp_scale: float = 1.0,\n",
    "    base_size: int = 16\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Get 2D sinusoidal positional embeddings.\"\"\"\n",
    "    to_2tuple = ntuple(2)\n",
    "    if isinstance(grid_size, int):\n",
    "        grid_size = to_2tuple(grid_size)\n",
    "    # Interpolate position embeddings to adapt model across resolutions. Interestingly, without any interpolation\n",
    "    # the model does converge slowly at start (~1000 steps) but eventually achieves near similar qualitative performance.\n",
    "    grid_h = np.arange(grid_size[0], dtype=np.float32) / (grid_size[0] / base_size) / pos_interp_scale\n",
    "    grid_w = np.arange(grid_size[1], dtype=np.float32) / (grid_size[1] / base_size) / pos_interp_scale\n",
    "    grid = np.meshgrid(grid_w, grid_h)\n",
    "    grid = np.stack(grid, axis=0)\n",
    "    grid = grid.reshape([2, 1, grid_size[1], grid_size[0]])\n",
    "\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token and extra_tokens > 0:\n",
    "        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee89fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_out_token(x: torch.Tensor, ids_keep: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Mask out tokens specified by ids_keep.\"\"\"\n",
    "    N, L, D = x.shape  # batch, length, dim\n",
    "    x_masked = torch.gather(\n",
    "        x,\n",
    "        dim=1,\n",
    "        index=ids_keep.unsqueeze(-1).repeat(1, 1, D)\n",
    "    )\n",
    "    return x_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(batch: int, length: int, mask_ratio: float, device: torch.device) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Get binary mask for input sequence. \n",
    "    \n",
    "    mask: binary mask, 0 is keep, 1 is remove\n",
    "    ids_keep: indices of tokens to keep\n",
    "    ids_restore: indices to restore the original order\n",
    "    \"\"\"\n",
    "    len_keep = int(length * (1 - mask_ratio))\n",
    "    noise = torch.rand(batch, length, device=device)  # noise in [0, 1]\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "    # keep the first subset\n",
    "    ids_keep = ids_shuffle[:, :len_keep]\n",
    "\n",
    "    mask = torch.ones([batch, length], device=device)\n",
    "    mask[:, :len_keep] = 0\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "    return {\n",
    "        'mask': mask,\n",
    "        'ids_keep': ids_keep,\n",
    "        'ids_restore': ids_restore\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1beb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmask_tokens(x: torch.Tensor, ids_restore: torch.Tensor, mask_token: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Unmask tokens using provided mask token.\"\"\"\n",
    "    mask_tokens = mask_token.repeat(x.shape[0], ids_restore.shape[1] - x.shape[1], 1)\n",
    "    x_ = torch.cat([x, mask_tokens], dim=1)\n",
    "    x_ = torch.gather(\n",
    "        x_,\n",
    "        dim=1,\n",
    "        index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2])\n",
    "    )  # unshuffle\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c51c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiT(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion Transformer (DiT) model than support conditioning on caption embeddings for text-to-image generation.\n",
    "    \n",
    "    Args:\n",
    "        input_size (int, default: 32): Size of input image (assumed square)\n",
    "        patch_size (int, default: 2): Size of patches for patch embedding\n",
    "        in_channels (int, default: 4): Number of input image channels (by default assuming four channel latent space)\n",
    "        dim (int, default: 1152): Dimension of transformer backbone, i.e., dimension of major transformer layers\n",
    "        depth (int, default: 28): Number of transformer blocks\n",
    "        head_dim (int, default: 64): Dimension of each attention head\n",
    "        multiple_of (int, default: 256): Round hidden dimensions up to nearest multiple of this value in MLP block\n",
    "        caption_channels (int, default: 4096): Number of channels in caption embeddings\n",
    "        pos_interp_scale (float, default: 1.0): Scale for positional embedding interpolation (1.0 for 256x256, 2.0 for 512x512)\n",
    "        norm_eps (float, default: 1e-6): Epsilon for layer normalization\n",
    "        depth_init (bool, default: True): Whether to use depth-dependent initialization in DiT blocks\n",
    "        qkv_multipliers (List[float], default: [1.0]): Multipliers for QKV projection dimensions in DiT blocks\n",
    "        ffn_multipliers (List[float], default: [4.0]): Multipliers for FFN hidden dimensions in DiT blocks\n",
    "        use_patch_mixer (bool, default: True): Whether to use patch mixer layers\n",
    "        patch_mixer_depth (int, default: 4): Number of patch mixer blocks\n",
    "        patch_mixer_dim (int, default: 512): Dimension of patch-mixer layers\n",
    "        patch_mixer_qkv_ratio (float, default: 1.0): Multipliers for QKV projection dimensions in patch-mixer blocks\n",
    "        patch_mixer_mlp_ratio (float, default: 1.0): Multipliers for FFN hidden dimensions in patch-mixer blocks\n",
    "        use_bias (bool, default: True): Whether to use bias in linear layers\n",
    "        num_experts (int, default: 8):  Number of experts if using MoE block\n",
    "        expert_capacity (int, default: 1): Capacity factor for each expert if using MoE FFN layers\n",
    "        experts_every_n (int, default: 2): Add MoE FFN layers every n blocks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 32,\n",
    "        patch_size: int = 2,\n",
    "        in_channels: int = 4,\n",
    "        dim: int = 1152,\n",
    "        depth: int = 28,\n",
    "        head_dim: int = 64,\n",
    "        multiple_of: int = 256,\n",
    "        caption_channels: int = 1024,\n",
    "        pos_interp_scale: float = 1.0,\n",
    "        norm_eps: float = 1e-6,\n",
    "        depth_init: bool = True,\n",
    "        qkv_multipliers: List[float] = [1.0],\n",
    "        ffn_multipliers: List[float] = [4.0],\n",
    "        use_patch_mixer: bool = True,\n",
    "        patch_mixer_depth: int = 4,\n",
    "        patch_mixer_dim: int = 512,\n",
    "        patch_mixer_qkv_ratio: float = 1.0,\n",
    "        patch_mixer_mlp_ratio: float = 1.0,\n",
    "        use_bias: bool = True,\n",
    "        num_experts: int = 8,\n",
    "        expert_capacity: int = 1,\n",
    "        experts_every_n: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.head_dim = head_dim\n",
    "        self.pos_interp_scale = pos_interp_scale\n",
    "        self.use_patch_mixer = use_patch_mixer\n",
    "        \n",
    "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
    "        self.x_embedder = PatchEmbed(\n",
    "            input_size, patch_size, in_channels, dim, bias=True\n",
    "        )\n",
    "        self.t_embedder = TimestepEmbedder(dim, approx_gelu)\n",
    "\n",
    "        num_patches = self.x_embedder.num_patches\n",
    "        self.base_size = input_size // self.patch_size\n",
    "        self.register_buffer(\"pos_embed\", torch.zeros(1, num_patches, dim))\n",
    "\n",
    "        self.y_embedder = CaptionProjection(\n",
    "            in_channels=caption_channels,\n",
    "            hidden_size=dim,\n",
    "            act_layer=approx_gelu,\n",
    "            norm_layer=create_norm('layernorm', dim, eps=norm_eps)\n",
    "        )\n",
    "\n",
    "        self.y_emb_preprocess = AttentionBlockPromptEmbedding(\n",
    "            dim,\n",
    "            head_dim,\n",
    "            mlp_ratio=4.0,\n",
    "            multiple_of=multiple_of,\n",
    "            norm_eps=norm_eps,\n",
    "            use_bias=use_bias\n",
    "        )\n",
    "        \n",
    "        self.pooled_y_emb_process = Mlp(\n",
    "            dim,\n",
    "            dim,\n",
    "            dim,\n",
    "            approx_gelu,\n",
    "            norm_layer=create_norm('layernorm', dim, eps=norm_eps)\n",
    "        )\n",
    "\n",
    "        if self.use_patch_mixer:\n",
    "            expert_blocks_idx = [\n",
    "                i for i in range(1, patch_mixer_depth) \n",
    "                if (i+1) % experts_every_n == 0\n",
    "            ]\n",
    "            is_moe_block = [\n",
    "                True if i in expert_blocks_idx else False \n",
    "                for i in range(patch_mixer_depth)\n",
    "            ]\n",
    "                 \n",
    "            self.patch_mixer = nn.ModuleList([\n",
    "                DiTBlock(\n",
    "                    dim=patch_mixer_dim,\n",
    "                    head_dim=head_dim,\n",
    "                    mlp_ratio=patch_mixer_mlp_ratio,\n",
    "                    qkv_ratio=patch_mixer_qkv_ratio,\n",
    "                    multiple_of=multiple_of,\n",
    "                    pooled_emb_dim=dim,\n",
    "                    norm_eps=norm_eps,\n",
    "                    depth_init=False,\n",
    "                    layer_id=0,\n",
    "                    num_layers=depth,\n",
    "                    compress_xattn=False,\n",
    "                    use_bias=use_bias,\n",
    "                    moe_block=is_moe_block[i],\n",
    "                    num_experts=num_experts,\n",
    "                    expert_capacity=expert_capacity\n",
    "                ) for i in range(patch_mixer_depth)\n",
    "            ])\n",
    "\n",
    "            # Couple of projection layers\n",
    "            if patch_mixer_dim != dim:\n",
    "                self.patch_mixer_map_xin = nn.Sequential(\n",
    "                    create_norm('layernorm', dim, eps=norm_eps),\n",
    "                    nn.Linear(dim, patch_mixer_dim, bias=use_bias)\n",
    "                )\n",
    "                self.patch_mixer_map_xout = nn.Sequential(\n",
    "                    create_norm('layernorm', patch_mixer_dim, eps=norm_eps),\n",
    "                    nn.Linear(patch_mixer_dim, dim, bias=use_bias)\n",
    "                )\n",
    "                self.patch_mixer_map_y = nn.Sequential(\n",
    "                    create_norm('layernorm', dim, eps=norm_eps),\n",
    "                    nn.Linear(dim, patch_mixer_dim, bias=use_bias)\n",
    "                )\n",
    "            else:\n",
    "                self.patch_mixer_map_xin = nn.Identity()\n",
    "                self.patch_mixer_map_xout = nn.Identity()\n",
    "                self.patch_mixer_map_y = nn.Identity()\n",
    "\n",
    "        assert len(ffn_multipliers) == len(qkv_multipliers)\n",
    "        if len(ffn_multipliers) == depth:\n",
    "            qkv_ratios = qkv_multipliers\n",
    "            mlp_ratios = ffn_multipliers\n",
    "        else:\n",
    "            # Distribute the multiplers across each partition\n",
    "            num_splits = len(ffn_multipliers)\n",
    "            assert depth % num_splits == 0, 'number of blocks should be divisible by number of splits'\n",
    "            depth_per_split = depth // num_splits\n",
    "            qkv_ratios = list(np.array([\n",
    "                [m]*depth_per_split for m in qkv_multipliers\n",
    "            ]).reshape(-1))\n",
    "            mlp_ratios = list(np.array([\n",
    "                [m]*depth_per_split for m in ffn_multipliers\n",
    "            ]).reshape(-1))\n",
    "\n",
    "        # Don't use MoE in last block\n",
    "        expert_blocks_idx = [\n",
    "            i for i in range(0, depth - 1) \n",
    "            if (i+1) % experts_every_n == 0\n",
    "        ]\n",
    "        is_moe_block = [\n",
    "            True if i in expert_blocks_idx else False \n",
    "            for i in range(depth)\n",
    "        ]\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            DiTBlock(\n",
    "                dim=dim,\n",
    "                head_dim=head_dim,\n",
    "                mlp_ratio=mlp_ratios[i],\n",
    "                qkv_ratio=qkv_ratios[i],\n",
    "                multiple_of=multiple_of,\n",
    "                pooled_emb_dim=dim,\n",
    "                norm_eps=norm_eps,\n",
    "                depth_init=depth_init,\n",
    "                layer_id=i,\n",
    "                num_layers=depth,\n",
    "                compress_xattn=False,\n",
    "                use_bias=use_bias,\n",
    "                moe_block=is_moe_block[i],\n",
    "                num_experts=num_experts,\n",
    "                expert_capacity=expert_capacity\n",
    "            ) for i in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"mask_token\", \n",
    "            torch.zeros(1, 1, patch_size ** 2 * self.out_channels)\n",
    "        )\n",
    "        self.final_layer = T2IFinalLayer(\n",
    "            dim,\n",
    "            dim,\n",
    "            patch_size,\n",
    "            self.out_channels,\n",
    "            approx_gelu,\n",
    "            create_norm('layernorm', dim, eps=norm_eps)\n",
    "        )\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward_without_cfg(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        mask_ratio: float = 0,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"Forward pass without classifier-free guidance.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, channels, height, width)\n",
    "            t: Timestep tensor of shape (batch_size,)\n",
    "            y: Caption embedding tensor of shape (batch_size, 1, seq_len, dim)\n",
    "            mask_ratio: Ratio of patches to mask during training (between 0 and 1)\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing:\n",
    "                - 'sample': Output tensor of shape (batch_size, out_channels, height, width)\n",
    "                - 'mask': Optional binary mask tensor, if masking was applied else None\n",
    "        \"\"\"\n",
    "        self.h = x.shape[-2] // self.patch_size\n",
    "        self.w = x.shape[-1] // self.patch_size\n",
    "\n",
    "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
    "        t = self.t_embedder(t.expand(x.shape[0]))  # (N, D)\n",
    "\n",
    "        y = self.y_embedder(y)  # (N, 1, L, D)\n",
    "        y = self.y_emb_preprocess(y.squeeze(dim=1)).unsqueeze(dim=1)  # (N, 1, L, D) -> (N, D)\n",
    "        y_pooled = self.pooled_y_emb_process(y.mean(dim=-2).squeeze(dim=1))\n",
    "        t = t + y_pooled\n",
    "\n",
    "        mask = None\n",
    "        \n",
    "        if self.use_patch_mixer:\n",
    "            x = self.patch_mixer_map_xin(x)\n",
    "            y_mixer = self.patch_mixer_map_y(y)\n",
    "            for block in self.patch_mixer:\n",
    "                x = block(x, y_mixer, t)  # (N, T, D_mixer)\n",
    "        \n",
    "        if mask_ratio > 0:\n",
    "            mask_dict = get_mask(\n",
    "                x.shape[0], x.shape[1], \n",
    "                mask_ratio=mask_ratio, \n",
    "                device=x.device\n",
    "            )\n",
    "            ids_keep = mask_dict['ids_keep']\n",
    "            ids_restore = mask_dict['ids_restore']\n",
    "            mask = mask_dict['mask']\n",
    "            x = mask_out_token(x, ids_keep)\n",
    "        \n",
    "        if self.use_patch_mixer:\n",
    "            # Project mixer out to backbone transformer dim (do after masking to save compute)\n",
    "            x = self.patch_mixer_map_xout(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, y, t)  # (N, T, D)\n",
    "        \n",
    "        x = self.final_layer(x, t)  # (N, T, patch_size ** 2 * out_channels)\n",
    "        \n",
    "        if mask_ratio > 0:\n",
    "            x = unmask_tokens(x, ids_restore, self.mask_token)\n",
    "\n",
    "        x = self.unpatchify(x)  # (N, out_channels, H, W)\n",
    "        return {'sample': x, 'mask': mask}\n",
    "    \n",
    "    def forward_with_cfg(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        cfg: float = 1.0,\n",
    "        mask_ratio: float = 0,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"Forward pass with classifier-free guidance.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, channels, height, width)\n",
    "            t: Timestep tensor of shape (batch_size,)\n",
    "            y: Caption embedding tensor of shape (batch_size, 1, seq_len, dim)\n",
    "            cfg: Classifier-free guidance scale (1.0 means no guidance)\n",
    "            mask_ratio: Ratio of patches to mask during training (between 0 and 1)\n",
    "\n",
    "        Returns:\n",
    "            dict: Dict with output tensor of shape (batch_size, out_channels, height, width)\n",
    "        \"\"\"\n",
    "        x = torch.cat([x, x], 0)\n",
    "        y = torch.cat([y, torch.zeros_like(y)], 0)\n",
    "        if len(t) != 1:\n",
    "            t = torch.cat([t, t], 0)\n",
    "        \n",
    "        eps = self.forward_without_cfg(x, t, y, mask_ratio, **kwargs)['sample']\n",
    "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
    "        eps = uncond_eps + cfg * (cond_eps - uncond_eps)\n",
    "        return {'sample': eps}\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        cfg: float = 1.0,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"Routes to appropriate forward pass based on classifier-free guidance value.\"\"\"\n",
    "        if cfg != 1.0:\n",
    "            return self.forward_with_cfg(x, t, y, cfg, **kwargs)\n",
    "        else:\n",
    "            return self.forward_without_cfg(x, t, y, **kwargs)\n",
    "\n",
    "    def unpatchify(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Reverses the patch embedding process to reconstruct the original image dimensions.\"\"\"\n",
    "        c = self.out_channels\n",
    "        p = self.x_embedder.patch_size[0]\n",
    "        h = w = int(x.shape[1]**0.5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        return x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        \"\"\"Initialize model weights with custom initialization scheme.\"\"\"\n",
    "        def zero_bias(m: nn.Module) -> None:\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        def _basic_init(module: nn.Module) -> None:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                zero_bias(module)  # All bias in the model init to zero\n",
    "\n",
    "        # Baseline init of all parameters\n",
    "        self.apply(_basic_init)\n",
    "\n",
    "        pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.pos_embed.shape[-1],\n",
    "            int(self.x_embedder.num_patches**0.5),\n",
    "            pos_interp_scale=self.pos_interp_scale,\n",
    "            base_size=self.base_size\n",
    "        )\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        w = self.x_embedder.proj.weight.data\n",
    "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
    "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
    "        nn.init.normal_(self.pooled_y_emb_process.fc1.weight, std=0.02)\n",
    "        nn.init.normal_(self.pooled_y_emb_process.fc2.weight, std=0.02)\n",
    "        nn.init.normal_(self.y_embedder.y_proj.fc1.weight, std=0.02)\n",
    "        nn.init.normal_(self.y_embedder.y_proj.fc2.weight, std=0.02)\n",
    "        \n",
    "        # Custom init of blocks\n",
    "        for block in self.blocks:\n",
    "            block.custom_init()\n",
    "        for block in self.patch_mixer:\n",
    "            block.custom_init()\n",
    "\n",
    "        for block in self.blocks:\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
    "\n",
    "        for block in self.patch_mixer:\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
    "\n",
    "        self.y_emb_preprocess.custom_init()\n",
    "        nn.init.constant_(self.y_emb_preprocess.attn.proj.weight, 0)\n",
    "        nn.init.constant_(self.y_emb_preprocess.mlp.w3.weight, 0)\n",
    "\n",
    "        # Zero-out output layers\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.weight, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MicroDiT_Tiny_2(\n",
    "    caption_channels: int = 1024,\n",
    "    qkv_ratio: List[float] = [0.5, 1.0],\n",
    "    mlp_ratio: List[float] = [0.5, 4.0],\n",
    "    pos_interp_scale: float = 1.0,\n",
    "    input_size: int = 32,\n",
    "    num_experts: int = 8,\n",
    "    expert_capacity: float = 2.0,\n",
    "    experts_every_n: int = 2,\n",
    "    in_channels: int = 4,\n",
    "    **kwargs\n",
    ") -> DiT:\n",
    "    depth = 16\n",
    "    model = DiT(\n",
    "        input_size=input_size,\n",
    "        patch_size=2,\n",
    "        in_channels=in_channels,\n",
    "        dim=512,\n",
    "        depth=depth,\n",
    "        head_dim=32,\n",
    "        multiple_of=256,\n",
    "        caption_channels=caption_channels,\n",
    "        pos_interp_scale=pos_interp_scale,\n",
    "        norm_eps=1e-6,\n",
    "        depth_init=True,\n",
    "        qkv_multipliers=np.linspace(qkv_ratio[0], qkv_ratio[1], num=depth, dtype=float),\n",
    "        ffn_multipliers=np.linspace(mlp_ratio[0], mlp_ratio[1], num=depth, dtype=float),\n",
    "        use_patch_mixer=True,\n",
    "        patch_mixer_depth=4,\n",
    "        patch_mixer_dim=512,  # allocating higher budget to mixer layers\n",
    "        patch_mixer_qkv_ratio=1.0,\n",
    "        patch_mixer_mlp_ratio=4.0,\n",
    "        use_bias=False,\n",
    "        num_experts=num_experts,\n",
    "        expert_capacity=expert_capacity,\n",
    "        experts_every_n=experts_every_n,\n",
    "        **kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MicroDiT_XL_2(\n",
    "    caption_channels: int = 1024,\n",
    "    qkv_ratio: List[float] = [0.5, 1.0],\n",
    "    mlp_ratio: List[float] = [0.5, 4.0],\n",
    "    pos_interp_scale: float = 1.0,\n",
    "    input_size: int = 32,\n",
    "    num_experts: int = 8,\n",
    "    expert_capacity: float = 2.0,\n",
    "    experts_every_n: int = 2,\n",
    "    in_channels: int = 4,\n",
    "    **kwargs\n",
    ") -> DiT:\n",
    "    depth = 28\n",
    "    model = DiT(\n",
    "        input_size=input_size,\n",
    "        patch_size=2,\n",
    "        in_channels=in_channels,\n",
    "        dim=1024,\n",
    "        depth=depth,\n",
    "        head_dim=64,\n",
    "        multiple_of=256,\n",
    "        caption_channels=caption_channels,\n",
    "        pos_interp_scale=pos_interp_scale,\n",
    "        norm_eps=1e-6,\n",
    "        depth_init=True,\n",
    "        qkv_multipliers=np.linspace(qkv_ratio[0], qkv_ratio[1], num=depth, dtype=float),\n",
    "        ffn_multipliers=np.linspace(mlp_ratio[0], mlp_ratio[1], num=depth, dtype=float),\n",
    "        use_patch_mixer=True,\n",
    "        patch_mixer_depth=6,\n",
    "        patch_mixer_dim=768,  # allocating higher budget to mixer layers\n",
    "        patch_mixer_qkv_ratio=1.0,\n",
    "        patch_mixer_mlp_ratio=4.0,\n",
    "        use_bias=False,\n",
    "        num_experts=num_experts,\n",
    "        expert_capacity=expert_capacity,\n",
    "        experts_every_n=experts_every_n,\n",
    "        **kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3034baa6",
   "metadata": {},
   "source": [
    "## LatentDiffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610757c9",
   "metadata": {},
   "source": [
    "### UniversalTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ae948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_2_hf_tokenizer_wrapper:\n",
    "    \"\"\"Simple wrapper to make OpenCLIP tokenizer match HuggingFace interface.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (Any): OpenCLIP tokenizer instance\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: Any):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_max_length = self.tokenizer.context_length\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        caption: str,\n",
    "        padding: str = 'max_length',\n",
    "        max_length: Optional[int] = None,\n",
    "        truncation: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        return {'input_ids': self.tokenizer(caption, context_length=max_length)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalTokenizer:\n",
    "    \"\"\"Universal tokenizer supporting multiple model types.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Name/path of the tokenizer to load\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        s, d = text_encoder_embedding_format(name)\n",
    "        if self.name.startswith(\"openclip:\"):\n",
    "            self.tokenizer = simple_2_hf_tokenizer_wrapper(\n",
    "                open_clip.get_tokenizer(name.lstrip('openclip:'))\n",
    "            )\n",
    "            assert s == self.tokenizer.model_max_length, \"simply check of text_encoder_embedding_format\"\n",
    "        elif self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(name) # for t5 we would use a smaller than max_seq_length\n",
    "        else:\n",
    "            self.tokenizer = CLIPTokenizer.from_pretrained(name, subfolder='tokenizer')\n",
    "            assert s == self.tokenizer.model_max_length, \"simply check of text_encoder_embedding_format\"\n",
    "        self.model_max_length = s\n",
    "        \n",
    "    def tokenize(self, captions: Union[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
    "        if self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            text_tokens_and_mask = self.tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                max_length=self.model_max_length,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': text_tokens_and_mask['input_ids'],\n",
    "                'attention_mask': text_tokens_and_mask['attention_mask']\n",
    "            }\n",
    "        else:\n",
    "            # Avoid attention mask for CLIP tokenizers as they are not used\n",
    "            tokenized_caption = self.tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )['input_ids']\n",
    "            return {'input_ids': tokenized_caption}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b884a",
   "metadata": {},
   "source": [
    "### UniversalTextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class openclip_text_encoder(torch.nn.Module):\n",
    "    \"\"\"OpenCLIP text encoder wrapper.\n",
    "    \n",
    "    Args:\n",
    "        clip_model (Any): OpenCLIP model instance\n",
    "        dtype (torch.dtype, torch.float32): Data type for model weights\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model: Any, dtype: torch.dtype = torch.float32, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.device = None\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward_fxn(self, text: torch.Tensor) -> Tuple[torch.Tensor, None]:\n",
    "        cast_dtype = self.clip_model.transformer.get_cast_dtype()\n",
    "        x = self.clip_model.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + self.clip_model.positional_embedding.to(cast_dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒžã‚¹ã‚¯ã‚’å¤–ã™\n",
    "        # x = self.clip_model.transformer(x, attn_mask=self.clip_model.attn_mask)\n",
    "        x = self.clip_model.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip_model.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        x = x.unsqueeze(dim=1) # [batch_size, 1, n_ctx, transformer.width] expected for text_emb\n",
    "        return x, None # HF encoders expected to return multiple values with first being text emb\n",
    "\n",
    "    def forward(self, text: torch.Tensor, **kwargs) -> Tuple[torch.Tensor, None]:\n",
    "        with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "            return self.forward_fxn(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fa1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalTextEncoder(torch.nn.Module):\n",
    "    \"\"\"Universal text encoder supporting multiple model types.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Name/path of the model to load\n",
    "        dtype (str): Data type for model weights\n",
    "        pretrained (bool, True): Whether to load pretrained weights\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, dtype: str, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        if self.name.startswith(\"openclip:\"):\n",
    "            assert pretrained, 'Load default pretrained model from openclip'\n",
    "            self.encoder = openclip_text_encoder(\n",
    "                open_clip.create_model_and_transforms(name.lstrip('openclip:'))[0],\n",
    "                torch_dtype=DATA_TYPES[dtype]\n",
    "            )\n",
    "        elif self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            self.encoder = T5EncoderModel.from_pretrained(\n",
    "                name,\n",
    "                torch_dtype=DATA_TYPES[dtype],\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = CLIPTextModel.from_pretrained(\n",
    "                name,\n",
    "                subfolder='text_encoder',\n",
    "                torch_dtype=DATA_TYPES[dtype],\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "\n",
    "    def encode(self, tokenized_caption: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        if self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            out = self.encoder(\n",
    "                tokenized_caption,\n",
    "                attention_mask=attention_mask\n",
    "            )['last_hidden_state']\n",
    "            out = out.unsqueeze(dim=1)\n",
    "            return out, None\n",
    "        else:\n",
    "            return self.encoder(tokenized_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbf783",
   "metadata": {},
   "source": [
    "### DistLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistLoss(Metric):\n",
    "    \"\"\"Distributed loss metric.\n",
    "    \n",
    "    Args:\n",
    "        kwargs (Any): Additional arguments passed to parent class\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.add_state(\"loss\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"batches\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, value: torch.Tensor) -> None:\n",
    "        self.loss += value\n",
    "        self.batches += 1\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.loss.float() / self.batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e94ed",
   "metadata": {},
   "source": [
    "### LatentDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e95056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusion(ComposerModel):\n",
    "    \"\"\"Latent diffusion model that generates images from text prompts.\n",
    "\n",
    "    This model combines a DiT (Diffusion Transformer) model for denoising image latents,\n",
    "    a VAE for encoding/decoding images to/from the latent space, and a text encoder\n",
    "    for converting text prompts into embeddings. It implements the EDM (Elucidated\n",
    "    Diffusion Model) sampling process.\n",
    "\n",
    "    Args:\n",
    "        dit (nn.Module): Diffusion Transformer model\n",
    "        vae (AutoencoderKL): VAE model from diffusers for encoding/decoding images\n",
    "        text_encoder (UniversalTextEncoder): Text encoder for converting prompts to embeddings\n",
    "        tokenizer (UniversalTokenizer): Tokenizer for processing text prompts\n",
    "        image_key (str, optional): Key for image data in batch dict. Defaults to 'image'.\n",
    "        text_key (str, optional): Key for text data in batch dict. Defaults to 'captions'.\n",
    "        image_latents_key (str, optional): Key for precomputed image latents in batch dict. Defaults to 'image_latents'.\n",
    "        text_latents_key (str, optional): Key for precomputed text latents in batch dict. Defaults to 'caption_latents'.\n",
    "        precomputed_latents (bool, optional): Whether to use precomputed latents (must be in the batch). Defaults to True.\n",
    "        dtype (str, optional): Data type for model ops. Defaults to 'bfloat16'.\n",
    "        latent_res (int, optional): Resolution of latent space assuming 8x downsampling by VAE. Defaults to 32.\n",
    "        p_mean (float, optional): EDM log-normal noise mean. Defaults to -0.6.\n",
    "        p_std (float, optional): EDM log-normal noise standard-deviation. Defaults to 1.2.\n",
    "        train_mask_ratio (float, optional): Ratio of patches to mask during training. Defaults to 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dit: nn.Module,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: UniversalTextEncoder,\n",
    "        tokenizer: UniversalTokenizer,\n",
    "        image_key: str = 'image',\n",
    "        text_key: str = 'captions',\n",
    "        image_latents_key: str = 'image_latents',\n",
    "        text_latents_key: str = 'caption_latents',\n",
    "        precomputed_latents: bool = True,\n",
    "        dtype: str = 'bfloat16',\n",
    "        latent_res: int = 32,\n",
    "        p_mean: float = -0.6,\n",
    "        p_std: float = 1.2,\n",
    "        train_mask_ratio: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dit = dit\n",
    "        self.vae = vae\n",
    "        self.image_key = image_key\n",
    "        self.text_key = text_key\n",
    "        self.image_latents_key = image_latents_key\n",
    "        self.text_latents_key = text_latents_key\n",
    "        self.precomputed_latents = precomputed_latents\n",
    "        self.dtype = dtype\n",
    "        self.latent_res = latent_res\n",
    "        self.edm_config = EasyDict({\n",
    "            'sigma_min': 0.002,\n",
    "            'sigma_max': 80,\n",
    "            'P_mean': p_mean,\n",
    "            'P_std': p_std,\n",
    "            'sigma_data': 0.9,\n",
    "            'num_steps': 18,\n",
    "            'rho': 7,\n",
    "            'S_churn': 0,\n",
    "            'S_min': 0,\n",
    "            'S_max': float('inf'),\n",
    "            'S_noise': 1\n",
    "        })\n",
    "        self.train_mask_ratio = train_mask_ratio\n",
    "        self.eval_mask_ratio = 0.  # no masking during sampling/evaluation\n",
    "        assert self.train_mask_ratio >= 0, 'Masking ratio must be non-negative!'\n",
    "\n",
    "        self.randn_like = torch.randn_like\n",
    "        self.latent_scale = self.vae.config.scaling_factor\n",
    "\n",
    "        self.text_encoder = text_encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        # freeze vae and text_encoder during training\n",
    "        self.text_encoder.requires_grad_(False)\n",
    "        self.vae.requires_grad_(False)\n",
    "        # avoid wrapping the models that we aren't training\n",
    "        self.text_encoder._fsdp_wrap = False\n",
    "        self.vae._fsdp_wrap = False\n",
    "        self.dit._fsdp_wrap = True\n",
    "\n",
    "    def forward(self, batch: dict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Get image latents\n",
    "        if self.precomputed_latents and self.image_latents_key in batch:\n",
    "            # Assuming that latents have already been scaled, i.e., multiplied with the scaling factor\n",
    "            latents = batch[self.image_latents_key]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                images = batch[self.image_key]\n",
    "                latents = self.vae.encode(\n",
    "                    images.to(DATA_TYPES[self.dtype])\n",
    "                )['latent_dist'].sample().data\n",
    "                latents *= self.latent_scale\n",
    "\n",
    "        # Get text embeddings\n",
    "        if self.precomputed_latents and self.text_latents_key in batch:\n",
    "            conditioning = batch[self.text_latents_key]\n",
    "        else:\n",
    "            captions = batch[self.text_key]\n",
    "            captions = captions.view(-1, captions.shape[-1])\n",
    "            if 'attention_mask' in batch:\n",
    "                conditioning = self.text_encoder.encode(\n",
    "                    captions,\n",
    "                    attention_mask=batch['attention_mask'].view(-1, captions.shape[-1])\n",
    "                )[0]\n",
    "            else:\n",
    "                conditioning = self.text_encoder.encode(captions)[0]\n",
    "\n",
    "        # Zero out dropped captions. Needed for classifier-free guidance during inference.\n",
    "        if 'drop_caption_mask' in batch.keys():\n",
    "            conditioning *= batch['drop_caption_mask'].view(\n",
    "                [-1] + [1] * (len(conditioning.shape) - 1)\n",
    "            )\n",
    "\n",
    "        loss = self.edm_loss(\n",
    "            latents.float(),\n",
    "            conditioning.float(),\n",
    "            mask_ratio=self.train_mask_ratio if self.training else self.eval_mask_ratio\n",
    "        )\n",
    "        return (loss, latents, conditioning)\n",
    "\n",
    "    def model_forward_wrapper(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        sigma: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        model_forward_fxn: callable,\n",
    "        mask_ratio: float,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"Wrapper for the model call in EDM (https://github.com/NVlabs/edm/blob/main/training/networks.py#L632)\"\"\"\n",
    "        sigma = sigma.to(x.dtype).reshape(-1, 1, 1, 1)\n",
    "        c_skip = (\n",
    "            self.edm_config.sigma_data ** 2 /\n",
    "            (sigma ** 2 + self.edm_config.sigma_data ** 2)\n",
    "        )\n",
    "        c_out = (\n",
    "            sigma * self.edm_config.sigma_data /\n",
    "            (sigma ** 2 + self.edm_config.sigma_data ** 2).sqrt()\n",
    "        )\n",
    "        c_in = 1 / (self.edm_config.sigma_data ** 2 + sigma ** 2).sqrt()\n",
    "        c_noise = sigma.log() / 4\n",
    "\n",
    "        out = model_forward_fxn(\n",
    "            (c_in * x).to(x.dtype),\n",
    "            c_noise.flatten(),\n",
    "            y,\n",
    "            mask_ratio=mask_ratio,\n",
    "            **kwargs\n",
    "        )\n",
    "        F_x = out['sample']\n",
    "        c_skip = c_skip.to(F_x.device)\n",
    "        x = x.to(F_x.device)\n",
    "        c_out = c_out.to(F_x.device)\n",
    "        D_x = c_skip * x + c_out * F_x\n",
    "        out['sample'] = D_x\n",
    "        return out\n",
    "\n",
    "    def edm_loss(self, x: torch.Tensor, y: torch.Tensor, mask_ratio: float = 0, **kwargs) -> torch.Tensor:\n",
    "        rnd_normal = torch.randn([x.shape[0], 1, 1, 1], device=x.device)\n",
    "        sigma = (rnd_normal * self.edm_config.P_std + self.edm_config.P_mean).exp()\n",
    "        weight = (\n",
    "            (sigma ** 2 + self.edm_config.sigma_data ** 2) /\n",
    "            (sigma * self.edm_config.sigma_data) ** 2\n",
    "        )\n",
    "        n = self.randn_like(x) * sigma\n",
    "\n",
    "        model_out = self.model_forward_wrapper(\n",
    "            x + n,\n",
    "            sigma,\n",
    "            y,\n",
    "            self.dit,\n",
    "            mask_ratio=mask_ratio,\n",
    "            **kwargs\n",
    "        )\n",
    "        D_xn = model_out['sample']\n",
    "        loss = weight * ((D_xn - x) ** 2)  # (N, C, H, W)\n",
    "\n",
    "        if mask_ratio > 0:\n",
    "            # Masking is not feasible during image generation as it only returns denoised version\n",
    "            # for non-masked patches. Image generation requires all patches to be denoised.\n",
    "            assert (\n",
    "                self.dit.training and 'mask' in model_out\n",
    "            ), 'Masking is only recommended during training'\n",
    "            loss = F.avg_pool2d(loss.mean(dim=1), self.dit.patch_size).flatten(1)\n",
    "            unmask = 1 - model_out['mask']\n",
    "            loss = (loss * unmask).sum(dim=1) / unmask.sum(dim=1)  # (N,)\n",
    "        return loss.mean()\n",
    "\n",
    "    # Composer specific formatting of model loss and eval functions.\n",
    "    def loss(self, outputs: tuple, batch: dict) -> torch.Tensor:\n",
    "        # forward pass already computed the loss function\n",
    "        return outputs[0]\n",
    "\n",
    "    def eval_forward(self, batch: dict, outputs: Optional[tuple] = None) -> tuple:\n",
    "        # Skip if output already calculated (e.g., during training forward pass)\n",
    "        if outputs is not None:\n",
    "            return outputs\n",
    "        loss, _, _ = self.forward(batch)\n",
    "        return loss, None, None\n",
    "\n",
    "    def get_metrics(self, is_train: bool = False) -> dict:\n",
    "        # get_metrics expected to return a dict in composer\n",
    "        return {'loss': DistLoss()}\n",
    "\n",
    "    def update_metric(self, batch: dict, outputs: tuple, metric: DistLoss):\n",
    "        metric.update(outputs[0])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def edm_sampler_loop(\n",
    "        self, x: torch.Tensor, \n",
    "        y: torch.Tensor, \n",
    "        steps: Optional[int] = None, \n",
    "        cfg: float = 1.0, \n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        mask_ratio = 0  # no masking during image generation\n",
    "        model_forward_fxn = (\n",
    "            partial(self.dit.forward, cfg=cfg) if cfg > 1.0\n",
    "            else self.dit.forward\n",
    "        )\n",
    "\n",
    "        # Time step discretization.\n",
    "        num_steps = self.edm_config.num_steps if steps is None else steps\n",
    "        step_indices = torch.arange(num_steps, dtype=torch.float64, device=x.device)\n",
    "        t_steps = (\n",
    "            self.edm_config.sigma_max ** (1 / self.edm_config.rho) +\n",
    "            step_indices / (num_steps - 1) *\n",
    "            (self.edm_config.sigma_min ** (1 / self.edm_config.rho) -\n",
    "             self.edm_config.sigma_max ** (1 / self.edm_config.rho))\n",
    "        ) ** self.edm_config.rho\n",
    "        t_steps = torch.cat([torch.as_tensor(t_steps), torch.zeros_like(t_steps[:1])])\n",
    "\n",
    "        # Main sampling loop.\n",
    "        x_next = x.to(torch.float64) * t_steps[0]\n",
    "        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):  # 0, ..., N-1\n",
    "            x_cur = x_next\n",
    "            # Increase noise temporarily.\n",
    "            gamma = (\n",
    "                min(self.edm_config.S_churn / num_steps, np.sqrt(2) - 1)\n",
    "                if self.edm_config.S_min <= t_cur <= self.edm_config.S_max else 0\n",
    "            )\n",
    "            t_hat = torch.as_tensor(t_cur + gamma * t_cur)\n",
    "            x_hat = (\n",
    "                x_cur +\n",
    "                (t_hat ** 2 - t_cur ** 2).sqrt() *\n",
    "                self.edm_config.S_noise *\n",
    "                self.randn_like(x_cur)\n",
    "            )\n",
    "\n",
    "            # Euler step.\n",
    "            denoised = self.model_forward_wrapper(\n",
    "                x_hat.to(torch.float32),\n",
    "                t_hat.to(torch.float32),\n",
    "                y,\n",
    "                model_forward_fxn,\n",
    "                mask_ratio=mask_ratio,\n",
    "                **kwargs\n",
    "            )['sample'].to(torch.float64)\n",
    "            d_cur = (x_hat - denoised) / t_hat\n",
    "            x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "\n",
    "            # Apply 2nd order correction.\n",
    "            if i < num_steps - 1:\n",
    "                denoised = self.model_forward_wrapper(\n",
    "                    x_next.to(torch.float32),\n",
    "                    t_next.to(torch.float32),\n",
    "                    y,\n",
    "                    model_forward_fxn,\n",
    "                    mask_ratio=mask_ratio,\n",
    "                    **kwargs\n",
    "                )['sample'].to(torch.float64)\n",
    "                d_prime = (x_next - denoised) / t_next\n",
    "                x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "        return x_next.to(torch.float32)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Optional[list] = None,\n",
    "        tokenized_prompts: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        guidance_scale: Optional[float] = 5.0,\n",
    "        num_inference_steps: Optional[int] = 30,\n",
    "        seed: Optional[int] = None,\n",
    "        return_only_latents: Optional[bool] = False,\n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        # _check_prompt_given(prompt, tokenized_prompts, prompt_embeds=None)\n",
    "        assert prompt or tokenized_prompts, \"Must provide either prompt or tokenized prompts\"\n",
    "        device = self.vae.device  # hack to identify model device during training\n",
    "        rng_generator = torch.Generator(device=device)\n",
    "        if seed:\n",
    "            rng_generator = rng_generator.manual_seed(seed)\n",
    "\n",
    "        # Convert prompt text to embeddings (zero out embeddings for classifier-free guidance)\n",
    "        if tokenized_prompts is None:\n",
    "            out = self.tokenizer.tokenize(prompt)\n",
    "            tokenized_prompts = out['input_ids']\n",
    "            attention_mask = (\n",
    "                out['attention_mask'] if 'attention_mask' in out else None\n",
    "            )\n",
    "        text_embeddings = self.text_encoder.encode(\n",
    "            tokenized_prompts.to(device),\n",
    "            attention_mask=attention_mask.to(device) if attention_mask is not None else None\n",
    "        )[0]\n",
    "\n",
    "        latents = torch.randn(\n",
    "            (len(text_embeddings), self.dit.in_channels, self.latent_res, self.latent_res),\n",
    "            device=device,\n",
    "            generator=rng_generator,\n",
    "        )\n",
    "\n",
    "        # iteratively denoise latents\n",
    "        latents = self.edm_sampler_loop(\n",
    "            latents,\n",
    "            text_embeddings,\n",
    "            num_inference_steps,\n",
    "            cfg=guidance_scale\n",
    "        )\n",
    "\n",
    "        if return_only_latents:\n",
    "            return latents\n",
    "\n",
    "        # Decode latents with VAE\n",
    "        latents = 1 / self.latent_scale * latents\n",
    "        torch_dtype = DATA_TYPES[self.dtype]\n",
    "        image = self.vae.decode(latents.to(torch_dtype)).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.float().detach()\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder_embedding_format(enc: str) -> Tuple[int, int]:\n",
    "    \"\"\"Returns sequence length and token embedding dimension for text encoder.\"\"\"\n",
    "    if enc in [\n",
    "        'stabilityai/stable-diffusion-2-base',\n",
    "        'runwayml/stable-diffusion-v1-5',\n",
    "        'CompVis/stable-diffusion-v1-4'\n",
    "    ]:\n",
    "        return 77, 1024\n",
    "    if enc in ['openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378']:\n",
    "        return 77, 1024\n",
    "    if enc in [\"DeepFloyd/t5-v1_1-xxl\"]:\n",
    "        return 120, 4096\n",
    "    raise ValueError(f'Please specifcy the sequence and embedding size of {enc} encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0720cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latent_diffusion(\n",
    "    vae_name: str = 'stabilityai/stable-diffusion-xl-base-1.0',\n",
    "    text_encoder_name: str = 'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378', \n",
    "    dit_arch: str = 'MicroDiT_XL_2',\n",
    "    latent_res: int = 32,\n",
    "    in_channels: int = 4,\n",
    "    pos_interp_scale: float = 1.0,\n",
    "    dtype: str = 'bfloat16',\n",
    "    precomputed_latents: bool = True,\n",
    "    p_mean: float = -0.6,\n",
    "    p_std: float = 1.2,\n",
    "    train_mask_ratio: float = 0.\n",
    ") -> LatentDiffusion:\n",
    "    # retrieve max sequence length (s) and token embedding dim (d) from text encoder\n",
    "    s, d = text_encoder_embedding_format(text_encoder_name)\n",
    "\n",
    "    if dit_arch == 'MicroDiT_Tiny_2':\n",
    "        dit = MicroDiT_Tiny_2(\n",
    "            input_size=latent_res,\n",
    "            caption_channels=d,\n",
    "            pos_interp_scale=pos_interp_scale,\n",
    "            in_channels=in_channels\n",
    "        )\n",
    "    elif dit_arch == 'MicroDiT_XL_2':\n",
    "        dit = MicroDiT_XL_2(\n",
    "            input_size=latent_res,\n",
    "            caption_channels=d,\n",
    "            pos_interp_scale=pos_interp_scale,\n",
    "            in_channels=in_channels\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unknown DiT architecture: {dit_arch}')\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        vae_name,\n",
    "        subfolder=None if vae_name=='ostris/vae-kl-f8-d16' else 'vae',\n",
    "        torch_dtype=DATA_TYPES[dtype],\n",
    "        pretrained=True\n",
    "    )\n",
    "\n",
    "    text_encoder = UniversalTextEncoder(\n",
    "        text_encoder_name,\n",
    "        dtype=dtype,\n",
    "        pretrained=True\n",
    "    )\n",
    "    tokenizer = UniversalTokenizer(text_encoder_name)\n",
    "\n",
    "    model = LatentDiffusion(\n",
    "        dit=dit,\n",
    "        vae=vae,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        precomputed_latents=precomputed_latents,\n",
    "        dtype=dtype,\n",
    "        latent_res=latent_res,\n",
    "        p_mean=p_mean,\n",
    "        p_std=p_std,\n",
    "        train_mask_ratio=train_mask_ratio\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada8eda",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2228d7",
   "metadata": {},
   "source": [
    "### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_metadata():\n",
    "    # Only using a single process for downloading metadata\n",
    "    metadata_files = [\n",
    "        ('data/train', 'train_anno.jsonl.tgz'),\n",
    "        ('data/train', 'train_anno_realease_repath.jsonl.tgz'),\n",
    "        ('data/valid', 'valid_anno_repath.jsonl.tgz'),\n",
    "        ('data/test', 'test_questions.jsonl.tgz'),\n",
    "        ('data/test', 'imgs.tgz'),\n",
    "    ]\n",
    "\n",
    "    for subfolder, filename in metadata_files:\n",
    "        hf_hub_download(\n",
    "            repo_id=\"JourneyDB/JourneyDB\",\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=subfolder,\n",
    "            filename=filename,\n",
    "            local_dir=COMPRESSED_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "\n",
    "    metadata_tars = [\n",
    "        os.path.join(dir, fname) for (dir, fname) in metadata_files\n",
    "    ]\n",
    "\n",
    "    for tar_file in metadata_tars:\n",
    "        subprocess.call(\n",
    "            f'tar -xvzf {os.path.join(COMPRESSED_DIR, tar_file)} '\n",
    "            f'-C {os.path.join(COMPRESSED_DIR, os.path.dirname(tar_file))}',\n",
    "            shell=True,\n",
    "        )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/train/train_anno_realease_repath.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"train/train_anno_realease_repath.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/valid/valid_anno_repath.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"valid/valid_anno_repath.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/test/test_questions.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"test/test_questions.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.move(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/test/imgs\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"test/\")}',\n",
    "    )\n",
    "\n",
    "COMPRESSED_DIR = os.path.join(DATA_DIR, 'compressed')\n",
    "logger.info(f\"{COMPRESSED_DIR=}\")\n",
    "\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "logger.info(f\"{RAW_DIR=}\")\n",
    "\n",
    "if not os.path.exists(COMPRESSED_DIR):\n",
    "    os.makedirs(COMPRESSED_DIR, exist_ok=True)\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "    download_and_process_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737dbd3",
   "metadata": {},
   "source": [
    "### è§£å‡ã¨ãƒªã‚µã‚¤ã‚º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df247d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_uncompress_resize(\n",
    "    valid_ids: list,\n",
    "    max_image_size: int,\n",
    "    min_image_size: int,\n",
    "    split: str,\n",
    "    idx: int,\n",
    "):\n",
    "    \"\"\"Download, uncompress, and resize images for a given archive index.\"\"\"\n",
    "    assert split in ('train', 'valid')\n",
    "    assert idx in valid_ids\n",
    "\n",
    "    print(f\"Downloading idx: {idx}\")\n",
    "    if not os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/'):\n",
    "        hf_hub_download(\n",
    "            repo_id=\"JourneyDB/JourneyDB\",\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=f'data/{split}/imgs',\n",
    "            filename=f'{idx:>03}.tgz',\n",
    "            local_dir=COMPRESSED_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "    print(f\"Downloaded idx: {idx}\")\n",
    "\n",
    "    print(f\"Extracting idx: {idx}\")\n",
    "    if not os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/'):\n",
    "        subprocess.call(\n",
    "            f'tar -xzf {COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz '\n",
    "            f'-C {COMPRESSED_DIR}/data/{split}/imgs/',\n",
    "            shell=True,\n",
    "        )\n",
    "    print(f\"Extracted idx: {idx}\")\n",
    "\n",
    "    print(f\"Removing idx: {idx}\")\n",
    "    if os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz'):\n",
    "        os.remove(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz')\n",
    "    print(f\"Removed idx: {idx}\")\n",
    "\n",
    "    # add bicubic downsize\n",
    "    downsize = transforms.Resize(\n",
    "        max_image_size,\n",
    "        antialias=True,\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    )\n",
    "\n",
    "    print(f\"Downsizing idx: {idx}\")\n",
    "    os.makedirs(\n",
    "        f'{RAW_DIR}/{split}/imgs/{idx:>03}/',\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    for f in iglob(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/*'):\n",
    "        save_path = f'{RAW_DIR}/{split}/imgs/{idx:>03}/{os.path.basename(f)}'\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "            try:\n",
    "                img = Image.open(f)\n",
    "                w, h = img.size\n",
    "                if min(w, h) > max_image_size:\n",
    "                    img = downsize(img)\n",
    "                if min(w, h) < min_image_size:\n",
    "                    print(\n",
    "                        f'Skipping image with resolution ({h}, {w}) - '\n",
    "                        f'Since at least one side has resolution below {min_image_size}'\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                img.save(save_path)\n",
    "                os.remove(f)\n",
    "            except (UnidentifiedImageError, OSError) as e:\n",
    "                print(f\"Error {e}, File: {f}\")\n",
    "    print(f'Downsized idx: {idx}')\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(RAW_DIR, 'train/imgs')):\n",
    "\n",
    "    valid_ids = list(np.arange(1))\n",
    "    pool_args = [('train', i) for i in valid_ids] + [('valid', i) for i in valid_ids]\n",
    "    max_image_size = 512\n",
    "    min_image_size = 64\n",
    "\n",
    "    with Pool(processes=4) as pool:\n",
    "        pool.starmap(\n",
    "            download_uncompress_resize,\n",
    "            [(\n",
    "                valid_ids,\n",
    "                max_image_size,\n",
    "                min_image_size,\n",
    "                split,\n",
    "                idx\n",
    "            ) for split, idx in pool_args]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221dffcd",
   "metadata": {},
   "source": [
    "### MDSå½¢å¼ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50319bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mds(\n",
    "    images_dir: str,\n",
    "    captions_jsonl: str,\n",
    "    local_mds_dir: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    JourneyDBã‚’MDSå½¢å¼ã«å¤‰æ›ã™ã‚‹\n",
    "    MDSï¼ˆMosaic Data Storageï¼‰ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®é«˜é€Ÿã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ\n",
    "\n",
    "    Args:\n",
    "        images_dir: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "        captions_jsonl: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "        local_mds_dir: å‡ºåŠ›MDSãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    columns = {\n",
    "        'width': 'int32',\n",
    "        'height': 'int32',\n",
    "        'jpg': 'jpeg',\n",
    "        'caption': 'str',\n",
    "    }\n",
    "    \n",
    "    writer = MDSWriter(\n",
    "        out=local_mds_dir,\n",
    "        columns=columns,\n",
    "        compression=None,\n",
    "        size_limit=256 * (2**20),\n",
    "        max_workers=64,\n",
    "    )\n",
    "    \n",
    "    # Retrieving achieve indies, in case only a subset of the data is downloaded\n",
    "    valid_archieve_idx = [\n",
    "        os.path.basename(p) for p in glob(os.path.join(images_dir, '*'))\n",
    "    ]\n",
    "    \n",
    "    metadata = list(open(captions_jsonl, 'r'))\n",
    "\n",
    "    for f in tqdm(metadata):\n",
    "        d = json.loads(f)\n",
    "        cap, p = d['prompt'], d['img_path'].strip('./')\n",
    "        \n",
    "        if os.path.dirname(p) not in valid_archieve_idx:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            img = Image.open(os.path.join(images_dir, p))\n",
    "            w, h = img.size\n",
    "            mds_sample = {\n",
    "                'jpg': img,\n",
    "                'caption': cap,\n",
    "                'width': w,\n",
    "                'height': h,\n",
    "            }\n",
    "            writer.write(mds_sample)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Something went wrong in reading caption, \"\n",
    "                f\"skipping writing this sample in mds. Error: {e}\"\n",
    "            )\n",
    "\n",
    "    writer.finish()\n",
    "\n",
    "MDS_DIR = os.path.join(DATA_DIR, 'jdb', 'mds')\n",
    "logger.info(f\"{MDS_DIR=}\")\n",
    "\n",
    "if not os.path.exists(MDS_DIR):\n",
    "    convert_to_mds(\n",
    "        images_dir=os.path.join(RAW_DIR, 'train', 'imgs'),\n",
    "        captions_jsonl=os.path.join(RAW_DIR, 'train', 'train_anno_realease_repath.jsonl'),\n",
    "        local_mds_dir=os.path.join(MDS_DIR, 'train'),\n",
    "    )\n",
    "\n",
    "    convert_to_mds(\n",
    "        images_dir=os.path.join(RAW_DIR, 'valid', 'imgs'),\n",
    "        captions_jsonl=os.path.join(RAW_DIR, 'valid', 'valid_anno_repath.jsonl'),\n",
    "        local_mds_dir=os.path.join(MDS_DIR, 'valid'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912aa78",
   "metadata": {},
   "source": [
    "### å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingJdbDatasetForPreCompute(StreamingDataset):\n",
    "    \"\"\"Streaming dataset that resizes images to user-provided resolutions and tokenizes captions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        streams: Sequence[Stream],\n",
    "        transforms_list: List[Callable],\n",
    "        batch_size: int,\n",
    "        tokenizer_name: str,\n",
    "        shuffle: bool = False,\n",
    "        caption_key: str = 'caption',\n",
    "    ):\n",
    "        super().__init__(\n",
    "            streams=streams,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        self.transforms_list = transforms_list\n",
    "        self.caption_key = caption_key\n",
    "        self.tokenizer = UniversalTokenizer(tokenizer_name)\n",
    "        print(\"Created tokenizer: \", tokenizer_name)\n",
    "        assert self.transforms_list is not None, 'Must provide transforms to resize and center crop images'\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict:\n",
    "        sample = super().__getitem__(index)\n",
    "        ret = {}\n",
    "\n",
    "        out = self.tokenizer.tokenize(sample[self.caption_key])\n",
    "        ret[self.caption_key] = out['input_ids'].clone().detach()\n",
    "        if 'attention_mask' in out:\n",
    "            ret[f'{self.caption_key}_attention_mask'] = out['attention_mask'].clone().detach()\n",
    "\n",
    "        for i, tr in enumerate(self.transforms_list):\n",
    "            img = sample['jpg']\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img = tr(img)\n",
    "            ret[f'image_{i}'] = img\n",
    "\n",
    "        ret['sample'] = sample\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_streaming_jdb_precompute_dataloader(\n",
    "    datadir: Union[List[str], str],\n",
    "    batch_size: int,\n",
    "    resize_sizes: Optional[List[int]] = None,\n",
    "    drop_last: bool = False,\n",
    "    shuffle: bool = True,\n",
    "    caption_key: Optional[str] = None,\n",
    "    tokenizer_name: Optional[str] = None,\n",
    "    **dataloader_kwargs,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Builds a streaming mds dataloader returning multiple image sizes and text captions.\"\"\"\n",
    "    assert resize_sizes is not None, 'Must provide target resolution for image resizing'\n",
    "    datadir = [datadir] if isinstance(datadir, str) else datadir\n",
    "    streams = [Stream(remote=None, local=l) for l in datadir]\n",
    "\n",
    "    transforms_list = []\n",
    "    for resize in resize_sizes:\n",
    "        transforms_list.append(\n",
    "            transforms.Compose([\n",
    "                transforms.Resize(\n",
    "                    resize,\n",
    "                    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "                ),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    dataset = StreamingJdbDatasetForPreCompute(\n",
    "        streams=streams,\n",
    "        shuffle=shuffle,\n",
    "        transforms_list=transforms_list,\n",
    "        batch_size=batch_size,\n",
    "        caption_key=caption_key,\n",
    "        tokenizer_name=tokenizer_name,\n",
    "    )\n",
    "\n",
    "    def custom_collate(list_of_dict: List[Dict]) -> Dict:\n",
    "        out = {k: [] for k in list_of_dict[0].keys()}\n",
    "        for d in list_of_dict:\n",
    "            for k, v in d.items():\n",
    "                out[k].append(v)\n",
    "        return out\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=custom_collate,\n",
    "        **dataloader_kwargs,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute(\n",
    "    datadir: str,\n",
    "    savedir: str = \"\",\n",
    "    image_resolutions: list = [256, 512],\n",
    "    save_images: bool = False,\n",
    "    model_dtype: str = \"bfloat16\",\n",
    "    save_dtype: str = \"float16\",\n",
    "    vae: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    text_encoder: str = \"openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\",\n",
    "    batch_size: int = 32,\n",
    "    seed: int = 2024,\n",
    "):\n",
    "    \"\"\"Precompute image and text latents and store them in MDS format.\n",
    "\n",
    "    By default, we only save the image latents for 256x256 and 512x512 image\n",
    "    resolutions (using center crop).\n",
    "\n",
    "    Note that the image latents will be scaled by the vae_scaling_factor.\n",
    "    \"\"\"\n",
    "    cap_key = 'caption'  # Hardcoding the image caption key to 'caption' in MDS dataset\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    device_idx = int(accelerator.process_index)\n",
    "\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(device_idx + seed)\n",
    "    torch.cuda.manual_seed(device_idx + seed)\n",
    "    np.random.seed(device_idx + seed)\n",
    "\n",
    "    dataloader = build_streaming_jdb_precompute_dataloader(\n",
    "        datadir=[datadir],\n",
    "        batch_size=batch_size,\n",
    "        resize_sizes=image_resolutions,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        caption_key=cap_key,\n",
    "        tokenizer_name=text_encoder,\n",
    "        # prefetch_factor=2,\n",
    "        # num_workers=2,\n",
    "        # persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f'Device: {device_idx}, Dataloader sample count: {len(dataloader.dataset)}')\n",
    "\n",
    "    # print(\n",
    "    #     f\"MP variable -> world size: {os.environ['WORLD_SIZE']}, \"\n",
    "    #     f\"RANK: {os.environ['RANK']}, {device}\"\n",
    "    # )\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        vae,\n",
    "        subfolder='vae',  # Change subfolder to appropriate one in hf_hub, if needed\n",
    "        torch_dtype=DATA_TYPES[model_dtype],\n",
    "    )\n",
    "    print(\"Created VAE: \", vae)\n",
    "    assert isinstance(vae, AutoencoderKL)\n",
    "\n",
    "    text_encoder = UniversalTextEncoder(\n",
    "        text_encoder,\n",
    "        dtype=model_dtype,\n",
    "        pretrained=True,\n",
    "    )\n",
    "    print(\"Created text encoder: \", text_encoder)\n",
    "\n",
    "    vae = vae.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "    columns = {\n",
    "        cap_key: 'str',\n",
    "        f'{cap_key}_latents': 'bytes',\n",
    "        'latents_256': 'bytes',\n",
    "        'latents_512': 'bytes',\n",
    "    }\n",
    "    if save_images:\n",
    "        columns['jpg'] = 'jpeg'\n",
    "\n",
    "    remote_upload = os.path.join(savedir, str(accelerator.process_index))\n",
    "\n",
    "    writer = MDSWriter(\n",
    "        out=remote_upload,\n",
    "        columns=columns,\n",
    "        compression=None,\n",
    "        size_limit=256 * (2**20),\n",
    "        max_workers=64,\n",
    "    )\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        image_256 = torch.stack(batch['image_0']).to(device)\n",
    "        image_512 = torch.stack(batch['image_1']).to(device)\n",
    "        captions = torch.stack(batch[cap_key]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type='cuda', dtype=DATA_TYPES[model_dtype]):\n",
    "                latent_dist_256 = vae.encode(image_256)\n",
    "                assert isinstance(latent_dist_256, AutoencoderKLOutput)\n",
    "                latents_256 = (\n",
    "                    latent_dist_256['latent_dist'].sample().data * vae.config.scaling_factor\n",
    "                ).to(DATA_TYPES[save_dtype])\n",
    "\n",
    "                latent_dist_512 = vae.encode(image_512)\n",
    "                assert isinstance(latent_dist_512, AutoencoderKLOutput)\n",
    "                latents_512 = (\n",
    "                    latent_dist_512['latent_dist'].sample().data * vae.config.scaling_factor\n",
    "                ).to(DATA_TYPES[save_dtype])\n",
    "\n",
    "                attention_mask = None\n",
    "\n",
    "                if f'{cap_key}_attention_mask' in batch:\n",
    "                    attention_mask = torch.stack(\n",
    "                        batch[f'{cap_key}_attention_mask']\n",
    "                    ).to(device)\n",
    "\n",
    "                conditioning = text_encoder.encode(\n",
    "                    captions.view(-1, captions.shape[-1]),\n",
    "                    attention_mask=attention_mask,\n",
    "                )[0].to(DATA_TYPES[save_dtype])\n",
    "\n",
    "        try:\n",
    "            if isinstance(latents_256, torch.Tensor) and isinstance(\n",
    "                latents_512, torch.Tensor\n",
    "            ):\n",
    "                latents_256 = latents_256.detach().cpu().numpy()\n",
    "                latents_512 = latents_512.detach().cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if isinstance(conditioning, torch.Tensor):\n",
    "                conditioning = conditioning.detach().cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Write the batch to the MDS file\n",
    "            for i in range(latents_256.shape[0]):\n",
    "                mds_sample = {\n",
    "                    cap_key: batch['sample'][i][cap_key],\n",
    "                    f'{cap_key}_latents': np.reshape(conditioning[i], -1).tobytes(),\n",
    "                    'latents_256': latents_256[i].tobytes(),\n",
    "                    'latents_512': latents_512[i].tobytes(),\n",
    "                }\n",
    "                if save_images:\n",
    "                    mds_sample['jpg'] = batch['sample'][i]['jpg']\n",
    "                writer.write(mds_sample)\n",
    "        except RuntimeError:\n",
    "            print('Runtime error CUDA, skipping this batch')\n",
    "\n",
    "    writer.finish()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    accelerator.wait_for_everyone()\n",
    "    print(f'Process {accelerator.process_index} finished')\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Merge the mds shards created by each device (only do on main process)\n",
    "    if accelerator.is_main_process:\n",
    "        shards_metadata = [\n",
    "            os.path.join(savedir, str(i), 'index.json')\n",
    "            for i in range(accelerator.num_processes)\n",
    "        ]\n",
    "        merge_index(shards_metadata, out=savedir, keep_local=True)\n",
    "\n",
    "PRECOMPUTE_DIR = os.path.join(DATA_DIR, 'mds_latents_sdxl1_dfnclipH14',)\n",
    "logger.info(f\"{PRECOMPUTE_DIR=}\")\n",
    "\n",
    "if not os.path.exists(PRECOMPUTE_DIR):\n",
    "    precompute(\n",
    "        datadir=os.path.join(DATA_DIR, 'jdb', 'mds', 'train'),\n",
    "        savedir=os.path.join(PRECOMPUTE_DIR, 'train'),\n",
    "        image_resolutions=[256, 512],\n",
    "        save_images=False,\n",
    "        model_dtype=\"bfloat16\",\n",
    "        save_dtype=\"float16\",\n",
    "        vae=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        text_encoder=\"openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\",\n",
    "        batch_size=16,\n",
    "        seed=2024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22573ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(PRECOMPUTE_DIR, 'train', 'index.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3c39e",
   "metadata": {},
   "source": [
    "## è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b423a0",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—ï¼‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_latent_diffusion(\n",
    "    vae_name='stabilityai/stable-diffusion-xl-base-1.0',\n",
    "    text_encoder_name='openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378',\n",
    "    # dit_arch='MicroDiT_XL_2',\n",
    "    dit_arch='MicroDiT_Tiny_2',\n",
    "    latent_res=32,\n",
    "    in_channels=4,\n",
    "    pos_interp_scale=1.0,\n",
    "    dtype='bfloat16',\n",
    "    precomputed_latents=True,\n",
    "    p_mean=-0.6,\n",
    "    p_std=1.2,\n",
    "    train_mask_ratio=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f14a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer with special handling for MoE parameters\n",
    "\n",
    "moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "\n",
    "if len(moe_params) > 0:\n",
    "    print('Reducing learning rate of MoE parameters by 1/2')\n",
    "    opt_dict = dict(cfg.optimizer)\n",
    "    opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "    del opt_dict['_target_']\n",
    "    optimizer = getattr(torch.optim, opt_name)(\n",
    "        params=[{'params': rest_params}, {'params': moe_params, 'lr': cfg.optimizer.lr / 2}], **opt_dict)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.dit.parameters(),\n",
    "        lr=2.4e-4,\n",
    "        weight_decay=0.1,\n",
    "        eps=1.0e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "for p in optimizer.param_groups:\n",
    "    p['betas'] = list(p['betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27062f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_seq_size, cap_emb_dim = text_encoder_embedding_format(\n",
    "    'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n",
    ")\n",
    "logger.info(f\"{cap_seq_size=}, {cap_emb_dim=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingLatentsDataset(StreamingDataset):\n",
    "    \"\"\"Dataset class for loading precomputed latents from mds format.\n",
    "    \n",
    "    Args:\n",
    "        streams: List of individual streams (in our case streams of individual datasets)\n",
    "        shuffle: Whether to shuffle the dataset\n",
    "        image_size: Size of images (256 or 512)\n",
    "        cap_seq_size: Context length of text-encoder\n",
    "        cap_emb_dim: Dimension of caption embeddings\n",
    "        cap_drop_prob: Probability of using all zeros caption embedding (classifier-free guidance)\n",
    "        batch_size: Batch size for streaming\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        streams: Optional[List[Stream]] = None,\n",
    "        shuffle: bool = False,\n",
    "        image_size: Optional[int] = None,\n",
    "        cap_seq_size: Optional[int] = None,\n",
    "        cap_emb_dim: Optional[int] = None,\n",
    "        cap_drop_prob: float = 0.0,\n",
    "        batch_size: Optional[int] = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            streams=streams,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.cap_seq_size = cap_seq_size\n",
    "        self.cap_emb_dim = cap_emb_dim\n",
    "        self.cap_drop_prob = cap_drop_prob\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Union[torch.Tensor, str, float]]:\n",
    "        sample = super().__getitem__(index)\n",
    "        out = {}\n",
    "\n",
    "        # Mask for zero'ed out captions in classifier-free guidance (cfg) training.\n",
    "        # We replace caption embeddings with a zero vector in cfg guidance.\n",
    "        out['drop_caption_mask'] = (\n",
    "            0. if torch.rand(1) < self.cap_drop_prob else 1.\n",
    "        )\n",
    "        out['caption_latents'] = torch.from_numpy(\n",
    "            np.frombuffer(sample['caption_latents'], dtype=np.float16)\n",
    "            .copy()\n",
    "        ).reshape(1, self.cap_seq_size, self.cap_emb_dim)\n",
    "\n",
    "        if self.image_size == 256 and 'latents_256' in sample:\n",
    "            out['image_latents'] = torch.from_numpy(\n",
    "                np.frombuffer(sample['latents_256'], dtype=np.float16)\n",
    "                .copy()\n",
    "            ).reshape(-1, 32, 32)\n",
    "\n",
    "        if self.image_size == 512 and 'latents_512' in sample:\n",
    "            out['image_latents'] = torch.from_numpy(\n",
    "                np.frombuffer(sample['latents_512'], dtype=np.float16)\n",
    "                .copy()\n",
    "            ).reshape(-1, 64, 64)\n",
    "\n",
    "        # out['caption'] = sample['caption']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_streaming_latents_dataloader(\n",
    "    datadir: Union[str, List[str]],\n",
    "    batch_size: int,\n",
    "    image_size: int = 256,\n",
    "    cap_seq_size: int = 77,\n",
    "    cap_emb_dim: int = 1024,\n",
    "    cap_drop_prob: float = 0.0,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = True,\n",
    "    **dataloader_kwargs\n",
    ") -> DataLoader:\n",
    "    \"\"\"Creates a DataLoader for streaming latents dataset.\"\"\"\n",
    "    if isinstance(datadir, str):\n",
    "        datadir = [datadir]\n",
    "\n",
    "    streams = [Stream(remote=None, local=d) for d in datadir]\n",
    "\n",
    "    dataset = StreamingLatentsDataset(\n",
    "        streams=streams,\n",
    "        shuffle=shuffle,\n",
    "        image_size=image_size,\n",
    "        cap_seq_size=cap_seq_size,\n",
    "        cap_emb_dim=cap_emb_dim,\n",
    "        cap_drop_prob=cap_drop_prob,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=None,\n",
    "        drop_last=drop_last,\n",
    "        **dataloader_kwargs,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84934954",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_streaming_latents_dataloader(\n",
    "    datadir=['/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/'],\n",
    "    image_size=256,\n",
    "    batch_size=128 // dist.get_world_size(), # 2048\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    cap_drop_prob=0.1,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "num_train_dataset_images = len(train_loader.dataset) * dist.get_world_size()\n",
    "logger.info(f\"{num_train_dataset_images=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db877f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = build_streaming_latents_dataloader(\n",
    "    datadir='/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/',\n",
    "    image_size=256,\n",
    "    batch_size=1024 // dist.get_world_size(),\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "num_eval_dataset_images = len(eval_loader.dataset) * dist.get_world_size()\n",
    "logger.info(f\"{num_eval_dataset_images=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725deb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "loggers.append(TensorboardLogger())\n",
    "\n",
    "exp_name = 'MicroDiTXL_mask_75_res_256_pretrain'\n",
    "wandb_logger = WandBLogger(\n",
    "    init_kwargs={\n",
    "        'name': f'{exp_name}',\n",
    "        'project': 'microdit_training',  # insert wandb project name\n",
    "        'group': f'{exp_name}'\n",
    "    }\n",
    ")\n",
    "# loggers.append(wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_low_precision_layernorm(\n",
    "    model=model.dit,\n",
    "    precision=Precision('amp_bf16'),\n",
    "    optimizers=optimizer\n",
    ")\n",
    "\n",
    "algorithms = []\n",
    "algorithms.append(\n",
    "    GradientClipping(clipping_type='norm', clipping_threshold=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca63d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(SpeedMonitor(window_size=3))\n",
    "callbacks.append(LRMonitor())\n",
    "callbacks.append(RuntimeEstimator())\n",
    "callbacks.append(OptimizerMonitor())\n",
    "callbacks.append(\n",
    "    LogDiffusionImages(\n",
    "        prompts=[\n",
    "            \"a photograph of an astronaut riding a horse\",\n",
    "            \"An astronaut riding a pig, highly realistic dslr photo, cinematic shot\",\n",
    "            \"Panda mad scientist mixing sparkling chemicals, artstation\",\n",
    "            \"a close-up of a fire spitting dragon, cinematic shot.\",\n",
    "            \"A small cactus with a happy face in the Sahara desert\",\n",
    "            \"Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\",\n",
    "            \"A dog that has been meditating all the time\",\n",
    "            \"A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style\",\n",
    "            \"A worker that looks like a mixture of cow and horse is working hard to type code\",\n",
    "            \"A capybara made of lego sitting in a realistic, natural field\",\n",
    "            \"A grand piano with a white bench.\",\n",
    "            \"In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and clothing texture.\",\n",
    "            \"A illustration from a graphic novel. A bustling city street under the shine of a full moon. The sidewalks bustling with pedestrians enjoying the nightlife. At the corner stall, a young woman with fiery red hair, dressed in a signature velvet cloak, is haggling with the grumpy old vendor. the grumpy vendor, a tall, sophisticated man is wearing a sharp suit, sports a noteworthy moustache is animatedly conversing on his steampunk telephone.\",\n",
    "            \"A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.\",\n",
    "            \"A giant cobra snake made from corn\",\n",
    "            \"A green sign that says 'Very Deep Learning' and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\"\n",
    "        ],\n",
    "        guidance_scale=5,\n",
    "        sampling_steps=30,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "callbacks.append(NaNCatcher())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = CosineAnnealingWithWarmupScheduler(\n",
    "    t_warmup='2500ba',\n",
    "    alpha_f=0.33 # decay to 0.8e-4 after 256x256 masked pre-training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887aca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.compileã«ã‚ˆã‚Š15%ç¨‹åº¦é«˜é€ŸåŒ–\n",
    "compile = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    loggers=loggers,\n",
    "    algorithms=algorithms,\n",
    "    callbacks=callbacks,\n",
    "    device='gpu',\n",
    "    # max_duration='250000ba',\n",
    "    max_duration='1ba',\n",
    "    eval_interval='2500ba' if compile else 0,\n",
    "    save_interval='2500ba',\n",
    "    save_num_checkpoints_to_keep=1,\n",
    "    device_train_microbatch_size=256,\n",
    "    run_name='microdit_experiment',\n",
    "    seed=42,\n",
    "    save_folder='./trained_models/microdit_experiment/',\n",
    "    save_overwrite=True,\n",
    "    autoresume=False,\n",
    "    parallelism_config=None,\n",
    "    precision='amp_bf16',  # if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "    python_log_level='debug',\n",
    "    compile_config={} if compile else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Ensure models are on correct device\n",
    "device = next(model.dit.parameters()).device\n",
    "model.vae.to(device)\n",
    "model.text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a89870",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.WARNING)  # to reduce streaming dataset info logs\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11922547",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—ï¼’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb97edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer with special handling for MoE parameters\n",
    "moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "\n",
    "if len(moe_params) > 0:\n",
    "    print('Reducing learning rate of MoE parameters by 1/2')\n",
    "    opt_dict = dict(cfg.optimizer)\n",
    "    opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "    del opt_dict['_target_']\n",
    "    optimizer = getattr(torch.optim, opt_name)(\n",
    "        params=[\n",
    "            {'params': rest_params},\n",
    "            {'params': moe_params, 'lr': 8e-5 / 2}], **opt_dict)\n",
    "else:\n",
    "    # optimizer = hydra.utils.instantiate(cfg.optimizer, params=model.dit.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.dit.parameters(),\n",
    "        lr=8e-5,\n",
    "        weight_decay=0.1,\n",
    "        eps=1.0e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "for p in optimizer.param_groups:\n",
    "    p['betas'] = list(p['betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acfd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "\n",
    "cap_seq_size, cap_emb_dim = text_encoder_embedding_format(\n",
    "    'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n",
    ")\n",
    "\n",
    "train_loader = build_streaming_latents_dataloader(\n",
    "    datadir=[\n",
    "        '/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/'\n",
    "    ],\n",
    "    image_size=256,\n",
    "    batch_size=128 // dist.get_world_size(), # 2048\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    cap_drop_prob=0.1,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_loader = build_streaming_latents_dataloader(\n",
    "    datadir='/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/',\n",
    "    image_size=256,\n",
    "    batch_size=1024 // dist.get_world_size(),\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d83dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "loggers.append(TensorboardLogger())\n",
    "\n",
    "exp_name = 'MicroDiTXL_mask_0_res_256_finetune'\n",
    "wandb_logger = WandBLogger(\n",
    "    init_kwargs={\n",
    "        'name': f'{exp_name}',\n",
    "        'project': 'microdit_training',  # insert wandb project name\n",
    "        'group': f'{exp_name}'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e88c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = []\n",
    "\n",
    "apply_low_precision_layernorm(\n",
    "    model=model.dit,\n",
    "    precision=Precision('amp_bf16'),\n",
    "    optimizers=optimizer\n",
    ")\n",
    "\n",
    "algorithms.append(\n",
    "    GradientClipping(clipping_type='norm', clipping_threshold=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a509a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(SpeedMonitor(window_size=3))\n",
    "callbacks.append(LRMonitor())\n",
    "callbacks.append(RuntimeEstimator())\n",
    "callbacks.append(OptimizerMonitor())\n",
    "callbacks.append(\n",
    "    LogDiffusionImages(\n",
    "        prompts=[\n",
    "            \"a photograph of an astronaut riding a horse\",\n",
    "            \"An astronaut riding a pig, highly realistic dslr photo, cinematic shot\",\n",
    "            \"Panda mad scientist mixing sparkling chemicals, artstation\",\n",
    "            \"a close-up of a fire spitting dragon, cinematic shot.\",\n",
    "            \"A small cactus with a happy face in the Sahara desert\",\n",
    "            \"Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\",\n",
    "            \"A dog that has been meditating all the time\",\n",
    "            \"A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style\",\n",
    "            \"A worker that looks like a mixture of cow and horse is working hard to type code\",\n",
    "            \"A capybara made of lego sitting in a realistic, natural field\",\n",
    "            \"A grand piano with a white bench.\",\n",
    "            \"In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and clothing texture.\",\n",
    "            \"A illustration from a graphic novel. A bustling city street under the shine of a full moon. The sidewalks bustling with pedestrians enjoying the nightlife. At the corner stall, a young woman with fiery red hair, dressed in a signature velvet cloak, is haggling with the grumpy old vendor. the grumpy vendor, a tall, sophisticated man is wearing a sharp suit, sports a noteworthy moustache is animatedly conversing on his steampunk telephone.\",\n",
    "            \"A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.\",\n",
    "            \"A giant cobra snake made from corn\",\n",
    "            \"A green sign that says 'Very Deep Learning' and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\"\n",
    "        ],\n",
    "        guidance_scale=5,\n",
    "        sampling_steps=30,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "callbacks.append(NaNCatcher())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33979c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ConstantScheduler(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147483ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile = False\n",
    "\n",
    "fsdp_config = {\n",
    "    'sharding_strategy': 'SHARD_GRAD_OP',\n",
    "}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    loggers=loggers,\n",
    "    algorithms=algorithms,\n",
    "    callbacks=callbacks,\n",
    "    device='gpu',\n",
    "    # max_duration='280000ba',\n",
    "    max_duration='1ba',\n",
    "    eval_interval='2500ba',\n",
    "    save_interval='2500ba',\n",
    "    save_num_checkpoints_to_keep=1,\n",
    "    device_train_microbatch_size=64,\n",
    "    run_name='microdit_experiment',\n",
    "    seed=18,\n",
    "    save_folder='./trained_models/microdit_experiment/',\n",
    "    # load_path='./trained_models/MicroDiTXL_mask_75_res_256_pretrain/latest-rank0.pt',\n",
    "    # load_ignore_keys=[\n",
    "    #     \"state/optimizers/AdamW/param_groups/initial_lr\",\n",
    "    #     \"state/optimizers/AdamW/param_groups/lr\",\n",
    "    #     \"state/schedulers/LambdaLR/base_lrs\",\n",
    "    #     \"state/schedulers/LambdaLR/_last_lr\"\n",
    "    # ],\n",
    "    save_overwrite=True,\n",
    "    autoresume=False,\n",
    "    parallelism_config=None,\n",
    "    precision='amp_bf16',  # if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "    python_log_level='debug',\n",
    "    compile_config={} if compile else None,\n",
    "    # fsdp_config=fsdp_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.dit.parameters()).device\n",
    "model.vae.to(device)\n",
    "model.text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070a0ac",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—ï¼“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer with special handling for MoE parameters\n",
    "\n",
    "moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "\n",
    "if len(moe_params) > 0:\n",
    "    print('Reducing learning rate of MoE parameters by 1/2')\n",
    "    opt_dict = dict(cfg.optimizer)\n",
    "    opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "    del opt_dict['_target_']\n",
    "    optimizer = getattr(torch.optim, opt_name)(\n",
    "        params=[\n",
    "            {'params': rest_params},\n",
    "            {'params': moe_params, 'lr': 8e-5 / 2}], **opt_dict)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.dit.parameters(),\n",
    "        lr=8e-5,\n",
    "        weight_decay=0.1,\n",
    "        eps=1.0e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d56753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "for p in optimizer.param_groups:\n",
    "    p['betas'] = list(p['betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "# cap_seq_size, cap_emb_dim = text_encoder_embedding_format(cfg.model.text_encoder_name)\n",
    "cap_seq_size, cap_emb_dim = text_encoder_embedding_format(\n",
    "    'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n",
    ")\n",
    "\n",
    "train_loader = build_streaming_latents_dataloader(\n",
    "    datadir=[\n",
    "        '/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/'\n",
    "    ],\n",
    "    image_size=256,\n",
    "    batch_size=128 // dist.get_world_size(), # 2048\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    cap_drop_prob=0.0,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# print(f\"Found {len(train_loader.dataset)*dist.get_world_size()} images in the training dataset\")\n",
    "# time.sleep(3)\n",
    "\n",
    "eval_loader = build_streaming_latents_dataloader(\n",
    "    datadir='/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/',\n",
    "    image_size=256,\n",
    "    batch_size=1024 // dist.get_world_size(),\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473782e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "loggers.append(TensorboardLogger())\n",
    "exp_name = 'MicroDiTXL_mask_0_res_256_finetune_final'\n",
    "wandb_logger = WandBLogger(\n",
    "    init_kwargs={\n",
    "        'name': f'{exp_name}',\n",
    "        'project': 'microdit_training',  # insert wandb project name\n",
    "        'group': f'{exp_name}'\n",
    "    }\n",
    ")\n",
    "# loggers.append(wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = []\n",
    "apply_low_precision_layernorm(\n",
    "    model=model.dit,\n",
    "    precision=Precision('amp_bf16'),\n",
    "    optimizers=optimizer\n",
    ")\n",
    "algorithms.append(\n",
    "    GradientClipping(clipping_type='norm', clipping_threshold=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca32d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(SpeedMonitor(window_size=3))\n",
    "callbacks.append(LRMonitor())\n",
    "callbacks.append(RuntimeEstimator())\n",
    "callbacks.append(OptimizerMonitor())\n",
    "callbacks.append(\n",
    "    LogDiffusionImages(\n",
    "        prompts=[\n",
    "            \"a photograph of an astronaut riding a horse\",\n",
    "            \"An astronaut riding a pig, highly realistic dslr photo, cinematic shot\",\n",
    "            \"Panda mad scientist mixing sparkling chemicals, artstation\",\n",
    "            \"a close-up of a fire spitting dragon, cinematic shot.\",\n",
    "            \"A small cactus with a happy face in the Sahara desert\",\n",
    "            \"Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\",\n",
    "            \"A dog that has been meditating all the time\",\n",
    "            \"A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style\",\n",
    "            \"A worker that looks like a mixture of cow and horse is working hard to type code\",\n",
    "            \"A capybara made of lego sitting in a realistic, natural field\",\n",
    "            \"A grand piano with a white bench.\",\n",
    "            \"In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and clothing texture.\",\n",
    "            \"A illustration from a graphic novel. A bustling city street under the shine of a full moon. The sidewalks bustling with pedestrians enjoying the nightlife. At the corner stall, a young woman with fiery red hair, dressed in a signature velvet cloak, is haggling with the grumpy old vendor. the grumpy vendor, a tall, sophisticated man is wearing a sharp suit, sports a noteworthy moustache is animatedly conversing on his steampunk telephone.\",\n",
    "            \"A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.\",\n",
    "            \"A giant cobra snake made from corn\",\n",
    "            \"A green sign that says 'Very Deep Learning' and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\"\n",
    "        ],\n",
    "        guidance_scale=5,\n",
    "        sampling_steps=30,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "callbacks.append(NaNCatcher())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5738a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ConstantScheduler(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    loggers=loggers,\n",
    "    algorithms=algorithms,\n",
    "    callbacks=callbacks,\n",
    "    device='gpu',\n",
    "    # max_duration='50000ba',\n",
    "    max_duration='1ba',\n",
    "    eval_interval='2500ba',\n",
    "    save_interval='2500ba',\n",
    "    save_num_checkpoints_to_keep=1,\n",
    "    device_train_microbatch_size=64,\n",
    "    run_name='microdit_experiment',\n",
    "    seed=18,\n",
    "    save_folder='./trained_models/microdit_experiment/',\n",
    "    load_path='./trained_models/MicroDiTXL_mask_0_res_256_finetune/latest-rank0.pt',\n",
    "    load_weights_only=True,\n",
    "    load_strict_model_weights=False,\n",
    "    load_ignore_keys=[\"state/model/dit.pos_embed\"],\n",
    "    save_overwrite=True,\n",
    "    autoresume=False,\n",
    "    parallelism_config=None,\n",
    "    precision='amp_bf16',  # if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "    python_log_level='debug',\n",
    "    compile_config={} if compile else None,\n",
    "    # fsdp_config=fsdp_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.dit.parameters()).device\n",
    "model.vae.to(device)\n",
    "model.text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0007779",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8565a4e",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—ï¼”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8931ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer with special handling for MoE parameters\n",
    "moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "if len(moe_params) > 0:\n",
    "    print('Reducing learning rate of MoE parameters by 1/2')\n",
    "    opt_dict = dict(cfg.optimizer)\n",
    "    opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "    del opt_dict['_target_']\n",
    "    optimizer = getattr(torch.optim, opt_name)(\n",
    "        params=[{'params': rest_params}, {'params': moe_params, 'lr': cfg.optimizer.lr / 2}], **opt_dict)\n",
    "else:\n",
    "    # optimizer = hydra.utils.instantiate(cfg.optimizer, params=model.dit.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.dit.parameters(),\n",
    "        lr=8e-5,\n",
    "        weight_decay=0.1,\n",
    "        eps=1.0e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "# Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "for p in optimizer.param_groups:\n",
    "    p['betas'] = list(p['betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_seq_size, cap_emb_dim = text_encoder_embedding_format(\n",
    "    'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n",
    ")\n",
    "\n",
    "train_loader = build_streaming_latents_dataloader(\n",
    "    datadir=[\n",
    "        '/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/'\n",
    "    ],\n",
    "    image_size=512,\n",
    "    batch_size=128 // dist.get_world_size(), # 2048\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    cap_drop_prob=0.0,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_loader = build_streaming_latents_dataloader(\n",
    "    datadir='/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/',\n",
    "    image_size=512,\n",
    "    batch_size=1024 // dist.get_world_size(),\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "\n",
    "loggers.append(TensorboardLogger())\n",
    "exp_name = 'MicroDiTXL_mask_0_res_512_finetune_final'\n",
    "wandb_logger = WandBLogger(\n",
    "    init_kwargs={\n",
    "        'name': f'{exp_name}',\n",
    "        'project': 'microdit_training',  # insert wandb project name\n",
    "        'group': f'{exp_name}'\n",
    "    }\n",
    ")\n",
    "# loggers.append(wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61067727",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = []\n",
    "apply_low_precision_layernorm(\n",
    "    model=model.dit,\n",
    "    precision=Precision('amp_bf16'),\n",
    "    optimizers=optimizer\n",
    ")\n",
    "algorithms.append(\n",
    "    GradientClipping(clipping_type='norm', clipping_threshold=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae206a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(SpeedMonitor(window_size=3))\n",
    "callbacks.append(LRMonitor())\n",
    "callbacks.append(RuntimeEstimator())\n",
    "callbacks.append(OptimizerMonitor())\n",
    "callbacks.append(\n",
    "    LogDiffusionImages(\n",
    "        prompts=[\n",
    "            \"a photograph of an astronaut riding a horse\",\n",
    "            \"An astronaut riding a pig, highly realistic dslr photo, cinematic shot\",\n",
    "            \"Panda mad scientist mixing sparkling chemicals, artstation\",\n",
    "            \"a close-up of a fire spitting dragon, cinematic shot.\",\n",
    "            \"A small cactus with a happy face in the Sahara desert\",\n",
    "            \"Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\",\n",
    "            \"A dog that has been meditating all the time\",\n",
    "            \"A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style\",\n",
    "            \"A worker that looks like a mixture of cow and horse is working hard to type code\",\n",
    "            \"A capybara made of lego sitting in a realistic, natural field\",\n",
    "            \"A grand piano with a white bench.\",\n",
    "            \"In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and clothing texture.\",\n",
    "            \"A illustration from a graphic novel. A bustling city street under the shine of a full moon. The sidewalks bustling with pedestrians enjoying the nightlife. At the corner stall, a young woman with fiery red hair, dressed in a signature velvet cloak, is haggling with the grumpy old vendor. the grumpy vendor, a tall, sophisticated man is wearing a sharp suit, sports a noteworthy moustache is animatedly conversing on his steampunk telephone.\",\n",
    "            \"A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.\",\n",
    "            \"A giant cobra snake made from corn\",\n",
    "            \"A green sign that says 'Very Deep Learning' and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\"\n",
    "        ],\n",
    "        guidance_scale=5,\n",
    "        sampling_steps=30,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "callbacks.append(NaNCatcher())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ConstantScheduler(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    loggers=loggers,\n",
    "    algorithms=algorithms,\n",
    "    callbacks=callbacks,\n",
    "    device='gpu',\n",
    "    # max_duration='55000ba',\n",
    "    max_duration='1ba',\n",
    "    eval_interval='500ba',\n",
    "    save_interval='500ba',\n",
    "    save_num_checkpoints_to_keep=1,\n",
    "    device_train_microbatch_size=64,\n",
    "    run_name='microdit_experiment',\n",
    "    seed=18,\n",
    "    save_folder='./trained_models/microdit_experiment/',\n",
    "    load_path='./trained_models/MicroDiTXL_mask_0_res_256_finetune/latest-rank0.pt',\n",
    "    load_weights_only=True,\n",
    "    load_strict_model_weights=False,\n",
    "    load_ignore_keys=[\"state/optimizers/AdamW/param_groups/initial_lr\", \"state/optimizers/AdamW/param_groups/lr\", \"state/schedulers/LambdaLR/base_lrs\", \"state/schedulers/LambdaLR/_last_lr\"],\n",
    "    save_overwrite=True,\n",
    "    autoresume=False,\n",
    "    parallelism_config=None,\n",
    "    precision='amp_bf16',  # if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "    python_log_level='debug',\n",
    "    compile_config={} if compile else None,\n",
    "    # fsdp_config=fsdp_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.dit.parameters()).device\n",
    "model.vae.to(device)\n",
    "model.text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c41188",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
