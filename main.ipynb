{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add57ae6",
   "metadata": {},
   "source": [
    "# micro diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf5321",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/JourneyDB/JourneyDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f988b",
   "metadata": {},
   "source": [
    "## æ¦‚è¦\n",
    "\n",
    "micro diffusionã¯ã€ä½ã‚³ã‚¹ãƒˆã§å­¦ç¿’å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ï¼ˆT2Iï¼‰æ‹¡æ•£Transformerãƒ¢ãƒ‡ãƒ«ï¼ˆDiTï¼‰\n",
    "\n",
    "- T2I = Text to Image\n",
    "- DiT = Diffusion Transformer\n",
    "- Vision Transformerï¼ˆViTï¼‰ãƒ™ãƒ¼ã‚¹ã®æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆLatent Diffusion Modelsï¼‰\n",
    "\n",
    "ç‰¹å¾´:\n",
    "\n",
    "- è¨“ç·´æ™‚ã«ç”»åƒã®ãƒ‘ãƒƒãƒã‚’æœ€å¤§75%ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚­ãƒ³ã‚°ã—ã€Transformerã®è¨ˆç®—é‡ã‚’å‰Šæ¸›\n",
    "- ãƒã‚¹ã‚­ãƒ³ã‚°ã®ç›´å‰ã«Patch-mixerã¨ã„ã†å‰å‡¦ç†ã‚’æŒŸã‚€é…å»¶ãƒã‚¹ã‚­ãƒ³ã‚°ï¼ˆDiffered Maskingï¼‰ã‚’æ¡ç”¨ã—ã€ç”Ÿæˆå“è³ªã‚’æ”¹å–„\n",
    "- MoEï¼ˆMixture-of-Expertsï¼‰ãªã©æœ€æ–°ã®æ‰‹æ³•ã‚’æ¡ç”¨ã—ã€ç”Ÿæˆå“è³ªã‚’æ”¹å–„ï¼ˆåˆè¨ˆ11.6å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n",
    "- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«åˆæˆãƒ‡ãƒ¼ã‚¿ï¼ˆSynthetic imagesï¼‰ã‚’ä½¿ç”¨ã—ã€ç”Ÿæˆå“è³ªã‚’æ”¹å–„\n",
    "- å…¬é–‹ã•ã‚Œã¦ã„ã‚‹3700ä¸‡æšã®å®Ÿç”»åƒã¨ã€è¿½åŠ ã®åˆæˆç”»åƒã‚’è¨“ç·´ã«ä½¿ç”¨\n",
    "- è¨“ç·´ã«è¦ã—ãŸã‚³ã‚¹ãƒˆã¯1,890ãƒ‰ãƒ«ï¼ˆ30ä¸‡å††ï¼‰ã§ã€Stable Diffusionãªã©ã¨åŒ¹æ•µã™ã‚‹æ€§èƒ½ï¼ˆFIDã‚¹ã‚³ã‚¢ï¼‰\n",
    "    - 8å°ã®H100 GPUã‚’å˜ä¸€ãƒãƒ¼ãƒ‰ã§2.6æ—¥\n",
    "    - Stable Diffusionã®è¨“ç·´ã‚³ã‚¹ãƒˆï¼ˆ3,345ä¸‡å††ï¼‰ã‚ˆã‚Š118å€ä½ã„\n",
    "    - æœ€å…ˆç«¯ã®æ‰‹æ³•ï¼ˆ430ä¸‡å††ï¼‰ã‚ˆã‚Š14å€ä½ã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0b449",
   "metadata": {},
   "source": [
    "micro_diffusionã®ç”Ÿæˆä¾‹ï¼ˆ512x512ï¼‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe77901",
   "metadata": {},
   "source": [
    "![](image/fig1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a86737",
   "metadata": {},
   "source": [
    "å·¦: Image of an atronaut riding a hourse in ___ styleã®æ¯”è¼ƒ, å³: FIDã‚¹ã‚³ã‚¢ã®æ¯”è¼ƒ:\n",
    "\n",
    "![](image/fig1b.png)\n",
    "\n",
    "- origami\n",
    "- pixel art\n",
    "- line art\n",
    "- cyberpunk\n",
    "- Van Goph Strarry Night"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3c8b2",
   "metadata": {},
   "source": [
    "## æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å‰æçŸ¥è­˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef83bea",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ$\\mathcal{D}$ã‹ã‚‰ã€ç”»åƒ$x$ã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³$c$ã‚’ã‚µãƒ³ãƒ—ãƒ«ã™ã‚‹ã¨ãã€æ¬¡å¼ã§è¡¨ç¾ã™ã‚‹:\n",
    "\n",
    "$$\n",
    "(x, c) \\sim \\mathcal{D}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8d513",
   "metadata": {},
   "source": [
    "ç”»åƒ$x$ã«ã€åˆ†æ•£$\\sigma^2$ã®ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚’åŠ ãˆãŸã¨ãã€ç”»åƒåˆ†å¸ƒã¯æ¬¡å¼ã§è¡¨ç¾ã§ãã‚‹:\n",
    "\n",
    "$$\n",
    "p(x; \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53749fc3",
   "metadata": {},
   "source": [
    "![](image/gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b29858",
   "metadata": {},
   "source": [
    "æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã¯ã€é †æ‹¡æ•£éç¨‹ã§ãƒã‚¤ã‚ºã‚’åŠ ãˆãŸç”»åƒã‚’ã€é€†æ‹¡æ•£éç¨‹ã§å¾©å…ƒã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã™ã‚‹:\n",
    "\n",
    "![](image/diffusion_process.png)\n",
    "\n",
    "- ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹æ–¹å‘ãŒé †æ‹¡æ•£éç¨‹ï¼ˆForward Diffusion Processï¼‰\n",
    "- ãƒã‚¤ã‚ºã‚’é™¤å»ã™ã‚‹æ–¹å‘ãŒé€†æ‹¡æ•£éç¨‹ï¼ˆReverse Denoising Process, Reverse Diffusion Processï¼‰\n",
    "- ãƒã‚¤ã‚ºã‚’åŠ ç®—/æ¸›ç®—ã™ã‚‹ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—$t$ã§ã¯ã€æ™‚é–“ä¾å­˜ã®ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚º$\\sigma(t)$ã‚’ä½¿ç”¨ã™ã‚‹\n",
    "- ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã¯ã€$p(x;\\sigma_{max})$ã‹ã‚‰ãƒã‚¤ã‚ºã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€åå¾©çš„ã«ãƒã‚¤ã‚ºé™¤å»ã™ã‚‹ä»•çµ„ã¿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5c84a",
   "metadata": {},
   "source": [
    "æ‹¡æ•£éç¨‹ã‚’ã€Œæ±ºå®šè«–çš„ã§é€£ç¶šçš„ãªã‚¹ãƒ†ãƒƒãƒ—ã€ã¨ã™ã‚‹ã¨æ¬¡ã®å¸¸å¾®åˆ†æ–¹ç¨‹å¼ã§è¡¨ç¾ã§ãã‚‹ï¼ˆODEå®šå¼åŒ–ï¼‰:\n",
    "\n",
    "$$\n",
    "dx = -\\dot{\\sigma}(t)\\sigma(t)\\nabla_{x}\\log p(x;\\sigma(t))dt\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fabc9b",
   "metadata": {},
   "source": [
    "- æ±ºå®šè«–çš„ï¼ˆderteministic samplingï¼‰: ãƒã‚¤ã‚ºé™¤å»ã«ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒç„¡ã„åˆ¶ç´„æ¡ä»¶\n",
    "- å¸¸å¾®åˆ†æ–¹ç¨‹å¼ï¼ˆODE, Ordinary Differential Equationï¼‰: 1ã¤ã®ç‹¬ç«‹å¤‰æ•°ã«å¯¾ã™ã‚‹é–¢æ•°ã®å¤‰åŒ–ã‚’è¨˜è¿°ã™ã‚‹å¼\n",
    "- $dx$: ç”»åƒ$x$ã®å¾®å°å¤‰åŒ–é‡\n",
    "- $\\sigma(t)$: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—$t$ã§ã®ãƒã‚¤ã‚º\n",
    "- $\\dot{\\sigma}(t)$: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—$t$ã§ã®ãƒã‚¤ã‚ºã®å¤‰åŒ–é‡ï¼ˆãƒ‰ãƒƒãƒˆã¯å¾®åˆ†ï¼‰\n",
    "- $\\nabla_{x}\\log p(x;\\sigma(t))$: ã‚ˆã‚Šç”»åƒã‚‰ã—ããªã‚‹æ–¹å‘ã®å‚¾ãï¼ˆç¢ºç‡å¯†åº¦é–¢æ•°ã®å‹¾é…ã€ã‚¹ã‚³ã‚¢é–¢æ•°ï¼‰\n",
    "- $dt$: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å¾®å°å¤‰åŒ–é‡\n",
    "- ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãŒæ­£æ–¹å‘ï¼ˆ$0$ã‹ã‚‰$t$ï¼‰ã®ã¨ãé †æ‹¡æ•£éç¨‹ã§ã€è² ã®æ–¹å‘ã®ã¨ãé€†æ‹¡æ•£éç¨‹\n",
    "- æ¨è«–æ™‚ã®å®Ÿè£…ã«ä½¿ç”¨ï¼ˆæœ€çŸ­ã‚¹ãƒ†ãƒƒãƒ—ã§ç”»åƒç”Ÿæˆã§ãã‚‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4c5ee",
   "metadata": {},
   "source": [
    "æ‹¡æ•£éç¨‹ã‚’ã€Œç¢ºç‡çš„ã§é€£ç¶šçš„ãªã‚¹ãƒ†ãƒƒãƒ—ã€ã¨ã™ã‚‹ã¨æ¬¡ã®ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼ã§è¡¨ç¾ã§ãã‚‹ï¼ˆSDEå®šå¼åŒ–ï¼‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe1061",
   "metadata": {},
   "source": [
    "$$\n",
    "dx = [-\\dot{\\sigma}(t)\\sigma(t)\\nabla_{x}\\log p(x;\\sigma(t)) - \\beta(t)\\sigma(t)^{2}\\nabla_{x}\\log p(x;\\sigma(t))]dt + \\sqrt{2\\beta(t)}\\sigma(t)dw_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85fadd",
   "metadata": {},
   "source": [
    "- $[-\\dot{\\sigma}(t)\\sigma(t)\\nabla_{x}\\log p(x;\\sigma(t)) - \\beta(t)\\sigma(t)^{2}\\nabla_{x}\\log p(x;\\sigma(t))]dt$\n",
    "    - æ±ºå®šè«–çš„ãªæ‹¡æ•£éç¨‹ï¼ˆDrifté …ï¼‰\n",
    "    - $- \\beta(t)\\sigma(t)^{2}\\nabla_{x}\\log p(x;\\sigma(t))$ã¯ã€å®‰å®šåŒ–ã®ãŸã‚ã®å€¤\n",
    "- $\\sqrt{2\\beta(t)}\\sigma(t)dw_{t}$\n",
    "    - ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¤ã‚ºï¼ˆDiffusioné …ï¼‰\n",
    "- $\\beta(t)$: ãƒã‚¤ã‚ºå¼·åº¦ã‚’ç¤ºã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "- $dw_t$: ãƒ©ãƒ³ãƒ€ãƒ ãªå¾®å°ãƒã‚¤ã‚ºï¼ˆæ¨™æº–ã‚¦ã‚£ãƒ¼ãƒŠãƒ¼éç¨‹ï¼‰\n",
    "- å­¦ç¿’æ™‚ã®å®Ÿè£…ã«ä½¿ç”¨ï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒãƒ¢ãƒ‡ãƒ«ã‚’å¼·ãã™ã‚‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e86d8",
   "metadata": {},
   "source": [
    "æ•°å­¦çš„ã«ã‚¹ã‚³ã‚¢é–¢æ•°ã¯ã€Œãƒã‚¤ã‚ºé™¤å»å™¨ãŒäºˆæ¸¬ã™ã‚‹ä¿®æ­£æ–¹å‘ã€ã¨ã»ã¼åŒã˜:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\log p(x; \\sigma(t)) \\approx (F_\\theta(x, \\sigma(t)) - x) / \\sigma(t)^2\n",
    "$$\n",
    "\n",
    "- $F_\\theta(x, \\sigma(t))$\n",
    "    - ãƒã‚¤ã‚ºé™¤å»é–¢æ•°\n",
    "    - é€šå¸¸ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚Œã‚‹\n",
    "\n",
    "T2Iã®å ´åˆã€ãƒã‚¤ã‚ºé™¤å»å™¨ã¯ãƒã‚¤ã‚ºåˆ†å¸ƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å—ã‘å–ã‚‹:\n",
    "\n",
    "$$\n",
    "F_\\theta(x + \\epsilon; \\sigma(t), c)\n",
    "$$\n",
    "\n",
    "- $x + \\epsilon$: å…ƒç”»åƒ$x$ã«ãƒã‚¤ã‚º$\\epsilon$ã‚’è¶³ã—ãŸãƒã‚¤ã‚ºã‚ã‚Šç”»åƒ\n",
    "- $c$: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217666e8",
   "metadata": {},
   "source": [
    "T2Iã®æå¤±é–¢æ•°ã¯æ¬¡å¼ã§è¡¨ç¾ã§ãã‚‹:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{(x,c)\\sim\\mathcal{D}}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\sigma(t)^{2}I)}||F_{\\theta}(x+\\epsilon;\\sigma(t),c)-x||_{2}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201fb85",
   "metadata": {},
   "source": [
    "- $\\mathbb{E}_{(x,c)\\sim\\mathcal{D}}$: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ$D$ã‹ã‚‰ç”»åƒ$x$ã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³$c$ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ãŸã¨ãã®ã®æå¤±ã®å¹³å‡\n",
    "- $\\mathbb{E}_{\\epsilon\\sim{\\mathcal{N}(0,\\sigma(t)^2I)}}$: åˆ†æ•£$\\sigma(t)^2$ã®ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‹ã‚‰ãƒã‚¤ã‚º$\\epsilon$ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ãŸã¨ãã®æå¤±ã®å¹³å‡\n",
    "- $||...||_2^2$: äºŒä¹—èª¤å·®\n",
    "- $F_\\theta(\\cdot)$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\\theta$ã®ãƒã‚¤ã‚ºé™¤å»å™¨\n",
    "- $x$: æ­£è§£ã®å…ƒç”»åƒ\n",
    "- ãƒã‚¤ã‚ºé™¤å»å™¨ãŒæ­£ã—ãå…ƒã®ç”»åƒã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b2fe4",
   "metadata": {},
   "source": [
    "è¨“ç·´ä¸­ã®ãƒã‚¤ã‚ºã®å¼·åº¦ã¯ã€å¯¾æ•°æ­£è¦åˆ†å¸ƒï¼ˆlog-normal distributionï¼‰ã‚’ä½¿ç”¨ã™ã‚‹:\n",
    "\n",
    "$$\n",
    "\\ln(\\sigma) \\sim \\mathcal{N}(P_{mean}, P_{std})\n",
    "$$\n",
    "\n",
    "![](image/lognormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a382ac7",
   "metadata": {},
   "source": [
    "- ç”»åƒã®è§£åƒåº¦ãŒé«˜ããªã‚‹ã»ã©ã€ä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼ˆSNRï¼‰ãŒå¢—åŠ ã™ã‚‹ãŸã‚ã€åˆ†å¸ƒã‚’å³ã«ã‚·ãƒ•ãƒˆã™ã‚‹ï¼ˆå¹³å‡ã¨æ¨™æº–åå·®ã‚’å¤‰ãˆã‚‹ï¼‰\n",
    "    - SNRã¯ã€é«˜è§£åƒåº¦ã«ãªã‚‹ã»ã©ï¼ˆç”»åƒãŒå¤§ãããªã‚‹ã»ã©ï¼‰ãƒã‚¤ã‚ºãŒç›¸å¯¾çš„ã«å°ã•ããªã‚‹ã®ã§ã€å¢—åŠ ã™ã‚‹\n",
    "    - SNRãŒé«˜ã„ã¨ã€ãƒã‚¤ã‚ºã®å½±éŸ¿ãŒå°‘ãªããªã‚Šå­¦ç¿’ã®åŠ¹ç‡ãŒæ‚ªããªã‚‹ãŸã‚ã€å³ã«ã‚·ãƒ•ãƒˆã—ã¦ãƒã‚¤ã‚ºã‚’å¢—ã‚„ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd3927",
   "metadata": {},
   "source": [
    "ç”Ÿæˆç”»åƒã«å¯¾ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼·åº¦ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«ã€CFGï¼ˆClassifier-free Guidanceï¼‰ã‚’æ¡ç”¨:\n",
    "\n",
    "$$\n",
    "F_{\\theta}(x;\\sigma(t),c) = F_{\\theta}(x;\\sigma(t)) + w \\cdot (F_{\\theta}(x;\\sigma(t),c) - F_{\\theta}(x;\\sigma(t)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee3e442",
   "metadata": {},
   "source": [
    "- $F_\\theta(x; \\sigma(t), c)$: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚ã‚Šã®äºˆæ¸¬ï¼ˆç„¡æ¡ä»¶ã®äºˆæ¸¬ï¼‰\n",
    "- $F_\\theta(x; \\sigma(t))$: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãªã—ã®äºˆæ¸¬ï¼ˆæ¡ä»¶ä»˜ãã®äºˆæ¸¬ï¼‰\n",
    "- $w$: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼·åº¦ï¼ˆGuidance Scaleï¼‰ï¼ˆ$w \\ge 1$ï¼‰ã€‚\n",
    "- æ¡ä»¶ä»˜ãã®äºˆæ¸¬ = ç„¡æ¡ä»¶ã®äºˆæ¸¬ + å¼·åº¦ * ï¼ˆæ¡ä»¶ä»˜ãã®äºˆæ¸¬ - ç„¡æ¡ä»¶ã®äºˆæ¸¬ï¼‰\n",
    "- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç„¡ã—ã§ã®ç”Ÿæˆã«å¯¾å¿œã™ã‚‹ãŸã‚ã€å­¦ç¿’ä¸­ã¯ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ä¸€éƒ¨ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‰ãƒ­ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae9f22",
   "metadata": {},
   "source": [
    "æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆLatent Diffusion Models, LDMï¼‰ã¯ã€ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®ç”»åƒã§ã¯ãªãã€åœ§ç¸®ã—ãŸæ½œåœ¨è¡¨ç¾ã‚’å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹:\n",
    "\n",
    "- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“: $\\mathbb{R}^{h\\times w\\times 3}$\n",
    "- æ½œåœ¨ç©ºé–“ï¼ˆlatent spaceï¼‰: $\\mathbb{R}^{\\frac{h}{n}\\times\\frac{w}{n}\\times c}$\n",
    "    - $n$: åœ§ç¸®ç‡\n",
    "    - $c$: æ½œåœ¨ç©ºé–“ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°\n",
    "- ç”»åƒã‹ã‚‰æ½œåœ¨è¡¨ç¾ï¼ˆencodingï¼‰ã€æ½œåœ¨è¡¨ç¾ã‹ã‚‰ç”»åƒï¼ˆreverse mappingï¼‰ã§ã¯ã€VAEï¼ˆVariable Auto Encoderï¼‰ã‚’ä½¿ç”¨\n",
    "- LDMã«ã‚ˆã‚Šã€å­¦ç¿’ã®åæŸã‚’æ—©ã‚ã‚‹ã“ã¨ãŒå¯èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1004556",
   "metadata": {},
   "source": [
    "æ‹¡æ•£Transformerï¼ˆDiffision Transformer, DiTï¼‰ãƒ¢ãƒ‡ãƒ«$F_\\theta$ã¯ã€$k$å€‹ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã§æ§‹æˆ:\n",
    "\n",
    "- å„Transformerãƒ–ãƒ­ãƒƒã‚¯ã®æ§‹æˆ\n",
    "    - ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆSelf-attentionï¼‰\n",
    "    - ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆCross-attentionï¼‰\n",
    "    - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆFeed-forward layerï¼‰\n",
    "- å…¨ã¦ã®ç”»åƒã¯ã€ãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›ã•ã‚Œã‚‹ï¼ˆViTã¨åŒæ§˜ï¼‰\n",
    "- å…¨ã¦ã®ç·šå½¢å±¤ã®æ¬¡å…ƒæ•°ã‚’$d$ã¨ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34e9e8",
   "metadata": {},
   "source": [
    "## DiTã®è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c3a58",
   "metadata": {},
   "source": [
    "DiTã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã«æ§˜ã€…ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43d42b",
   "metadata": {},
   "source": [
    "![](image/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd45f34",
   "metadata": {},
   "source": [
    "- a: å‰Šæ¸›ã—ãªã„å ´åˆã€$\\text{ã‚µãƒ³ãƒ—ãƒ«æ•°}M\\times \\text{ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°}N \\times \\text{ãƒ‘ãƒƒãƒæ•°}S$ã«æ¯”ä¾‹ã™ã‚‹è¨ˆç®—ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹\n",
    "- b: ãƒ‘ãƒƒãƒã‚’å¤§ããã™ã‚‹å ´åˆã€VAEã®åœ§ç¸®ç‡ãŒé«˜ããªã‚‹ãŸã‚æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹\n",
    "- c: ãƒ‘ãƒƒãƒã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹æ‰‹æ³•ã¯ã‚ˆãä½¿ã‚ã‚Œã‚‹ãŒã€75%æ¨ã¦ã‚‹ã¨æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹\n",
    "- d: ãƒ‘ãƒƒãƒã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã—ã€ãƒ‰ãƒ­ãƒƒãƒ—ã•ã‚ŒãŸç®‡æ‰€ã«å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒƒãƒã‚’å·®ã—è¾¼ã¿ã€ãã‚Œã‚’ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãŒå¾©å…ƒã™ã‚‹ï¼ˆMaskDiTï¼‰\n",
    "- e: ãƒ‘ãƒƒãƒã«è»½é‡ãªTransformerã‚’é©ç”¨ã—ï¼ˆPatch-mixerï¼‰ã€ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹æ‰‹æ³•ï¼ˆDeferred maskingï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d7503",
   "metadata": {},
   "source": [
    "Deferred maskingã§ã¯ã€Patch-mixerã«ã‚ˆã‚Šã€ãƒã‚¹ã‚¯ã™ã‚‹å‰ã«ãƒ‘ãƒƒãƒã«ç”»åƒå…¨ä½“ã®æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã‚€:\n",
    "\n",
    "- Patch-mixerã¯ã€ã‚ãšã‹ãªå±¤ã§æ§‹æˆã•ã‚Œã‚‹è»½é‡ãªTransformer\n",
    "- 75%ãƒ‰ãƒ­ãƒƒãƒ—ã—ãŸå ´åˆã§ã‚‚æ€§èƒ½ã‚’ç¶­æŒã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75001eb",
   "metadata": {},
   "source": [
    "Deferred maskingã®æå¤±é–¢æ•°ã¯æ¬¡å¼ã§è¡¨ã•ã‚Œã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe17bbd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{(x,c)\\sim\\mathcal{D}}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\sigma(t)^{2}I)} || F_{\\theta}(M_{\\phi}(x+\\epsilon) \\odot (1-m); \\sigma(t), c) - x \\odot (1-m) ||_{2}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92bc8a6",
   "metadata": {},
   "source": [
    "- $M_\\phi(x + \\epsilon)$: ãƒ‘ãƒƒãƒãƒŸã‚­ã‚µãƒ¼ã‚’é©ç”¨ã—ã€ç”»åƒå…¨ä½“ã®æƒ…å ±ã‚’æ··ãœåˆã‚ã›ã‚‹\n",
    "- $\\cdot (1 - m)$: ãƒ‘ãƒƒãƒãƒŸã‚­ã‚µãƒ¼ã®å‡ºåŠ›ã«ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯$m$ã‚’é©ç”¨ã™ã‚‹\n",
    "- ã‚ã¨ã¯DiTæœ¬æ¥ã®æå¤±é–¢æ•°ã¨åŒã˜å½¢å¼\n",
    "- å®Ÿéš›ã®è¨“ç·´ã§ã¯ã€ãƒã‚¹ã‚¯ã‚ã‚Šã®äº‹å‰å­¦ç¿’ã«ã€ãƒã‚¹ã‚­ãƒ³ã‚°ãªã—ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729bc1a",
   "metadata": {},
   "source": [
    "micro diffusionã§ã¯ã€MoEï¼ˆMixture-of-Expertsï¼‰ã‚’æ¡ç”¨:\n",
    "\n",
    "- å­¦ç¿’ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å¢—ã‚„ã™ã“ã¨ãªãã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¾åŠ›ã‚’å‘ä¸Š\n",
    "- Expert-choice-routingã«ã‚ˆã‚Šã€è² è·åˆ†æ•£ã®ãŸã‚ã®è£œåŠ©æå¤±é–¢æ•°ãŒä¸è¦ãªè¨­è¨ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0287ddd",
   "metadata": {},
   "source": [
    "Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’å±¤ã”ã¨ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹æ‰‹æ³•ã‚‚æ¤œè¨ï¼ˆLayer-wise scalingï¼‰:\n",
    "\n",
    "- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ·±ã„å±¤ã¯ã€æµ…ã„å±¤ã‚ˆã‚Šã‚‚å¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹\n",
    "- è¦–è¦šãƒ¢ãƒ‡ãƒ«ã®æ·±ã„å±¤ã¯ã‚ˆã‚Šè¤‡é›‘ãªç‰¹å¾´ã‚’å­¦ç¿’ã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ãŸã‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9118c1",
   "metadata": {},
   "source": [
    "micro diffusionã®DiTã®å…¨ä½“çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082478d",
   "metadata": {},
   "source": [
    "![](image/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1427d",
   "metadata": {},
   "source": [
    "## å®Ÿé¨“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9e31a",
   "metadata": {},
   "source": [
    "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:\n",
    "\n",
    "- [2ã¤ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®DiTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£][3]ã‚’ä½¿ç”¨:\n",
    "    - DiT-Tiny/2: å°è¦æ¨¡ãªå®Ÿé¨“ç”¨\n",
    "    - DiT-XL/2: å¤§è¦æ¨¡ãªå®Ÿé¨“ç”¨\n",
    "- ãƒ‘ãƒƒãƒã‚µã‚¤ã‚ºã¯2\n",
    "    - æ½œåœ¨ç©ºé–“ã®$2\\times 2$ã‚°ãƒªãƒƒãƒ‰\n",
    "    - ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã ã¨$16\\times 16$\n",
    "- Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’ä¸€ã¤ãŠãã«ã€8ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’ã‚‚ã¤MoEãƒ–ãƒ­ãƒƒã‚¯ã«ç½®ãæ›ãˆã‚‹\n",
    "- ç”»åƒã®æ½œåœ¨è¡¨ç¾ã®æŠ½å‡ºã«ã€[Stable Diffusion XL][2]ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹4ãƒãƒ£ãƒãƒ«ã®VAEã‚’ä½¿ç”¨\n",
    "    - [æœ€æ–°ã®16ãƒãƒ£ãƒãƒ«VAE][1]ã§ã‚‚å®Ÿé¨“\n",
    "- ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯[CLIP][4]ã®ä¸€éƒ¨ã‚’ä½¿ç”¨\n",
    "    - [T5-xxl][5]ã¯æ€§èƒ½ãŒé«˜ã„ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŸã‚ä»Šå›ã¯ä½¿ç”¨ã—ãªã„\n",
    "- Patch-Mixerã¯ã€4ã¤ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã§æ§‹æˆï¼ˆãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®10%æœªæº€ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼‰\n",
    "\n",
    "[1]: https://arxiv.org/abs/2403.03206\n",
    "[2]: https://arxiv.org/abs/2307.01952\n",
    "[3]: https://arxiv.org/abs/2212.09748\n",
    "[4]: https://arxiv.org/abs/2212.07143\n",
    "[5]: https://arxiv.org/abs/1910.10683"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23fb51",
   "metadata": {},
   "source": [
    "è¨“ç·´:\n",
    "\n",
    "- AdamWæœ€é©åŒ–é–¢æ•°ã‚’ä½¿ç”¨\n",
    "- ã‚³ã‚µã‚¤ãƒ³å­¦ç¿’ç‡æ¸›è¡°ï¼ˆCosine learnig rate decayï¼‰ã‚’ä½¿ç”¨\n",
    "- é«˜ã„é‡ã¿æ¸›è¡°ï¼ˆWeight decayï¼‰ã‚’ä½¿ç”¨\n",
    "- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬è¨­å®šã¯[EDMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯][1]ã‚’ä½¿ç”¨\n",
    "\n",
    "[1]: https://arxiv.org/abs/2206.00364"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6c280",
   "metadata": {},
   "source": [
    "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:\n",
    "\n",
    "3ã¤ã®å®Ÿç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ï¼ˆåˆè¨ˆ2200ä¸‡ãƒšã‚¢ã®ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆï¼‰:\n",
    "\n",
    "- [Conceptual Captionsï¼ˆCC12Mï¼‰][1]\n",
    "- [Segment Anythingï¼ˆSA1Bï¼‰][2]\n",
    "    - ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¯LLaVAã§ä½œæˆ\n",
    "- [TextCaps][3]\n",
    "\n",
    "å¤§è¦æ¨¡å­¦ç¿’ã§ã¯æ›´ã«2ã¤ã®åˆæˆç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ï¼ˆåˆè¨ˆ1500ä¸‡ãƒšã‚¢ï¼‰:\n",
    "\n",
    "- [JourneyDB][4]\n",
    "- [DiffusionDB][5]\n",
    "\n",
    "å°è¦æ¨¡ã®æ¯”è¼ƒå®Ÿé¨“ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ:\n",
    "\n",
    "- cifar-captions\n",
    "    - [COYO-700M][6]ã‹ã‚‰CIFAR-10ã®10ã‚¯ãƒ©ã‚¹ã®ç”»åƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸã‚‚ã®\n",
    "\n",
    "[1]: https://arxiv.org/abs/2102.08981\n",
    "[2]: https://arxiv.org/abs/2304.02643\n",
    "[3]: https://arxiv.org/abs/2003.12462\n",
    "[4]: https://arxiv.org/abs/2307.00716\n",
    "[5]: https://arxiv.org/abs/2210.14896\n",
    "[6]: https://github.com/kakaobrain/coyo-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8680dce",
   "metadata": {},
   "source": [
    "è©•ä¾¡æŒ‡æ¨™:\n",
    "\n",
    "- [FIDï¼ˆFrÃ©chet Inception Distanceï¼‰][1]\n",
    "    - ç”Ÿæˆç”»åƒã®å“è³ªã¨å¤šæ§˜æ€§ã‚’æ¸¬ã‚‹æŒ‡æ¨™\n",
    "    - Inception-v3ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸã‚‚ã®ã¨ã€CLIpãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸã‚‚ã®ã§æ¸¬å®š\n",
    "    - ä½ã„ã»ã©ã‚ˆã„\n",
    "- [CLIPã‚¹ã‚³ã‚¢][2]\n",
    "    - ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®æ•´åˆæ€§ã‚’æ¸¬ã‚‹æŒ‡æ¨™\n",
    "    - é«˜ã„ã»ã©ã‚ˆã„\n",
    "\n",
    "[1]: https://arxiv.org/abs/1706.08500\n",
    "[2]: https://arxiv.org/abs/2104.08718"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c2465",
   "metadata": {},
   "source": [
    "## Deferred Maskingã®æœ‰åŠ¹æ€§è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72039a",
   "metadata": {},
   "source": [
    "è©•ä¾¡è¨­å®š:\n",
    "\n",
    "- DiT-Tiny/2ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
    "- cifar-captionsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ256x256ï¼‰ã‚’ä½¿ç”¨\n",
    "- å„ãƒ¢ãƒ‡ãƒ«ã‚’60,000ã‚¹ãƒ†ãƒƒãƒ—ã§å­¦ç¿’\n",
    "- AdamWæœ€é©åŒ–é–¢æ•°ã‚’ä½¿ç”¨\n",
    "- æœ€å¾Œã®10,000ã‚¹ãƒ†ãƒƒãƒ—ã§å¹³æ»‘åŒ–ä¿‚æ•°0.995ã®EMAï¼ˆExponential Moving Averageï¼‰ã‚’æœ‰åŠ¹åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34181a3e",
   "metadata": {},
   "source": [
    "Deferred Maskingã¯ã€ãƒã‚¹ã‚¯ãŒ75%ã‚’è¶…ãˆã‚‹ã¨å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301ec49",
   "metadata": {},
   "source": [
    "![](image/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1a057",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆOut-of-boxï¼‰ã‹ã‚‰ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ”¹è‰¯ã—ã¦ã„ã£ãŸçµŒé:\n",
    "\n",
    "![](image/fig5a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc7a05",
   "metadata": {},
   "source": [
    "- Out-of-box: æ”¹è‰¯å‰\n",
    "    - ãƒã‚¹ã‚­ãƒ³ã‚°ç‡ 75%\n",
    "    - å­¦ç¿’ç‡ $1.6\\times 10^{-4}$\n",
    "    - é‡ã¿æ¸›è¡° 0.01\n",
    "    - ãƒã‚¤ã‚ºåˆ†å¸ƒ EDMæ¨å¥¨ã®$(P_\\text{mean}, P_\\text{std}) = (-1.2, 1.2)$\n",
    "- Higher weight decay: å¼·ã„é‡ã¿æ¸›è¡°\n",
    "    - 0.01ã‹ã‚‰0.1ã¸\n",
    "- Sigma distribution: ãƒã‚¤ã‚ºåˆ†å¸ƒã®å¤‰æ›´\n",
    "    - $(-0.6, 1.2)$ã«å¤‰æ›´\n",
    "    - ç”»è³ªï¼ˆFIDï¼‰ã¯ä¸‹ãŒã£ãŸãŒã€æŒ‡ç¤ºè¿½å¾“æ€§ï¼ˆClip-scoreï¼‰ã‚’é«˜ã‚ãŸ\n",
    "- Larget patch-mixer: ãƒ‘ãƒƒãƒãƒŸã‚­ã‚µãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã™\n",
    "    - FIDãŒå¤§å¹…ã«æ”¹å–„\n",
    "- Higher learning rate: å­¦ç¿’ç‡ã‚’é«˜ã‚ã¦ã€è¨“ç·´æ™‚é–“ã‚’çŸ­ç¸®\n",
    "    - å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒå°‘ãªã„ãŸã‚ã€å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹æ‰‹å‰ã®æœ€å¤§å€¤ã¾ã§å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ã¨FIDã‚‚æ”¹å–„ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48dac7",
   "metadata": {},
   "source": [
    "ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚­ãƒ³ã‚°ã¨ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¹ã‚­ãƒ³ã‚°ã®æ¯”è¼ƒå®Ÿé¨“ã§ã¯ã€å‰è€…ã®ã»ã†ãŒæ€§èƒ½ãŒé«˜ã‹ã£ãŸ:\n",
    "\n",
    "![](image/fig5b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73296885",
   "metadata": {},
   "source": [
    "## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹è‰¯ã®æ¤œè¨¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99db721",
   "metadata": {},
   "source": [
    "Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’å±¤ã”ã¨ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã¨æ€§èƒ½ãŒæ”¹å–„ï¼ˆLayer-wise scalingï¼‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f322bff",
   "metadata": {},
   "source": [
    "![](image/table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75af38",
   "metadata": {},
   "source": [
    "MoEãƒ–ãƒ­ãƒƒã‚¯ã®ä½¿ç”¨ã¯æ”¹å–„ãŒè¦‹ã‚‰ã‚Œãªã‹ã£ãŸ:\n",
    "\n",
    "- CLIPã‚¹ã‚³ã‚¢ãŒã€28.11ã‹ã‚‰28.66ã¸ã‚ãšã‹ã«æ”¹å–„\n",
    "- FIDã‚¹ã‚³ã‚¢ãŒã€6.92ã‹ã‚‰6.98ã¸æ‚ªåŒ–\n",
    "- å›ºå®šã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§MoEã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å­¦ç¿’é‡ãŒç›¸å¯¾çš„ã«æ¸›ã‚‹ãŸã‚ã ã¨ä»®èª¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07363e",
   "metadata": {},
   "source": [
    "Patch-mixerã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€é«˜ã„ãƒã‚¹ã‚¯ç‡ã§MaskDiTã‚ˆã‚Šã‚‚æ€§èƒ½ãŒå‘ä¸Š:\n",
    "\n",
    "![](image/fig6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0010941",
   "metadata": {},
   "source": [
    "## Deferred Maskingã®æœ‰åŠ¹æ€§ã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e09d5",
   "metadata": {},
   "source": [
    "ç·å­¦ç¿’è¨ˆç®—é‡ã‚’ã‹ãˆãšã«ã€Transformerã®ãƒ€ã‚¦ãƒ³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨Deferred Maskingã‚’æ¯”è¼ƒ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bd23e",
   "metadata": {},
   "source": [
    "![](image/fig7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330e093",
   "metadata": {},
   "source": [
    "åŒä¸€è¨ˆç®—ã‚³ã‚¹ãƒˆã§ã€ãƒã‚¹ã‚¯ç„¡ã—ã®äº‹å‰å­¦ç¿’ï¼ˆç ´ç·šï¼‰ã¨ãƒã‚¹ã‚¯ã‚ã‚Šã®äº‹å‰å­¦ç¿’ï¼‹ãƒã‚¹ã‚¯ãªã—ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé’ç·šï¼‰ã‚’æ¯”è¼ƒ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4cfd3",
   "metadata": {},
   "source": [
    "![](image/fig7b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959fe684",
   "metadata": {},
   "source": [
    "å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®2æ®µéšå­¦ç¿’ã«ãŠã‘ã‚‹è¨ˆç®—ã‚³ã‚¹ãƒˆ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08047fe0",
   "metadata": {},
   "source": [
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d81d2",
   "metadata": {},
   "source": [
    "1. $256\\times 256$ã®è§£åƒåº¦ã§ãƒã‚¹ã‚¯ã‚ã‚Šã§å­¦ç¿’\n",
    "2. $256\\times 256$ã®è§£åƒåº¦ã§ãƒã‚¹ã‚¯ãªã—ã§å­¦ç¿’\n",
    "3. $512\\times 512$ã®è§£åƒåº¦ã§ãƒã‚¹ã‚¯ã‚ã‚Šã§å­¦ç¿’\n",
    "4. $512\\times 512$ã®è§£åƒåº¦ã§ãƒã‚¹ã‚¯ãªã—ã§å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e38a6",
   "metadata": {},
   "source": [
    "## ç”»åƒç”Ÿæˆèƒ½åŠ›ã®æ¤œè¨¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881dec1c",
   "metadata": {},
   "source": [
    "å®Ÿç”»åƒã¨åˆæˆç”»åƒã‚’çµ„ã¿åˆã‚ã›ãŸ3700ä¸‡æšã®ç”»åƒã‚’ä½¿ç”¨\n",
    "\n",
    "ã¾ãŸã€40%ã®åˆæˆç”»åƒã‚’å«ã‚€ç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ¯”è¼ƒå®Ÿé¨“\n",
    "\n",
    "8ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’æŒã¤Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’1ã¤ãŠãã«é…ç½®ã—ãŸDiT-XL/2ã‚’ä½¿ç”¨ï¼ˆMicroDiTï¼‰\n",
    "\n",
    "å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹:\n",
    "\n",
    "- ãƒ•ã‚§ã‚¤ã‚º1\n",
    "    - $256\\times 256$ã®ç”»åƒã‚’ä½¿ç”¨\n",
    "    - 75ï¼…ã®ãƒãƒƒãƒãƒã‚¹ã‚­ãƒ³ã‚°ã§25ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã—ã€ãƒãƒƒãƒãƒã‚¹ã‚­ãƒ³ã‚°ç„¡ã—ã§3ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- ãƒ•ã‚§ã‚¤ã‚º2\n",
    "    - $512\\times 512$ã®ç”»åƒã‚’ä½¿ç”¨\n",
    "    - 75%ã®ãƒãƒƒãƒãƒã‚¹ã‚­ãƒ³ã‚°ã§5ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒãƒƒãƒãƒã‚¹ã‚­ãƒ³ã‚°ç„¡ã—ã§5000ã‚¹ãƒ†ãƒƒãƒ—ã‚’æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060a4ac",
   "metadata": {},
   "source": [
    "å®Ÿç”»åƒã®ã¿ã¨ã€å®Ÿç”»åƒï¼‹åˆæˆç”»åƒã§ã®GPT-4oã«ã‚ˆã‚‹è©•ä¾¡çµæœ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91d3cb",
   "metadata": {},
   "source": [
    "![](image/fig8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ac4ec",
   "metadata": {},
   "source": [
    "å®Ÿç”»åƒã®ã¿ã¨ã€å®Ÿç”»åƒï¼‹åˆæˆç”»åƒã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d48dea",
   "metadata": {},
   "source": [
    "![](image/fig9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e666d6d",
   "metadata": {},
   "source": [
    "## æ½œåœ¨ç©ºé–“ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã®æ¤œè¨¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814c9f7",
   "metadata": {},
   "source": [
    "é™ã‚‰ã‚ŒãŸè¨ˆç®—ã‚³ã‚¹ãƒˆã®å ´åˆã€16ãƒãƒ£ãƒãƒ«ã‚ˆã‚Šã‚‚4ãƒãƒ£ãƒãƒ«ã®ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½¿ç”¨ã—ãŸã»ã†ãŒæ€§èƒ½ãŒé«˜ã„:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8099dd9",
   "metadata": {},
   "source": [
    "![](image/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4ff84",
   "metadata": {},
   "source": [
    "## å…ˆè¡Œç ”ç©¶ã¨ã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412717c8",
   "metadata": {},
   "source": [
    "COCOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒç”Ÿæˆã®æ¯”è¼ƒ:\n",
    "\n",
    "![](image/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5bcee",
   "metadata": {},
   "source": [
    "GenEvalã«ã‚ˆã‚‹ç´°ã‹ã„ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯ã§ã®æ¯”è¼ƒ:\n",
    "\n",
    "![](image/table4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16b488",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041e148",
   "metadata": {},
   "source": [
    "### ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    \"mosaicml[tensorboard, wandb]\" \\\n",
    "    accelerate \\\n",
    "    beautifulsoup4 \\\n",
    "    datasets \\\n",
    "    diffusers \\\n",
    "    easydict \\\n",
    "    einops \\\n",
    "    fastparquet \\\n",
    "    huggingface_hub \\\n",
    "    hydra-core \\\n",
    "    mosaicml-streaming \\\n",
    "    omegaconf \\\n",
    "    open_clip_torch \\\n",
    "    pandas \\\n",
    "    timm \\\n",
    "    torchmetrics \\\n",
    "    tqdm \\\n",
    "    transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from collections.abc import Iterable\n",
    "from composer import Trainer\n",
    "from composer.algorithms import GradientClipping\n",
    "from composer.algorithms.low_precision_layernorm import apply_low_precision_layernorm\n",
    "from composer.callbacks import LRMonitor\n",
    "from composer.callbacks import OptimizerMonitor\n",
    "from composer.callbacks import RuntimeEstimator\n",
    "from composer.callbacks import SpeedMonitor\n",
    "from composer.core import Precision\n",
    "from composer.loggers import TensorboardLogger\n",
    "from composer.loggers.wandb_logger import WandBLogger\n",
    "from composer.models import ComposerModel\n",
    "from composer.optim import CosineAnnealingWithWarmupScheduler\n",
    "from composer.optim import ConstantScheduler\n",
    "from composer.utils import dist, reproducibility\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers.models.modeling_outputs import AutoencoderKLOutput\n",
    "from easydict import EasyDict\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from huggingface_hub import hf_hub_download\n",
    "from hydra import compose, initialize_config_dir\n",
    "from itertools import repeat\n",
    "from micro_diffusion.models.callbacks import LogDiffusionImages\n",
    "from micro_diffusion.models.callbacks import NaNCatcher\n",
    "from micro_diffusion.models.utils import text_encoder_embedding_format\n",
    "from multiprocessing import Pool\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from streaming import MDSWriter\n",
    "from streaming import Stream, StreamingDataset\n",
    "from streaming.base import MDSWriter\n",
    "from streaming.base.util import merge_index\n",
    "from timm.models.vision_transformer import PatchEmbed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5Tokenizer\n",
    "from typing import Callable, Dict, List, Optional, Sequence, Union\n",
    "from typing import List, Dict, Union, Optional\n",
    "from typing import List, Optional\n",
    "from typing import Optional, Tuple, Dict, Union, List, Any\n",
    "import hydra\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "# cuDNN(CUDA Deep Neural Network libraryï¼‰ã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–\n",
    "# 3-5%ã®é€Ÿåº¦å‘ä¸ŠãŒè¦‹è¾¼ã¾ã‚Œã‚‹\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®è¨­å®š\n",
    "seed = 18\n",
    "reproducibility.seed_all(seed)\n",
    "\n",
    "logger.info(f\"PyTorch {torch.__version__}\")\n",
    "logger.info(f\"Transformers {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9446431",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ROOT = os.path.expanduser(\"~\")\n",
    "CACHE_DIR = os.path.join(USER_ROOT, \".cache\", \"micro_diffusion\")\n",
    "\n",
    "DATA_DIR = os.path.join(CACHE_DIR, \"data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "logger.info(f\"{DATA_DIR=}\")\n",
    "\n",
    "MODEL_DIR = os.path.join(CACHE_DIR, \"models\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "logger.info(f\"{MODEL_DIR=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = {\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16,\n",
    "    'float32': torch.float32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82559f70",
   "metadata": {},
   "source": [
    "## DiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ab53c",
   "metadata": {},
   "source": [
    "![](image/dit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c6c486",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulate(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®å¹³å‡ã‚’ãšã‚‰ã—ï¼ˆshiftï¼‰ã€ã‚¹ã‚±ãƒ¼ãƒ«ã‚’èª¿æ•´ã™ã‚‹\n",
    "    Adaptive Layer Normalizationï¼ˆAdaLNï¼‰ã®æ‰‹æ³•ã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    æ¡ä»¶ä»˜ã‘ã‚’ç‰¹å¾´è¡¨ç¾ã«é©ç”¨ã™ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "    shiftã¨scaleã¯ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—tã¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿cã‚’MLPã«å…¥åŠ›ã—ã¦äºˆæ¸¬ã™ã‚‹\n",
    "\n",
    "    y = x * (1 + scale) + shift\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        shift (torch.Tensor): ã‚·ãƒ•ãƒˆé‡\n",
    "        scale (torch.Tensor): ã‚¹ã‚±ãƒ¼ãƒ«é‡\n",
    "    Returns:\n",
    "        torch.Tensor: å¤‰èª¿å¾Œã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_norm(norm_type: str, dim: int, eps: float = 1e-6) -> nn.Module:\n",
    "    \"\"\"\n",
    "    ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–å±¤ã‚’ä½œæˆã™ã‚‹\n",
    "    https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
    "\n",
    "    Args:\n",
    "        norm_type (str): æ­£è¦åŒ–ã®ç¨®é¡\n",
    "            - layernorm:\n",
    "                - ãƒã‚¤ã‚¢ã‚¹ã‚’æŒãŸãªã„LayerNorm\n",
    "            - np_layernorm:\n",
    "                - å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒãŸãªã„LayerNorm\n",
    "                - AdaLNã®å¤‰èª¿ã¯modulateé–¢æ•°ã§è¡Œã†ãŸã‚\n",
    "        dim (int): æ­£è¦åŒ–ã™ã‚‹æ¬¡å…ƒã®ã‚µã‚¤ã‚º\n",
    "        eps (float): æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å°ã•ãªå€¤\n",
    "    Returns:\n",
    "        nn.Module: æŒ‡å®šã•ã‚ŒãŸæ­£è¦åŒ–å±¤\n",
    "    \"\"\"\n",
    "    if norm_type == \"layernorm\":\n",
    "        return nn.LayerNorm(dim, eps=eps, bias=False)\n",
    "    elif norm_type == \"np_layernorm\":\n",
    "        return nn.LayerNorm(dim, eps=eps, elementwise_affine=False, bias=False)\n",
    "    else:\n",
    "        raise ValueError('Norm type not supported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa5473",
   "metadata": {},
   "source": [
    "### SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    DiTå†…éƒ¨ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ãƒ»ã‚»ãƒ«ãƒ•ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤\n",
    "\n",
    "    - QK-Normã§å­¦ç¿’ã‚’å®‰å®šåŒ–\n",
    "    - FlashAttentionã§è¨ˆç®—ã‚’é«˜é€ŸåŒ–ãƒ»VRAMä½¿ç”¨é‡å‰Šæ¸›\n",
    "    - ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»ãƒã‚¤ã‚¢ã‚¹ãªã—ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–å±¤ã‚’ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_eps: float = 1e-6,\n",
    "        hidden_dim: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Transformerã®å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®æ¬¡å…ƒ\n",
    "            num_heads: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ•°\n",
    "            qkv_bias: QKVç·šå½¢å±¤ã§ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            norm_eps: æ­£è¦åŒ–å±¤ã®ã‚¼ãƒ­é™¤ç®—é˜²æ­¢ç”¨ã‚¤ãƒ—ã‚·ãƒ­ãƒ³\n",
    "            hidden_dim: QKVç©ºé–“ã®æ¬¡å…ƒã€‚Noneã®å ´åˆã€å…¥åŠ›æ¬¡å…ƒã¨åŒã˜\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ– {dim=} {num_heads=} {qkv_bias=} {norm_eps=} {hidden_dim=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) å±æ€§ã®åˆæœŸåŒ–\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim\n",
    "\n",
    "        assert hidden_dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        logger.debug(f\"{self.head_dim=}\")\n",
    "\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        # 2) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–\n",
    "\n",
    "        # QKVã‚’åŒæ™‚ã«è¨ˆç®—ã™ã‚‹ç·šå½¢å±¤\n",
    "        self.qkv = nn.Linear(dim, hidden_dim * 3, bias=qkv_bias)\n",
    "\n",
    "        # å‡ºåŠ›ã®ç·šå½¢å±¤\n",
    "        self.proj = nn.Linear(hidden_dim, dim, bias=qkv_bias)\n",
    "\n",
    "        # Qã®æ­£è¦åŒ–å±¤\n",
    "        self.ln_q = create_norm(\n",
    "            'np_layernorm', dim=hidden_dim, eps=norm_eps\n",
    "        )\n",
    "\n",
    "        # Kã®æ­£è¦åŒ–å±¤\n",
    "        self.ln_k = create_norm(\n",
    "            'np_layernorm', dim=hidden_dim, eps=norm_eps\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ãƒ»ã‚»ãƒ«ãƒ•ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            mask (Optional[torch.Tensor]): ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆæœªä½¿ç”¨ï¼‰\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é †ä¼æ’­ {x.shape=}\")\n",
    "\n",
    "        # B: ãƒãƒƒãƒã‚µã‚¤ã‚º, N: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, C: ãƒãƒ£ãƒ³ãƒãƒ«æ•°\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # 1) QKVã®è¨ˆç®—\n",
    "\n",
    "        # (B, N, C) -> (B, N, 3 * hidden_dim)\n",
    "        # -> (B, N, 3, num_heads, head_dim)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Q, K, Vã«åˆ†å‰²\n",
    "        # unbindã¯ãƒ†ãƒ³ã‚½ãƒ«ã‚’æŒ‡å®šã—ãŸæ¬¡å…ƒã§åˆ†å‰²ã—ã€ã‚¿ãƒ—ãƒ«ã¨ã—ã¦è¿”ã™\n",
    "        # ãã‚Œãã‚Œ (B, N, num_heads, head_dim)\n",
    "        q, k, v = qkv.unbind(2)\n",
    "\n",
    "        # 2) QK-Normã®é©ç”¨\n",
    "\n",
    "        # (B, N, num_heads, head_dim) -> (B, N, num_heads * head_dim)\n",
    "        # -> ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ– -> (B, N, num_heads, head_dim)\n",
    "        q = self.ln_q(\n",
    "            q.view(B, N, self.num_heads * self.head_dim)\n",
    "        ).view(B, N, self.num_heads, self.head_dim).to(q.dtype)\n",
    "\n",
    "        # åŒæ§˜ã«Kã«ã‚‚é©ç”¨\n",
    "        k = self.ln_k(\n",
    "            k.view(B, N, self.num_heads * self.head_dim)\n",
    "        ).view(B, N, self.num_heads, self.head_dim).to(k.dtype)\n",
    "\n",
    "        # 3) ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ»ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—ï¼ˆFlashAttentionä½¿ç”¨ï¼‰\n",
    "\n",
    "        # (B, N, num_heads, head_dim) -> (B, num_heads, N, head_dim)\n",
    "        # -> FlashAttention -> (B, num_heads, N, head_dim)\n",
    "        # -> (B, N, num_heads, head_dim)\n",
    "        x = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q.transpose(1, 2),\n",
    "            k.transpose(1, 2),\n",
    "            v.transpose(1, 2),\n",
    "            is_causal=False\n",
    "        ).transpose(1, 2).contiguous()\n",
    "        \n",
    "        # 4) å‡ºåŠ›å±¤ã®é©ç”¨\n",
    "\n",
    "        # ãƒ˜ãƒƒãƒ‰ã‚’çµåˆ\n",
    "        # (B, N, num_heads, head_dim) -> (B, N, hidden_dim)\n",
    "        x = x.view(B, N, self.num_heads * self.head_dim)\n",
    "\n",
    "        # ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def custom_init(self, init_std: float) -> None:\n",
    "        \"\"\"\n",
    "        é‡ã¿ã®åˆæœŸåŒ–ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
    "        \"\"\"\n",
    "        logger.info(f\"SelfAttentionã®é‡ã¿ã‚’ã‚«ã‚¹ã‚¿ãƒ åˆæœŸåŒ– {init_std=}\")\n",
    "        nn.init.trunc_normal_(self.qkv.weight, mean=0.0, std=0.02)\n",
    "        nn.init.trunc_normal_(self.proj.weight, mean=0.0, std=init_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93796f",
   "metadata": {},
   "source": [
    "### CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ãƒ»ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤\n",
    "    DiTã®ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "\n",
    "    - ã‚¯ã‚¨ãƒªã¯ç”»åƒç‰¹å¾´é‡ã‹ã‚‰è¨ˆç®—ã—ã€ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã¯ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹\n",
    "    - QK-Normã§å­¦ç¿’ã‚’å®‰å®šåŒ–\n",
    "    - FlashAttentionã§è¨ˆç®—ã‚’é«˜é€ŸåŒ–ãƒ»VRAMä½¿ç”¨é‡å‰Šæ¸›\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_eps: float = 1e-6,\n",
    "        hidden_dim: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®æ¬¡å…ƒ\n",
    "            num_heads: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ•°\n",
    "            qkv_bias: QKVç·šå½¢å±¤ã§ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            norm_eps: æ­£è¦åŒ–å±¤ã®ã‚¼ãƒ­é™¤ç®—é˜²æ­¢ç”¨ã‚¤ãƒ—ã‚·ãƒ­ãƒ³\n",
    "            hidden_dim: QKVç©ºé–“ã®æ¬¡å…ƒï¼ˆNoneã®å ´åˆã€å…¥åŠ›æ¬¡å…ƒã¨åŒã˜ï¼‰\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ– {dim=} {num_heads=} {qkv_bias=} {norm_eps=} {hidden_dim=}\")\n",
    "\n",
    "        # 1) å±æ€§ã®è¨­å®š\n",
    "\n",
    "        super(CrossAttention, self).__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim\n",
    "\n",
    "        assert hidden_dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        # 2) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–\n",
    "\n",
    "        # å…¥åŠ›ã‹ã‚‰Qã‚’è¨ˆç®—ã™ã‚‹ç·šå½¢å±¤\n",
    "        self.q_linear = nn.Linear(dim, hidden_dim, bias=qkv_bias)\n",
    "\n",
    "        # æ¡ä»¶ä»˜ã‘ã‹ã‚‰Kã¨Vã‚’è¨ˆç®—ã™ã‚‹ç·šå½¢å±¤\n",
    "        self.kv_linear = nn.Linear(dim, hidden_dim*2, bias=qkv_bias)\n",
    "\n",
    "        # å‡ºåŠ›ã®ç·šå½¢å±¤\n",
    "        self.proj = nn.Linear(hidden_dim, dim, bias=qkv_bias)\n",
    "\n",
    "        # Qã®æ­£è¦åŒ–å±¤\n",
    "        self.ln_q = create_norm(\n",
    "            'np_layernorm', dim=hidden_dim, eps=norm_eps\n",
    "        )\n",
    "\n",
    "        # Kã®æ­£è¦åŒ–å±¤\n",
    "        self.ln_k = create_norm(\n",
    "            'np_layernorm', dim=hidden_dim, eps=norm_eps\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ãƒ»ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            cond: æ¡ä»¶ä»˜ã‘ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            mask: ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆæœªä½¿ç”¨ï¼‰\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) ã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼ã®è¨ˆç®—\n",
    "\n",
    "        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã€ãƒãƒ£ãƒ³ãƒãƒ«æ•°\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # ã‚¯ã‚¨ãƒªã®è¨ˆç®—\n",
    "        # (B, N, C) -> (B, N, hidden_dim) -> (B, N, num_heads, head_dim)\n",
    "        q = self.q_linear(x).reshape(B, N, self.num_heads, self.head_dim)\n",
    "\n",
    "        # ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®è¨ˆç®—\n",
    "        # (B, M, C) -> (B, M, 2 * hidden_dim)\n",
    "        # -> (B, M, 2, num_heads, head_dim)\n",
    "        kv = self.kv_linear(cond).reshape(\n",
    "            B, -1, 2, self.num_heads, self.head_dim\n",
    "        )\n",
    "\n",
    "        # Kã¨Vã«åˆ†å‰²\n",
    "        # ãã‚Œãã‚Œ (B, M, num_heads, head_dim)\n",
    "        k, v = kv.unbind(2)\n",
    "\n",
    "        # 2) QK-Normã®é©ç”¨\n",
    "\n",
    "        # Qã«æ­£è¦åŒ–ã‚’é©ç”¨\n",
    "        # (B, N, num_heads, head_dim) -> (B, N, num_heads * head_dim)\n",
    "        # -> ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ– -> (B, N, num_heads, head\n",
    "        q = self.ln_q(\n",
    "            q.view(B, N, self.num_heads * self.head_dim)\n",
    "        ).view(B, N, self.num_heads, self.head_dim).to(q.dtype)\n",
    "\n",
    "        # Kã«æ­£è¦åŒ–ã‚’é©ç”¨\n",
    "        # (B, M, num_heads, head_dim) -> (B, M, num_heads * head_dim)\n",
    "        # -> ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ– -> (B, M, num_heads, head_dim)\n",
    "        k = self.ln_k(\n",
    "            k.view(B, -1, self.num_heads * self.head_dim)\n",
    "        ).view(B, -1, self.num_heads, self.head_dim).to(k.dtype)\n",
    "\n",
    "        # 3) ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ»ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—ï¼ˆFlashAttentionä½¿ç”¨ï¼‰\n",
    "\n",
    "        # (B, N, num_heads, head_dim) -> (B, num_heads, N, head_dim)\n",
    "        # -> FlashAttention -> (B, num_heads, N, head_dim)\n",
    "        # -> (B, N, num_heads, head_dim)\n",
    "        x = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q.transpose(1, 2),\n",
    "            k.transpose(1, 2),\n",
    "            v.transpose(1, 2),\n",
    "            is_causal=False\n",
    "        ).transpose(1, 2).contiguous()\n",
    "        \n",
    "        # 4) å‡ºåŠ›å±¤ã®é©ç”¨\n",
    "\n",
    "        # ãƒ˜ãƒƒãƒ‰ã‚’çµåˆ\n",
    "        # (B, N, num_heads, head_dim) -> (B, N, hidden_dim)\n",
    "        x = x.view(B, -1, self.num_heads * self.head_dim)\n",
    "\n",
    "        # ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "        # (B, N, hidden_dim) -> (B, N, dim)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def custom_init(self, init_std: float) -> None:\n",
    "        logger.info(f\"CrossAttentionã®é‡ã¿ã‚’ã‚«ã‚¹ã‚¿ãƒ åˆæœŸåŒ– {init_std=}\")\n",
    "\n",
    "        for linear in (self.q_linear, self.kv_linear):\n",
    "            nn.init.trunc_normal_(linear.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        nn.init.trunc_normal_(self.proj.weight, mean=0.0, std=init_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c782ffe",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLPï¼ˆMulti-Layer Perceptronï¼‰ã®å®Ÿè£…\n",
    "\n",
    "    - timmãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å‚è€ƒ https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/mlp.py\n",
    "    - Dropoutå±¤ã¯å«ã¾ã‚Œã¦ã„ãªã„\n",
    "    - Transformerå†…éƒ¨ã§ã¯ãªãã€DiTã®æ¡ä»¶ä»˜ã‘ã®å‡¦ç†ã§ä½¿ç”¨ã™ã‚‹\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        act_layer: Any = lambda: nn.GELU(approximate=\"tanh\"),\n",
    "        norm_layer: Optional[Any] = None,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®æ¬¡å…ƒ\n",
    "            hidden_features: éš ã‚Œå±¤ã®ç‰¹å¾´æ•°ã€‚Noneã®å ´åˆã€in_featuresã¨åŒã˜\n",
    "            out_features: å‡ºåŠ›ç‰¹å¾´æ•°ã€‚Noneã®å ´åˆã€in_featuresã¨åŒã˜\n",
    "            act_layer: æ´»æ€§åŒ–é–¢æ•°ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿\n",
    "            norm_layer: æ­£è¦åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã€‚Noneã®å ´åˆã€Identityã‚’ä½¿ç”¨\n",
    "            bias: ç·šå½¢å±¤ã§ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"MLPã‚’åˆæœŸåŒ– {in_features=} {hidden_features=} {out_features=} {act_layer=} {norm_layer=} {bias=}\")\n",
    "\n",
    "        # 1) å±æ€§ã®è¨­å®š\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        # 2) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "\n",
    "        self.act = act_layer()\n",
    "\n",
    "        self.norm = norm_layer if norm_layer is not None else nn.Identity()\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.debug(f\"MLPã®é †ä¼æ’­ {x.shape=}\")\n",
    "\n",
    "        # 1) æœ€åˆã®å…¨çµåˆå±¤ã‚’é©ç”¨ã—ã€ä¸­é–“æ¬¡å…ƒã«æ‹¡å¼µ\n",
    "        x = self.fc1(x)\n",
    "        logger.debug(f\"MLP fc1å¾Œ {x.shape=}\")\n",
    "\n",
    "        # 2) æ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨\n",
    "\n",
    "        x = self.act(x)\n",
    "\n",
    "        # å¿…è¦ã«å¿œã˜ã¦æ­£è¦åŒ–å±¤ã‚’é©ç”¨\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 3) 2ç•ªç›®ã®å…¨çµåˆå±¤ã‚’é©ç”¨ã—ã€å…ƒã®æ¬¡å…ƒã«æˆ»ã™\n",
    "        x = self.fc2(x)\n",
    "        logger.debug(f\"MLP fc2å¾Œ {x.shape=}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    SiLUæ´»æ€§åŒ–é–¢æ•°ã‚’ç”¨ã„ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯\n",
    "\n",
    "    - DiTBlockå†…éƒ¨ã§ä½¿ç”¨ã•ã‚Œã‚‹Feed-forwardãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "    - SwiGLU(SiLU Gated Linear Unit)ã‚’æ¡ç”¨ã—é«˜æ€§èƒ½åŒ–\n",
    "    - SwiGLUã¯3ã¤ã®ç·šå½¢å±¤ã‚’ä½¿ã†ãŸã‚ã€é€šå¸¸ã®FFNã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒç­‰ã—ããªã‚‹ã‚ˆã†ã«èª¿æ•´\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        use_bias: bool,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã®æ¬¡å…ƒ\n",
    "            hidden_dim (int): 2ã¤ã®ç·šå½¢å±¤ã®é–“ã®éš ã‚Œæ¬¡å…ƒ\n",
    "            multiple_of (int): éš ã‚Œæ¬¡å…ƒã‚’ã“ã®å€¤ã®æœ€å°å…¬å€æ•°ã«ä¸¸ã‚ã‚‹\n",
    "            use_bias (bool): ç·šå½¢å±¤ã§ãƒã‚¤ã‚¢ã‚¹é …ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"FeedForwardãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ– {dim=} {hidden_dim=} {multiple_of=} {use_bias=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) å±æ€§ã®è¨­å®š\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        # éš ã‚Œæ¬¡å…ƒã‚’2/3ã«èª¿æ•´\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        logger.debug(f\"FeedForwardéš ã‚Œæ¬¡å…ƒèª¿æ•´å‰ {hidden_dim=}\")\n",
    "\n",
    "        # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åŠ¹ç‡ã®ãŸã‚ã«ã€multiple_ofã®æœ€å°å…¬å€æ•°ã«ä¸¸ã‚ã‚‹\n",
    "        self.hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        logger.debug(f\"FeedForwardéš ã‚Œæ¬¡å…ƒèª¿æ•´å¾Œ {self.hidden_dim=}\")\n",
    "        \n",
    "        # 2) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ– \n",
    "\n",
    "        # ã‚²ãƒ¼ãƒˆã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ç·šå½¢å±¤\n",
    "        self.w1 = nn.Linear(dim, self.hidden_dim, bias=use_bias)\n",
    "\n",
    "        # æœ€åˆã®ç·šå½¢å±¤\n",
    "        self.w2 = nn.Linear(dim, self.hidden_dim, bias=use_bias)\n",
    "\n",
    "        # å‡ºåŠ›ã®ç·šå½¢å±¤\n",
    "        self.w3 = nn.Linear(self.hidden_dim, dim, bias=use_bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        FeedForwardãƒ–ãƒ­ãƒƒã‚¯ã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.debug(f\"FeedForwardã®é †ä¼æ’­ {x.shape=}\")\n",
    "\n",
    "        # ã‚²ãƒ¼ãƒˆã®å‡ºåŠ›ã«SiLUæ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨ã—ã€ã‚‚ã†ä¸€æ–¹ã®ç·šå½¢å±¤ã®å‡ºåŠ›ã¨è¦ç´ ã”ã¨ã«ä¹—ç®—\n",
    "        # æœ€å¾Œã«å‡ºåŠ›ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
    "\n",
    "    def custom_init(self, init_std: float) -> None:\n",
    "        logger.info(f\"FeedForwardã®é‡ã¿ã‚’ã‚«ã‚¹ã‚¿ãƒ åˆæœŸåŒ– {init_std=}\")\n",
    "\n",
    "        nn.init.trunc_normal_(self.w1.weight, mean=0.0, std=0.02)\n",
    "        for linear in (self.w2, self.w3):\n",
    "            nn.init.trunc_normal_(linear.weight, mean=0.0, std=init_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc12771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardECMoe(nn.Module):\n",
    "    \"\"\"\n",
    "    Expert choice routingã‚’ä½¿ç”¨ã—ãŸMoEãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤\n",
    "\n",
    "    - å°‚é–€å®¶ãŒã€è‡ªåˆ†ãŒå¾—æ„ã¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šä½kå€‹ã‚’é¸ã¶ã“ã¨ã§ã€è² è·é›†ä¸­ã‚’å›é¿\n",
    "    - GELUæ´»æ€§åŒ–é–¢æ•°ã‚’ä½¿ç”¨\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_experts: int,\n",
    "        expert_capacity: float,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_experts (int): ãƒ¬ã‚¤ãƒ¤ãƒ¼å†…ã®å°‚é–€å®¶ã®æ•°\n",
    "            expert_capacity (float): å°‚é–€å®¶ã‚ãŸã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ±ºå®šã™ã‚‹å®¹é‡ä¿‚æ•°\n",
    "            dim (int): å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã®æ¬¡å…ƒ\n",
    "            hidden_dim (int): 2ã¤ã®ç·šå½¢å±¤ã®é–“ã®éš ã‚Œæ¬¡å…ƒ\n",
    "            multiple_of (int): éš ã‚Œæ¬¡å…ƒã‚’ã“ã®å€¤ã®æœ€å°å…¬å€æ•°ã«ä¸¸ã‚ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"FeedForwardECMoeã‚’åˆæœŸåŒ– {num_experts=} {expert_capacity=} {dim=} {hidden_dim=} {multiple_of=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) å±æ€§ã®è¨­å®š\n",
    "\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        self.expert_capacity = expert_capacity\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åŠ¹ç‡ã®ãŸã‚ã«ã€multiple_ofã®æœ€å°å…¬å€æ•°ã«ä¸¸ã‚ã‚‹\n",
    "        self.hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        # 2) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–\n",
    "\n",
    "        # å°‚é–€å®¶ã”ã¨ã«ç•°ãªã‚‹é‡ã¿ã‚’ä½œæˆ\n",
    "        self.w1 = nn.Parameter(\n",
    "            torch.ones(num_experts, dim, self.hidden_dim)\n",
    "        )\n",
    "\n",
    "        # å°‚é–€å®¶ã”ã¨ã«ç•°ãªã‚‹é‡ã¿ã‚’ä½œæˆ\n",
    "        self.w2 = nn.Parameter(\n",
    "            torch.ones(num_experts, self.hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "        self.gate = nn.Linear(dim, num_experts, bias=False)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        FeedForwardECMoeã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"FeedForwardECMoeã®é †ä¼æ’­ {x.shape=}\")\n",
    "\n",
    "        assert len(x.shape) == 3\n",
    "\n",
    "        # (batch_size, seq_len, dim)\n",
    "        n, t, d = x.shape\n",
    "\n",
    "        # å°‚é–€å®¶ã‚ãŸã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "        tokens_per_expert = int(\n",
    "            self.expert_capacity * t / self.num_experts\n",
    "        )\n",
    "        logger.debug(f\"{tokens_per_expert=}\")\n",
    "\n",
    "        # 1) ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆå„å°‚é–€å®¶ã¨ã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼‰\n",
    "\n",
    "        # ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "        # (batch_size, seq_len, dim) -> (batch_size, seq_len, num_experts)\n",
    "        scores = self.gate(x)\n",
    "\n",
    "        # Softmaxã§ç¢ºç‡ã«å¤‰æ›\n",
    "        # (batch_size, seq_len, num_experts)\n",
    "        probs = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 2) ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹mã¨ã‚¹ã‚³ã‚¢gã‚’å–å¾—\n",
    "        # (batch_size, seq_len, num_experts)\n",
    "        # -> (batch_size, num_experts, tokens_per_expert)\n",
    "        # -> (batch_size, num_experts, tokens_per_expert),\n",
    "        #    (batch_size, num_experts, tokens_per_expert)\n",
    "        g, m = torch.topk(\n",
    "            probs.permute(0, 2, 1),\n",
    "            tokens_per_expert,\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # 3) å°‚é–€å®¶ã®é©ç”¨\n",
    "\n",
    "        # ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "        # (batch_size, num_experts, top_k, seq_len)\n",
    "        p = F.one_hot(m, num_classes=t).float()\n",
    "        logger.debug(f\"ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ {p.shape=}\")\n",
    "\n",
    "        # é¸ã°ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å„å°‚é–€å®¶ã«åˆ†é…\n",
    "        # (batch_size, num_experts, top_k, dim)\n",
    "        xin = torch.einsum('nekt, ntd -> nekd', p, x)\n",
    "        logger.debug(f\"å°‚é–€å®¶ã¸ã®åˆ†é… {xin.shape=}\")\n",
    "\n",
    "        # å„å°‚é–€å®¶ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’é©ç”¨\n",
    "        # (batch_size, num_experts, top_k, 4 * dim)\n",
    "        h = torch.einsum('nekd, edf -> nekf', xin, self.w1)\n",
    "        logger.debug(f\"å°‚é–€å®¶ã®æœ€åˆã®ç·šå½¢å±¤å¾Œ {h.shape=}\")\n",
    "\n",
    "        # GELUæ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨\n",
    "        h = self.gelu(h)\n",
    "\n",
    "        # å‡ºåŠ›ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "        # (batch_size, num_experts, top_k, dim)\n",
    "        h = torch.einsum('nekf, efd -> nekd', h, self.w2)\n",
    "        logger.debug(f\"å°‚é–€å®¶ã®å‡ºåŠ›ç·šå½¢å±¤å¾Œ {h.shape=}\")\n",
    "\n",
    "        # 4) å‡ºåŠ›ã®é›†ç´„\n",
    "\n",
    "        # å°‚é–€å®¶ã®å‡ºåŠ›ã‚’é‡ã¿ä»˜ã‘ã—ã¦é›†ç´„\n",
    "        # (batch_size, num_experts, top_k, dim)\n",
    "        out = g.unsqueeze(dim=-1) * h\n",
    "        logger.debug(f\"é‡ã¿ä»˜ã‘å¾Œ {out.shape=}\")\n",
    "\n",
    "        # (batch_size, seq_len, dim)\n",
    "        out = torch.einsum('nekt, nekd -> ntd', p, out)\n",
    "        logger.debug(f\"æœ€çµ‚å‡ºåŠ› {out.shape=}\")\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def custom_init(self, init_std: float):\n",
    "        logger.info(f\"FeedForwardECMoeã®é‡ã¿ã‚’ã‚«ã‚¹ã‚¿ãƒ åˆæœŸåŒ– {init_std=}\")\n",
    "        nn.init.trunc_normal_(self.gate.weight, mean=0.0, std=0.02)\n",
    "        nn.init.trunc_normal_(self.w1, mean=0.0, std=0.02)\n",
    "        nn.init.trunc_normal_(self.w2, mean=0.0, std=init_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c5e99d",
   "metadata": {},
   "source": [
    "### TimestepEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestepEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’åŸ‹ã‚è¾¼ã‚€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    DiTã®AdaLNã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¡ä»¶ä»˜ã‘ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        act_layer: Any,\n",
    "        frequency_embedding_size: int = 512\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): éš ã‚Œæ¬¡å…ƒã®ã‚µã‚¤ã‚º\n",
    "            act_layer (Any): æ´»æ€§åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿\n",
    "            frequency_embedding_size (int): å‘¨æ³¢æ•°åŸ‹ã‚è¾¼ã¿ã®ã‚µã‚¤ã‚º\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–\n",
    "\n",
    "        # MLPã®æ§‹ç¯‰\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
    "            act_layer(),\n",
    "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
    "        )\n",
    "\n",
    "        # 2) å±æ€§ã®è¨­å®š\n",
    "\n",
    "        self.frequency_embedding_size = frequency_embedding_size\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(t: torch.Tensor, dim: int, max_period: int = 10000) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        æ­£å¼¦æ³¢åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            t (torch.Tensor): ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            dim (int): åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒ\n",
    "            max_period (int): å‘¨æ³¢æ•°ã®æœ€å¤§å‘¨æœŸ\n",
    "        Returns:\n",
    "            torch.Tensor: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆ {t.shape=} {dim=} {max_period=}\")\n",
    "\n",
    "        half = dim // 2\n",
    "\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(\n",
    "                start=0,\n",
    "                end=half,\n",
    "                dtype=torch.float32,\n",
    "                device=t.device\n",
    "            ) / half\n",
    "        )\n",
    "        logger.debug(f\"{freqs=}\")\n",
    "\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "\n",
    "        # æ­£å¼¦æ³¢ã¨ä½™å¼¦æ³¢ã‚’è¨ˆç®—ã—ã¦çµåˆ\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "\n",
    "        # æ¬¡å…ƒãŒå¥‡æ•°ã®å ´åˆã€ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ \n",
    "        if dim % 2:\n",
    "            embedding = torch.cat(\n",
    "                [embedding, torch.zeros_like(embedding[:, :1])],\n",
    "                dim=-1\n",
    "            )\n",
    "\n",
    "        logger.debug(f\"ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« {embedding.shape=}\")\n",
    "\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            t (torch.Tensor): ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ã®é †ä¼æ’­ {t.shape=}\")\n",
    "\n",
    "        # 1) ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å‘¨æ³¢æ•°åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—\n",
    "\n",
    "        t_freq = self.timestep_embedding(\n",
    "            t, self.frequency_embedding_size\n",
    "        ).to(self.dtype)\n",
    "\n",
    "        # 2) MLPã‚’é©ç”¨ã—ã¦æœ€çµ‚çš„ãªåŸ‹ã‚è¾¼ã¿ã‚’å–å¾—\n",
    "\n",
    "        return self.mlp(t_freq)\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25999ac",
   "metadata": {},
   "source": [
    "### CaptionProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç‰¹å¾´é‡ã‚’DiTç”¨ã«å¤‰æ›ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    å†…éƒ¨ã§ã¯MLPã‚’ä½¿ç”¨\n",
    "    \n",
    "    Projects caption embeddings to model dimension.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        hidden_size (int): Size of hidden dimension\n",
    "        act_layer (Any): Activation layer constructor\n",
    "        norm_layer (Any): Normalization layer constructor\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_size: int,\n",
    "        act_layer: Any,\n",
    "        norm_layer: Any\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°\n",
    "            hidden_size (int): éš ã‚Œæ¬¡å…ƒã®ã‚µã‚¤ã‚º\n",
    "            act_layer (Any): æ´»æ€§åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿\n",
    "            norm_layer (Any): æ­£è¦åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿\n",
    "        \"\"\"\n",
    "        logger.info(f\"CaptionProjectionã‚’åˆæœŸåŒ– {in_channels=} {hidden_size=} {act_layer=} {norm_layer=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.y_proj = Mlp(\n",
    "            in_features=in_channels,\n",
    "            hidden_features=hidden_size,\n",
    "            out_features=hidden_size,\n",
    "            act_layer=act_layer,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, caption: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        CaptionProjectionã®é †ä¼æ’­\n",
    "        Args:\n",
    "            caption (torch.Tensor): å…¥åŠ›ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: å¤‰æ›å¾Œã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"CaptionProjectionã®é †ä¼æ’­ {caption.shape=}\")\n",
    "        result = self.y_proj(caption)\n",
    "        logger.debug(f\"å¤‰æ›å¾Œã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ã‚½ãƒ« {result.shape=}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bd18b",
   "metadata": {},
   "source": [
    "### AttentionBlockPromptEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlockPromptEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    CaptionProjectionã®å‡ºåŠ›ã‚’æ´—ç·´ã™ã‚‹ãŸã‚ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯\n",
    "\n",
    "    - ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§æ§‹æˆ\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Input and output dimension\n",
    "        head_dim (int): Dimension size per attention head\n",
    "        mlp_ratio (float): Multiplier for feed-forward network hidden dimension w.r.t input dim\n",
    "        multiple_of (int): Round feed-forward network hidden dimension up to nearest multiple of this value\n",
    "        norm_eps (float): Epsilon value for layer normalization\n",
    "        use_bias (bool): Whether to use bias terms in linear layers\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        head_dim: int, \n",
    "        mlp_ratio: float,\n",
    "        multiple_of: int,\n",
    "        norm_eps: float,\n",
    "        use_bias: bool,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã®æ¬¡å…ƒ\n",
    "            head_dim (int): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã‚ãŸã‚Šã®æ¬¡å…ƒã‚µã‚¤ã‚º\n",
    "            mlp_ratio (float): ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®éš ã‚Œæ¬¡å…ƒã‚’å…¥åŠ›æ¬¡å…ƒã«å¯¾ã—ã¦ä¹—ç®—ã™ã‚‹å€ç‡\n",
    "            multiple_of (int): ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®éš ã‚Œæ¬¡å…ƒã‚’ã“ã®å€¤ã®æœ€å°å…¬å€æ•°ã«ä¸¸ã‚ã‚‹\n",
    "            norm_eps (float): ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ã‚¤ãƒ—ã‚·ãƒ­ãƒ³å€¤\n",
    "            use_bias (bool): ç·šå½¢å±¤ã§ãƒã‚¤ã‚¢ã‚¹é …ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"AttentionBlockPromptEmbeddingã‚’åˆæœŸåŒ– {dim=} {head_dim=} {mlp_ratio=} {multiple_of=} {norm_eps=} {use_bias=}\")\n",
    "\n",
    "        # 1) å±æ€§ã®è¨­å®š\n",
    "\n",
    "        super().__init__()\n",
    "        assert dim % head_dim == 0, 'Hidden dimension must be divisible by head dim'\n",
    "        \n",
    "        self.dim = dim\n",
    "\n",
    "        self.num_heads = dim // head_dim\n",
    "        logger.debug(f\"{self.num_heads=}\")\n",
    "\n",
    "        # 2) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–\n",
    "        \n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.norm1 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "\n",
    "        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤\n",
    "        self.attn = SelfAttention(\n",
    "            dim=dim,\n",
    "            num_heads=self.num_heads,\n",
    "            qkv_bias=use_bias,\n",
    "            norm_eps=norm_eps,\n",
    "        )\n",
    "\n",
    "        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.norm2 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "\n",
    "        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤\n",
    "        self.mlp = FeedForward(\n",
    "            dim=dim,\n",
    "            hidden_dim=int(dim * mlp_ratio),\n",
    "            multiple_of=multiple_of,\n",
    "            use_bias=use_bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        AttentionBlockPromptEmbeddingã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"AttentionBlockPromptEmbeddingã®é †ä¼æ’­ {x.shape=}\")\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "    def custom_init(self, init_std: float = 0.02) -> None:\n",
    "        logger.info(f\"AttentionBlockPromptEmbeddingã®é‡ã¿ã‚’ã‚«ã‚¹ã‚¿ãƒ åˆæœŸåŒ– {init_std=}\")\n",
    "        self.attn.custom_init(init_std)\n",
    "        self.mlp.custom_init(init_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcda204",
   "metadata": {},
   "source": [
    "### DiTBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    DiTã‚’æ§‹æˆã™ã‚‹å˜ä¸€ã®Transformerãƒ–ãƒ­ãƒƒã‚¯\n",
    "\n",
    "    ç”»åƒãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å…¥åŠ›ã¨ã—ã€ãƒ†ã‚­ã‚¹ãƒˆyã¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—cã«ã‚ˆã‚‹æ¡ä»¶ä»˜ãã®å‡¦ç†\n",
    "\n",
    "    - ç”»åƒç‰¹å¾´é‡ã¯Self-Attentionã§å‡¦ç†\n",
    "    - ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘ã¯Cross-Attentionã§å®Ÿç¾\n",
    "    - ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¡ä»¶ä»˜ã¯ã€AdaLNã§shiftã¨scaleã¨gateã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã§å®Ÿç¾\n",
    "    - é€šå¸¸ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‹MoEã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’é¸æŠå¯èƒ½\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        head_dim: int,\n",
    "        mlp_ratio: float,\n",
    "        qkv_ratio: float,\n",
    "        multiple_of: int,\n",
    "        pooled_emb_dim: int,\n",
    "        norm_eps: float,\n",
    "        depth_init: bool,\n",
    "        layer_id: int,\n",
    "        num_layers: int,\n",
    "        compress_xattn: bool,\n",
    "        use_bias: bool,\n",
    "        moe_block: bool,\n",
    "        num_experts: int,\n",
    "        expert_capacity: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): ãƒ–ãƒ­ãƒƒã‚¯ã®å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã®æ¬¡å…ƒ\n",
    "            head_dim (int): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã‚ãŸã‚Šã®æ¬¡å…ƒ\n",
    "            mlp_ratio (float): MLPãƒ–ãƒ­ãƒƒã‚¯å†…ã®ç·šå½¢å±¤é–“ã®éš ã‚Œæ¬¡å…ƒã®å€ç‡\n",
    "            qkv_ratio (float): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯å†…ã®qkvå±¤ã®æ¬¡å…ƒã®å€ç‡\n",
    "            multiple_of (int): MLPãƒ–ãƒ­ãƒƒã‚¯å†…ã®éš ã‚Œæ¬¡å…ƒã‚’ã“ã®å€¤ã®æœ€å°å…¬å€æ•°ã«ä¸¸ã‚ã‚‹\n",
    "            pooled_emb_dim (int): ãƒ—ãƒ¼ãƒ«ã•ã‚ŒãŸã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒ\n",
    "            norm_eps (float): ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ã‚¤ãƒ—ã‚·ãƒ­ãƒ³\n",
    "            depth_init (bool): MLP/ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã®æœ€å¾Œã®å±¤ã®é‡ã¿ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦åˆæœŸåŒ–ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            layer_id (int): DiTãƒ¢ãƒ‡ãƒ«å†…ã®ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "            num_layers (int): DiTãƒ¢ãƒ‡ãƒ«å†…ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ç·æ•°\n",
    "            compress_xattn (bool): qkv_ratioã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®qkvæ¬¡å…ƒã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            use_bias (bool): ç·šå½¢å±¤ã§ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            moe_block (bool): MLPãƒ–ãƒ­ãƒƒã‚¯ã§å°‚é–€å®¶é¸æŠå‹ã®Mixture-of-Expertsã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            num_experts (int): MoEãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®å°‚é–€å®¶ã®æ•°\n",
    "            expert_capacity (float): MoEãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®å„å°‚é–€å®¶ã®å®¹é‡ä¿‚æ•°\n",
    "        \"\"\"\n",
    "        logger.info(f\"DiTBlockã‚’åˆæœŸåŒ– {dim=} {head_dim=} {mlp_ratio=} {qkv_ratio=} {multiple_of=} {pooled_emb_dim=} {norm_eps=} {depth_init=} {layer_id=} {num_layers=} {compress_xattn=} {use_bias=} {moe_block=} {num_experts=} {expert_capacity=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) å±æ€§ã®è¨­å®š\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        # qkvå±¤ã®éš ã‚Œæ¬¡å…ƒã‚’è¨ˆç®—\n",
    "        qkv_hidden_dim = (\n",
    "            (head_dim * 2) * ((int(dim * qkv_ratio) + head_dim * 2 - 1) // (head_dim * 2))\n",
    "            if qkv_ratio != 1 else dim\n",
    "        )\n",
    "        logger.debug(f\"{qkv_hidden_dim=}\")\n",
    "\n",
    "        # MLPã®éš ã‚Œæ¬¡å…ƒã‚’è¨ˆç®—\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        logger.debug(f\"{mlp_hidden_dim=}\")\n",
    "\n",
    "        # 2) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–\n",
    "\n",
    "        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.norm1 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "\n",
    "        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤\n",
    "        self.attn = SelfAttention(\n",
    "            dim=dim,\n",
    "            num_heads=qkv_hidden_dim // head_dim,\n",
    "            qkv_bias=use_bias,\n",
    "            norm_eps=norm_eps,\n",
    "            hidden_dim=qkv_hidden_dim,\n",
    "        )\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘ç”¨ã®ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤\n",
    "        self.cross_attn = CrossAttention(\n",
    "            dim=dim,\n",
    "            num_heads=qkv_hidden_dim // head_dim if compress_xattn else dim // head_dim,\n",
    "            qkv_bias=use_bias,\n",
    "            norm_eps=norm_eps,\n",
    "            hidden_dim=qkv_hidden_dim if compress_xattn else dim,\n",
    "        )\n",
    "\n",
    "        # ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.norm2 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "\n",
    "        # MLPå‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.norm3 = create_norm('layernorm', dim, eps=norm_eps)\n",
    "        \n",
    "        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ï¼ˆé€šå¸¸ã¾ãŸã¯MoEï¼‰\n",
    "        self.mlp = (\n",
    "            FeedForwardECMoe(num_experts, expert_capacity, dim, mlp_hidden_dim, multiple_of)\n",
    "            if moe_block else\n",
    "            FeedForward(dim, mlp_hidden_dim, multiple_of, use_bias)\n",
    "        )\n",
    "\n",
    "        # AdaLNã®ã‚·ãƒ•ãƒˆã¨ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã‚²ãƒ¼ãƒˆã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(pooled_emb_dim, 6 * dim, bias=True),\n",
    "        )\n",
    "        \n",
    "        # é‡ã¿ã®åˆæœŸåŒ–æ¨™æº–åå·®ã‚’è¨ˆç®—\n",
    "        self.weight_init_std = (\n",
    "            0.02 / (2 * (layer_id + 1)) ** 0.5 if depth_init else\n",
    "            0.02 / (2 * num_layers) ** 0.5\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, c: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        DiTBlockã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): ç”»åƒãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            y (torch.Tensor): ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            c (torch.Tensor): ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ¡ä»¶ä»˜ã‘ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"DiTBlockã®é †ä¼æ’­ {x.shape=} {y.shape=} {c.shape=}\")\n",
    "\n",
    "        # 1) AdaLNã®ã‚·ãƒ•ãƒˆã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ã‚²ãƒ¼ãƒˆã‚’äºˆæ¸¬\n",
    "\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (\n",
    "            self.adaLN_modulation(c).chunk(6, dim=1)\n",
    "        )\n",
    "        logger.debug(f\"{shift_msa.shape=} {scale_msa.shape=} {gate_msa.shape=} {shift_mlp.shape=} {scale_mlp.shape=} {gate_mlp.shape=}\")\n",
    "\n",
    "        # 2) Self-Attention\n",
    "\n",
    "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        # -> AdaLNã®shiftã¨scaleã‚’é©ç”¨\n",
    "        # -> ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\n",
    "        # -> AdaLNã®gateã‚’é©ç”¨\n",
    "        # -> æ®‹å·®æ¥ç¶šã‚’é©ç”¨\n",
    "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
    "\n",
    "        # 3) Cross-Attention\n",
    "\n",
    "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        # -> ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘ã‚’ç”¨ã„ãŸã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\n",
    "        # -> æ®‹å·®æ¥ç¶šã‚’é©ç”¨\n",
    "        x = x + self.cross_attn(self.norm2(x), y)\n",
    "\n",
    "        # 4) Feed-Forwardãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "\n",
    "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        # -> AdaLNã®shiftã¨scaleã‚’é©ç”¨\n",
    "        # -> ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’é©ç”¨\n",
    "        # -> AdaLNã®gateã‚’é©ç”¨\n",
    "        # -> æ®‹å·®æ¥ç¶šã‚’é©ç”¨\n",
    "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm3(x), shift_mlp, scale_mlp))\n",
    "\n",
    "        logger.debug(f\"DiTBlockå‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")\n",
    "        return x\n",
    "\n",
    "    def custom_init(self):\n",
    "        logger.info(f\"DiTBlockã®é‡ã¿ã‚’ã‚«ã‚¹ã‚¿ãƒ åˆæœŸåŒ–\")\n",
    "\n",
    "        for norm in (self.norm1, self.norm2, self.norm3):\n",
    "            norm.reset_parameters()\n",
    "        self.attn.custom_init(self.weight_init_std)\n",
    "        self.cross_attn.custom_init(self.weight_init_std)\n",
    "        self.mlp.custom_init(self.weight_init_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca184e",
   "metadata": {},
   "source": [
    "### T2IFinalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T2IFinalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    DiTãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚å‡ºåŠ›å±¤\n",
    "\n",
    "    - Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›ã‚’ç”»åƒãƒ‘ãƒƒãƒã«å¤‰æ›ã™ã‚‹\n",
    "    - å‡ºåŠ›ç›´å‰ã«ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æƒ…å ±cã‚’ç”¨ã„ãŸAdaLNã‚’é©ç”¨ã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        time_emb_dim: int,\n",
    "        patch_size: int,\n",
    "        out_channels: int,\n",
    "        act_layer: Any,\n",
    "        norm_final: nn.Module\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): éš ã‚Œæ¬¡å…ƒã®ã‚µã‚¤ã‚º\n",
    "            time_emb_dim (int): ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒ\n",
    "            patch_size (int): ç”»åƒãƒ‘ãƒƒãƒã®ã‚µã‚¤ã‚º\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°\n",
    "            act_layer (Any): æ´»æ€§åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿\n",
    "            norm_final (nn.Module): æœ€çµ‚æ­£è¦åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼\n",
    "        \"\"\"\n",
    "        logger.info(f\"T2IFinalLayerã‚’åˆæœŸåŒ– {hidden_size=} {time_emb_dim=} {patch_size=} {out_channels=} {act_layer=} {norm_final=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–\n",
    "\n",
    "        # éš ã‚Œæ¬¡å…ƒã‹ã‚‰ç”»åƒãƒ‘ãƒƒãƒã¸ã®ç·šå½¢å¤‰æ›\n",
    "        self.linear = nn.Linear(\n",
    "            hidden_size,\n",
    "            patch_size * patch_size * out_channels,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ã‹ã‚‰AdaLNã®shiftã¨scaleã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            act_layer(),\n",
    "            nn.Linear(time_emb_dim, 2 * hidden_size, bias=True)\n",
    "        )\n",
    "\n",
    "        # æœ€çµ‚æ­£è¦åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼\n",
    "        self.norm_final = norm_final\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        T2IFinalLayerã®é †ä¼æ’­\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            c (torch.Tensor): ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"T2IFinalLayerã®é †ä¼æ’­ {x.shape=} {c.shape=}\")\n",
    "\n",
    "        # AdaLNã®shiftã¨scaleã‚’äºˆæ¸¬\n",
    "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
    "\n",
    "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’é©ç”¨ã—ã€AdaLNã®shiftã¨scaleã‚’é©ç”¨\n",
    "        x = modulate(self.norm_final(x), shift, scale)\n",
    "\n",
    "        # ç·šå½¢å±¤ã‚’é©ç”¨ã—ã¦ç”»åƒãƒ‘ãƒƒãƒã«å¤‰æ›\n",
    "        x = self.linear(x)\n",
    "        logger.debug(f\"T2IFinalLayerå‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de00f8f",
   "metadata": {},
   "source": [
    "### DiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2637c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1æ¬¡å…ƒã®æ­£å¼¦æ³¢ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã™ã‚‹\n",
    "    Attention is All You Needè«–æ–‡ã®æ‰‹æ³•ã«åŸºã¥ãä½ç½®åŸ‹ã‚è¾¼ã¿\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒ\n",
    "        pos (np.ndarray): ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é…åˆ— (M,)\n",
    "    Returns:\n",
    "        np.ndarray: ä½ç½®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« (M, D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"1Dæ­£å¼¦æ³¢ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ {embed_dim=} {pos.shape=}\")\n",
    "\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # 1) å‘¨æ³¢æ•°ï¼ˆomegaï¼‰ã®è¨ˆç®—\n",
    "\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    # 2) ä½ç½®ã¨å‘¨æ³¢æ•°ã®ç©ã‚’è¨ˆç®—\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    logger.debug(f\"ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ reshaped {pos.shape=}\")\n",
    "\n",
    "    # å…¨ã¦ã®ä½ç½®ã¨å‘¨æ³¢æ•°ã®çµ„ã¿åˆã‚ã›ã®ç©ã‚’è¨ˆç®—\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "    logger.debug(f\"ä½ç½®ã¨å‘¨æ³¢æ•°ã®ç© {out.shape=}\")\n",
    "\n",
    "    # 3) æ­£å¼¦æ³¢ã¨ä½™å¼¦æ³¢ã‚’è¨ˆç®—ã—ã¦çµåˆ\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    result = np.concatenate([emb_sin, emb_cos], axis=1)\n",
    "    return result  # (M, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbeb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    2æ¬¡å…ƒã®æ­£å¼¦æ³¢ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã™ã‚‹\n",
    "    ç”»åƒãƒ‘ãƒƒãƒã®ä½ç½®æƒ…å ±ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒ\n",
    "        grid (np.ndarray): ä½ç½®ã‚°ãƒªãƒƒãƒ‰ã®é…åˆ— (2, H, W)\n",
    "    Returns:\n",
    "        np.ndarray: ä½ç½®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« (H*W, D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"2Dæ­£å¼¦æ³¢ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ {embed_dim=} {grid.shape=}\")\n",
    "\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])\n",
    "    logger.debug(f\"æ°´å¹³ä½ç½®åŸ‹ã‚è¾¼ã¿ {emb_h.shape=}\")\n",
    "\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])\n",
    "    logger.debug(f\"å‚ç›´ä½ç½®åŸ‹ã‚è¾¼ã¿ {emb_w.shape=}\")\n",
    "\n",
    "    result = np.concatenate([emb_h, emb_w], axis=1)\n",
    "    return result  # (H*W, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f671ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntuple(n: int):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ã®å€¤ã‚’æŒ‡å®šã•ã‚ŒãŸé•·ã•ã®ã‚¿ãƒ—ãƒ«ã«å¤‰æ›ã™ã‚‹é–¢æ•°ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®é–¢æ•°\n",
    "    get_2d_sincos_pos_embed_from_gridé–¢æ•°ã§ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        n (int): ã‚¿ãƒ—ãƒ«ã®é•·ã•\n",
    "    Returns:\n",
    "        Callable: å…¥åŠ›ã‚’n-ã‚¿ãƒ—ãƒ«ã«å¤‰æ›ã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    def parse(x):\n",
    "        if isinstance(x, Iterable) and not isinstance(x, str):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef991cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(\n",
    "    embed_dim: int,\n",
    "    grid_size: Union[int, Tuple[int, int]],\n",
    "    cls_token: bool = False,\n",
    "    extra_tokens: int = 0,\n",
    "    pos_interp_scale: float = 1.0,\n",
    "    base_size: int = 16\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ç”»åƒã®ãƒ‘ãƒƒãƒã®2æ¬¡å…ƒæ­£å¼¦æ³¢ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒ\n",
    "        grid_size (Union[int, Tuple[int, int]]): ã‚°ãƒªãƒƒãƒ‰ã®ã‚µã‚¤ã‚ºï¼ˆæ•´æ•°ã¾ãŸã¯ã‚¿ãƒ—ãƒ«ï¼‰\n",
    "        cls_token (bool): ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ç”¨ã®ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹\n",
    "        extra_tokens (int): è¿½åŠ ã®ãƒˆãƒ¼ã‚¯ãƒ³ç”¨ã®ä½ç½®åŸ‹ã‚è¾¼ã¿æ•°\n",
    "        pos_interp_scale (float): ä½ç½®åŸ‹ã‚è¾¼ã¿ã®è£œé–“ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "        base_size (int): åŸºæº–ã¨ãªã‚‹ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º\n",
    "    Returns:\n",
    "        np.ndarray: ä½ç½®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« (N, D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"2Dæ­£å¼¦æ³¢ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ {embed_dim=} {grid_size=} {cls_token=} {extra_tokens=} {pos_interp_scale=} {base_size=}\")\n",
    "\n",
    "    # 1) ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºã®æ­£è¦åŒ–\n",
    "\n",
    "    to_2tuple = ntuple(2)\n",
    "\n",
    "    # ä¾‹ãˆã°å…¥åŠ›ãŒ32ã®å ´åˆã€(32, 32)ã«å¤‰æ›\n",
    "    if isinstance(grid_size, int):\n",
    "        grid_size = to_2tuple(grid_size)\n",
    "\n",
    "    # Interpolate position embeddings to adapt model across resolutions. Interestingly, without any interpolation\n",
    "    # the model does converge slowly at start (~1000 steps) but eventually achieves near similar qualitative performance.\n",
    "\n",
    "    # 2) ã‚°ãƒªãƒƒãƒ‰åº§æ¨™ã®è¨ˆç®—ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆè£œå®Œï¼‰\n",
    "\n",
    "    # è§£åƒåº¦ãŒå¤‰ã‚ã£ã¦ã‚‚åŒã˜ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã€\n",
    "    # base_sizeã¨pos_interp_scaleã«åŸºã¥ã„ã¦ã‚°ãƒªãƒƒãƒ‰ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "\n",
    "    grid_h = np.arange(grid_size[0], dtype=np.float32) / (grid_size[0] / base_size) / pos_interp_scale\n",
    "\n",
    "    grid_w = np.arange(grid_size[1], dtype=np.float32) / (grid_size[1] / base_size) / pos_interp_scale\n",
    "\n",
    "    # 3) ãƒ¡ãƒƒã‚·ãƒ¥ã‚°ãƒªãƒƒãƒ‰ã‚’ä½œæˆ\n",
    "\n",
    "    grid = np.meshgrid(grid_w, grid_h)\n",
    "\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size[1], grid_size[0]])\n",
    "\n",
    "    # 4) å®Ÿéš›ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ\n",
    "\n",
    "    # sinãƒ»cosé–¢æ•°ã§ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "\n",
    "    # 5) ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®å‡¦ç†\n",
    "\n",
    "    # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚„è¿½åŠ ãƒˆãƒ¼ã‚¯ãƒ³ç”¨ã®ã‚¼ãƒ­åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ \n",
    "    if cls_token and extra_tokens > 0:\n",
    "        pos_embed = np.concatenate(\n",
    "            [np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0\n",
    "        )\n",
    "\n",
    "    return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee89fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_out_token(x: torch.Tensor, ids_keep: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ç”»åƒãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ã€æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘ã‚’æ®‹ã™\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« (N, L, D)\n",
    "        ids_keep (torch.Tensor): æ®‹ã™ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (N, L_masked)\n",
    "    Returns:\n",
    "        torch.Tensor: ãƒã‚¹ã‚¯å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« (N, L_masked, D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯ã‚¢ã‚¦ãƒˆ {x.shape=} {ids_keep.shape=}\")\n",
    "\n",
    "    N, L, D = x.shape  # batch, length, dim\n",
    "\n",
    "    # ids_keepã‚’ç‰¹å¾´é‡ã®æ¬¡å…ƒDã«åˆã‚ã›ã¦æ‹¡å¼µã—ã€\n",
    "    # gatherã§ids_keepã®ãƒãƒƒãƒã ã‘ã‚’æŠœãå‡ºã™\n",
    "    x_masked = torch.gather(\n",
    "        x,\n",
    "        dim=1,\n",
    "        index=ids_keep.unsqueeze(-1).repeat(1, 1, D)\n",
    "    )\n",
    "    logger.debug(f\"ãƒã‚¹ã‚¯å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x_masked.shape=}\")\n",
    "\n",
    "    return x_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(batch: int, length: int, mask_ratio: float, device: torch.device) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    ç”»åƒãƒ‘ãƒƒãƒã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã‚’ç”Ÿæˆã™ã‚‹\n",
    "\n",
    "    torch.randpermã§ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã§ã¯ãªãã€ãƒã‚¤ã‚ºç”Ÿæˆ+argsortã§è¨ˆç®—ã‚’åŠ¹ç‡åŒ–\n",
    "\n",
    "    Args:\n",
    "        batch (int): ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "        length (int): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•\n",
    "        mask_ratio (float): ãƒã‚¹ã‚¯ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®å‰²åˆ\n",
    "        device (torch.device): ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«ã‚’é…ç½®ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "    Returns:\n",
    "        Dict[str, torch.Tensor]: \n",
    "            - mask: ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã€0ã¯ä¿æŒã€1ã¯å‰Šé™¤\n",
    "            - ids_keep: æ®‹ã™ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆmask_out_tokené–¢æ•°ã§ä½¿ç”¨ï¼‰\n",
    "            - ids_restore: å…ƒã®é †åºã‚’å¾©å…ƒã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆunmasking_tokensã§ä½¿ç”¨ï¼‰\n",
    "    \"\"\"\n",
    "    logger.info(f\"ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ {batch=} {length=} {mask_ratio=} {device=}\")\n",
    "\n",
    "    # ä¿æŒã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    len_keep = int(length * (1 - mask_ratio))\n",
    "    logger.debug(f\"{len_keep=}\")\n",
    "\n",
    "    # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¤ã‚ºã‚’ç”Ÿæˆï¼ˆ0ã‹ã‚‰1ã®ä¸€æ§˜åˆ†å¸ƒï¼‰\n",
    "    # ä¾‹: [0.8, 0.1, 0.5, 0.9]\n",
    "    # (batch, length)\n",
    "    noise = torch.rand(batch, length, device=device)\n",
    "\n",
    "    # ãƒã‚¤ã‚ºã‚’æ˜‡é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "    # ä¾‹: [1, 2, 0, 3]\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)\n",
    "\n",
    "    # å…ƒã®é †åºã‚’å¾©å…ƒã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "    # ä¾‹: [2, 0, 1, 3]\n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "    # å…ˆé ­ã‹ã‚‰len_keepå€‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "    # ä¾‹: [1, 2]\n",
    "    ids_keep = ids_shuffle[:, :len_keep]\n",
    "\n",
    "    # ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã‚’1ã§åˆæœŸåŒ–ï¼ˆ0:ä¿æŒã€1:å‰Šé™¤ï¼‰\n",
    "    # (batch, length)\n",
    "    # ä¾‹: [1, 1, 1, 1]\n",
    "    mask = torch.ones([batch, length], device=device)\n",
    "\n",
    "    # å…ˆé ­ã‹ã‚‰len_keepå€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿æŒï¼ˆ0ã®ä¿æŒã«è¨­å®šï¼‰\n",
    "    # ä¾‹: [0, 0, 1, 1]\n",
    "    mask[:, :len_keep] = 0\n",
    "\n",
    "    # å…ƒã®é †åºã«ãƒã‚¹ã‚¯ã‚’å¾©å…ƒ\n",
    "    # gatherã¯ids_restoreã«åŸºã¥ã„ã¦maskã‚’ä¸¦ã³æ›¿ãˆã‚‹\n",
    "    # ä¾‹: [1, 0, 0, 1]\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "    return {\n",
    "        'mask': mask,\n",
    "        'ids_keep': ids_keep,\n",
    "        'ids_restore': ids_restore\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1beb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmask_tokens(x: torch.Tensor, ids_restore: torch.Tensor, mask_token: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ãƒã‚¹ã‚­ãƒ³ã‚°ã•ã‚Œã¦å‡¦ç†ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«ã€ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è£œå®Œã—ã€å…ƒã®ç”»åƒã®é †åºã‚’å¾©å…ƒã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): ãƒã‚¹ã‚¯å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ— (N, L_masked, D)\n",
    "        ids_restore (torch.Tensor): å…ƒã®é †åºã‚’å¾©å…ƒã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (N, L)\n",
    "        mask_token (torch.Tensor): ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ (1, 1, D)\n",
    "    Returns:\n",
    "        torch.Tensor: å…ƒã®é †åºã«å¾©å…ƒã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ— (N, L, D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è£œå®Œã—ã¦å…ƒã®é †åºã‚’å¾©å…ƒ {x.shape=} {ids_restore.shape=} {mask_token.shape=}\")\n",
    "\n",
    "    # ä¸è¶³ã—ã¦ã„ã‚‹åˆ†ã®ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œæˆ\n",
    "    mask_tokens = mask_token.repeat(\n",
    "        x.shape[0],\n",
    "        ids_restore.shape[1] - x.shape[1],\n",
    "        1\n",
    "    )\n",
    "    logger.debug(f\"è£œå®Œç”¨ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ãƒ³ã‚½ãƒ« {mask_tokens.shape=}\")\n",
    "\n",
    "    # å‡¦ç†æ¸ˆã¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å¾Œã‚ã«ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é€£çµ\n",
    "    x_ = torch.cat([x, mask_tokens], dim=1)\n",
    "    logger.debug(f\"ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é€£çµå¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x_.shape=}\")\n",
    "\n",
    "    # å…ƒã®é †åºã«å¾©å…ƒ\n",
    "    x_ = torch.gather(\n",
    "        x_,\n",
    "        dim=1,\n",
    "        index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2])\n",
    "    )  # unshuffle\n",
    "\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c51c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiT(nn.Module):\n",
    "    \"\"\"\n",
    "    ç”»åƒã®æ½œåœ¨å¤‰æ•°ã®å…¥åŠ›ã¨ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ»ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¡ä»¶ä»˜ã‘ã¨ã—ã¦å—ã‘å–ã‚Šã€\n",
    "    ãƒã‚¤ã‚ºé™¤å»ã•ã‚ŒãŸç”»åƒã®æ½œåœ¨å¤‰æ•°ã‚’å‡ºåŠ›ã™ã‚‹\n",
    "\n",
    "    - Patched Mixerã‚’å®Ÿè£…\n",
    "    - Mixture-of-Experts (MoE) FFNãƒ¬ã‚¤ãƒ¤ãƒ¼ã«å¯¾å¿œ\n",
    "\n",
    "    \n",
    "    Args:\n",
    "        input_size (int, default: 32): Size of input image (assumed square)\n",
    "        patch_size (int, default: 2): Size of patches for patch embedding\n",
    "        in_channels (int, default: 4): Number of input image channels (by default assuming four channel latent space)\n",
    "        dim (int, default: 1152): Dimension of transformer backbone, i.e., dimension of major transformer layers\n",
    "        depth (int, default: 28): Number of transformer blocks\n",
    "        head_dim (int, default: 64): Dimension of each attention head\n",
    "        multiple_of (int, default: 256): Round hidden dimensions up to nearest multiple of this value in MLP block\n",
    "        caption_channels (int, default: 4096): Number of channels in caption embeddings\n",
    "        pos_interp_scale (float, default: 1.0): Scale for positional embedding interpolation (1.0 for 256x256, 2.0 for 512x512)\n",
    "        norm_eps (float, default: 1e-6): Epsilon for layer normalization\n",
    "        depth_init (bool, default: True): Whether to use depth-dependent initialization in DiT blocks\n",
    "        qkv_multipliers (List[float], default: [1.0]): Multipliers for QKV projection dimensions in DiT blocks\n",
    "        ffn_multipliers (List[float], default: [4.0]): Multipliers for FFN hidden dimensions in DiT blocks\n",
    "        use_patch_mixer (bool, default: True): Whether to use patch mixer layers\n",
    "        patch_mixer_depth (int, default: 4): Number of patch mixer blocks\n",
    "        patch_mixer_dim (int, default: 512): Dimension of patch-mixer layers\n",
    "        patch_mixer_qkv_ratio (float, default: 1.0): Multipliers for QKV projection dimensions in patch-mixer blocks\n",
    "        patch_mixer_mlp_ratio (float, default: 1.0): Multipliers for FFN hidden dimensions in patch-mixer blocks\n",
    "        use_bias (bool, default: True): Whether to use bias in linear layers\n",
    "        num_experts (int, default: 8):  Number of experts if using MoE block\n",
    "        expert_capacity (int, default: 1): Capacity factor for each expert if using MoE FFN layers\n",
    "        experts_every_n (int, default: 2): Add MoE FFN layers every n blocks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 32,\n",
    "        patch_size: int = 2,\n",
    "        in_channels: int = 4,\n",
    "        dim: int = 1152,\n",
    "        depth: int = 28,\n",
    "        head_dim: int = 64,\n",
    "        multiple_of: int = 256,\n",
    "        caption_channels: int = 1024,\n",
    "        pos_interp_scale: float = 1.0,\n",
    "        norm_eps: float = 1e-6,\n",
    "        depth_init: bool = True,\n",
    "        qkv_multipliers: List[float] = [1.0],\n",
    "        ffn_multipliers: List[float] = [4.0],\n",
    "        use_patch_mixer: bool = True,\n",
    "        patch_mixer_depth: int = 4,\n",
    "        patch_mixer_dim: int = 512,\n",
    "        patch_mixer_qkv_ratio: float = 1.0,\n",
    "        patch_mixer_mlp_ratio: float = 1.0,\n",
    "        use_bias: bool = True,\n",
    "        num_experts: int = 8,\n",
    "        expert_capacity: int = 1,\n",
    "        experts_every_n: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int, default: 32): å…¥åŠ›ç”»åƒã®ã‚µã‚¤ã‚ºï¼ˆæ­£æ–¹å½¢ã¨ä»®å®šï¼‰\n",
    "            patch_size (int, default: 2): ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ã®ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º\n",
    "            in_channels (int, default: 4): å…¥åŠ›ç”»åƒã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯4ãƒãƒ£ãƒãƒ«ã®æ½œåœ¨ç©ºé–“ã‚’æƒ³å®šï¼‰\n",
    "            dim (int, default: 1152): ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®æ¬¡å…ƒã€ã™ãªã‚ã¡ä¸»è¦ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ¬¡å…ƒ\n",
    "            depth (int, default: 28): ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®æ•°\n",
    "            head_dim (int, default: 64): å„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ\n",
    "            multiple_of (int, default: 256): MLPãƒ–ãƒ­ãƒƒã‚¯å†…ã®éš ã‚Œæ¬¡å…ƒã‚’ã“ã®å€¤ã®æœ€å°å…¬å€æ•°ã«ä¸¸ã‚ã‚‹\n",
    "            caption_channels (int, default: 4096): ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°\n",
    "            pos_interp_scale (float, default: 1.0): ä½ç½®åŸ‹ã‚è¾¼ã¿è£œé–“ã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ256x256ã®å ´åˆã¯1.0ã€512x512ã®å ´åˆã¯2.0ï¼‰\n",
    "            norm_eps (float, default: 1e-6): ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ã‚¤ãƒ—ã‚·ãƒ­ãƒ³\n",
    "            depth_init (bool, default: True): DiTãƒ–ãƒ­ãƒƒã‚¯ã§æ·±ã•ä¾å­˜ã®åˆæœŸåŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            qkv_multipliers (List[float], default: [1.0]): DiTãƒ–ãƒ­ãƒƒã‚¯å†…ã®QKVæŠ•å½±æ¬¡å…ƒã®ä¹—æ•°\n",
    "            ffn_multipliers (List[float], default: [4.0]): DiTãƒ–ãƒ­ãƒƒã‚¯å†…ã®FFNéš ã‚Œæ¬¡å…ƒã®ä¹—æ•°\n",
    "            use_patch_mixer (bool, default: True): ãƒ‘ãƒƒãƒãƒŸã‚­ã‚µãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            patch_mixer_depth (int, default: 4): ãƒ‘ãƒƒãƒãƒŸã‚­ã‚µãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®æ•°\n",
    "            patch_mixer_dim (int, default: 512): ãƒ‘ãƒƒãƒãƒŸã‚­ã‚µãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ¬¡å…ƒ\n",
    "            patch_mixer_qkv_ratio (float, default: 1.0): ãƒ‘ãƒƒãƒãƒŸã‚­ã‚µãƒ¼ãƒ–ãƒ­ãƒƒã‚¯å†…ã®QKVæŠ•å½±æ¬¡å…ƒã®ä¹—æ•°\n",
    "            patch_mixer_mlp_ratio (float, default: 1.0): ãƒ‘ãƒƒãƒãƒŸã‚­ã‚µãƒ¼ãƒ–ãƒ­ãƒƒã‚¯å†…ã®FFNéš ã‚Œæ¬¡å…ƒã®ä¹—æ•°\n",
    "        \"\"\"\n",
    "        logger.info(f\"DiTã‚’åˆæœŸåŒ– {input_size=} {patch_size=} {in_channels=} {dim=} {depth=} {head_dim=} {multiple_of=} {caption_channels=} {pos_interp_scale=} {norm_eps=} {depth_init=} {qkv_multipliers=} {ffn_multipliers=} {use_patch_mixer=} {patch_mixer_depth=} {patch_mixer_dim=} {patch_mixer_qkv_ratio=} {patch_mixer_mlp_ratio=} {use_bias=} {num_experts=} {expert_capacity=} {experts_every_n=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) åŸºæœ¬çš„ãªå±æ€§ã®è¨­å®š\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.head_dim = head_dim\n",
    "        self.pos_interp_scale = pos_interp_scale\n",
    "        self.use_patch_mixer = use_patch_mixer\n",
    "\n",
    "        # 2) åŸ‹ã‚è¾¼ã¿å±¤ã®åˆæœŸåŒ–\n",
    "        \n",
    "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "        # ç”»åƒãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿å±¤\n",
    "        self.x_embedder = PatchEmbed(\n",
    "            input_size, patch_size, in_channels, dim, bias=True\n",
    "        )\n",
    "\n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿å±¤\n",
    "        self.t_embedder = TimestepEmbedder(dim, approx_gelu)\n",
    "\n",
    "        # åŸºæœ¬ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºã®è¨ˆç®—\n",
    "        self.base_size = input_size // self.patch_size\n",
    "        logger.debug(f\"{self.base_size=}\")\n",
    "\n",
    "        # ãƒ‘ãƒƒãƒã®ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ç™»éŒ²\n",
    "        num_patches = self.x_embedder.num_patches\n",
    "        self.register_buffer(\"pos_embed\", torch.zeros(1, num_patches, dim))\n",
    "\n",
    "        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã¨å‰å‡¦ç†ã‚’è¡Œã†å±¤\n",
    "        self.y_embedder = CaptionProjection(\n",
    "            in_channels=caption_channels,\n",
    "            hidden_size=dim,\n",
    "            act_layer=approx_gelu,\n",
    "            norm_layer=create_norm('layernorm', dim, eps=norm_eps)\n",
    "        )\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã®äº‹å‰å‡¦ç†å±¤\n",
    "        self.y_emb_preprocess = AttentionBlockPromptEmbedding(\n",
    "            dim,\n",
    "            head_dim,\n",
    "            mlp_ratio=4.0,\n",
    "            multiple_of=multiple_of,\n",
    "            norm_eps=norm_eps,\n",
    "            use_bias=use_bias\n",
    "        )\n",
    "        \n",
    "        # ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«åŠ ç®—ã™ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "        self.pooled_y_emb_process = Mlp(\n",
    "            dim,\n",
    "            dim,\n",
    "            dim,\n",
    "            approx_gelu,\n",
    "            norm_layer=create_norm('layernorm', dim, eps=norm_eps)\n",
    "        )\n",
    "\n",
    "        # 3) Patch-Mixerãƒ–ãƒ­ãƒƒã‚¯ã®æ§‹ç¯‰\n",
    "\n",
    "        # Patch-Mixerã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.use_patch_mixer:\n",
    "\n",
    "            # MoEãƒ–ãƒ­ãƒƒã‚¯ã‚’é…ç½®ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "            expert_blocks_idx = [\n",
    "                i for i in range(1, patch_mixer_depth) \n",
    "                if (i+1) % experts_every_n == 0\n",
    "            ]\n",
    "            logger.debug(f\"{expert_blocks_idx=}\")\n",
    "\n",
    "            # MoEãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ•ãƒ©ã‚°ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "            is_moe_block = [\n",
    "                True if i in expert_blocks_idx else False \n",
    "                for i in range(patch_mixer_depth)\n",
    "            ]\n",
    "            logger.debug(f\"{is_moe_block=}\")\n",
    "                 \n",
    "            # Patch-Mixerãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "            self.patch_mixer = nn.ModuleList([\n",
    "                DiTBlock(\n",
    "                    dim=patch_mixer_dim,\n",
    "                    head_dim=head_dim,\n",
    "                    mlp_ratio=patch_mixer_mlp_ratio,\n",
    "                    qkv_ratio=patch_mixer_qkv_ratio,\n",
    "                    multiple_of=multiple_of,\n",
    "                    pooled_emb_dim=dim,\n",
    "                    norm_eps=norm_eps,\n",
    "                    depth_init=False,\n",
    "                    layer_id=0,\n",
    "                    num_layers=depth,\n",
    "                    compress_xattn=False,\n",
    "                    use_bias=use_bias,\n",
    "                    moe_block=is_moe_block[i],\n",
    "                    num_experts=num_experts,\n",
    "                    expert_capacity=expert_capacity\n",
    "                ) for i in range(patch_mixer_depth)\n",
    "            ])\n",
    "\n",
    "            # Patch-Mixerã®æ¬¡å…ƒãŒå…¥åŠ›æ¬¡å…ƒã¨ç•°ãªã‚‹å ´åˆ\n",
    "            if patch_mixer_dim != dim:\n",
    "\n",
    "                # å…¥åŠ›æ¬¡å…ƒã‹ã‚‰Patch-Mixeræ¬¡å…ƒã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°å±¤\n",
    "                self.patch_mixer_map_xin = nn.Sequential(\n",
    "                    create_norm('layernorm', dim, eps=norm_eps),\n",
    "                    nn.Linear(dim, patch_mixer_dim, bias=use_bias)\n",
    "                )\n",
    "\n",
    "                # Patch-Mixeræ¬¡å…ƒã‹ã‚‰å‡ºåŠ›æ¬¡å…ƒã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°å±¤\n",
    "                self.patch_mixer_map_xout = nn.Sequential(\n",
    "                    create_norm('layernorm', patch_mixer_dim, eps=norm_eps),\n",
    "                    nn.Linear(patch_mixer_dim, dim, bias=use_bias)\n",
    "                )\n",
    "\n",
    "                # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‹ã‚‰Patch-Mixeræ¬¡å…ƒã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°å±¤\n",
    "                self.patch_mixer_map_y = nn.Sequential(\n",
    "                    create_norm('layernorm', dim, eps=norm_eps),\n",
    "                    nn.Linear(dim, patch_mixer_dim, bias=use_bias)\n",
    "                )\n",
    "\n",
    "            # åŒã˜å ´åˆ\n",
    "            else:\n",
    "                # ãã®ã¾ã¾é€šã™æ’ç­‰ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "                self.patch_mixer_map_xin = nn.Identity()\n",
    "                self.patch_mixer_map_xout = nn.Identity()\n",
    "                self.patch_mixer_map_y = nn.Identity()\n",
    "\n",
    "        # 4) ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®æ§‹ç¯‰\n",
    "\n",
    "        assert len(ffn_multipliers) == len(qkv_multipliers)\n",
    "\n",
    "        # FFNã®ä¹—æ•°ãŒdepthã¨åŒã˜å ´åˆ\n",
    "        if len(ffn_multipliers) == depth:\n",
    "            # QKVã¨FFNã®ä¹—æ•°ã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
    "            qkv_ratios = qkv_multipliers\n",
    "            mlp_ratios = ffn_multipliers\n",
    "\n",
    "        # ãã‚Œä»¥å¤–ã®å ´åˆã¯ã€ä¹—æ•°ã‚’å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«åˆ†é…\n",
    "        else:\n",
    "            num_splits = len(ffn_multipliers)\n",
    "\n",
    "            assert depth % num_splits == 0, 'number of blocks should be divisible by number of splits'\n",
    "\n",
    "            depth_per_split = depth // num_splits\n",
    "\n",
    "            qkv_ratios = list(np.array([\n",
    "                [m]*depth_per_split for m in qkv_multipliers\n",
    "            ]).reshape(-1))\n",
    "\n",
    "            mlp_ratios = list(np.array([\n",
    "                [m]*depth_per_split for m in ffn_multipliers\n",
    "            ]).reshape(-1))\n",
    "\n",
    "        # MoEãƒ–ãƒ­ãƒƒã‚¯ã‚’é…ç½®ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "        # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯MoEã‚’ä½¿ç”¨ã—ãªã„\n",
    "        expert_blocks_idx = [\n",
    "            i for i in range(0, depth - 1) \n",
    "            if (i+1) % experts_every_n == 0\n",
    "        ]\n",
    "\n",
    "        # MoEãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ•ãƒ©ã‚°ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "        is_moe_block = [\n",
    "            True if i in expert_blocks_idx else False \n",
    "            for i in range(depth)\n",
    "        ]\n",
    "        \n",
    "        # DitBlockã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DiTBlock(\n",
    "                dim=dim,\n",
    "                head_dim=head_dim,\n",
    "                mlp_ratio=mlp_ratios[i],\n",
    "                qkv_ratio=qkv_ratios[i],\n",
    "                multiple_of=multiple_of,\n",
    "                pooled_emb_dim=dim,\n",
    "                norm_eps=norm_eps,\n",
    "                depth_init=depth_init,\n",
    "                layer_id=i,\n",
    "                num_layers=depth,\n",
    "                compress_xattn=False,\n",
    "                use_bias=use_bias,\n",
    "                moe_block=is_moe_block[i],\n",
    "                num_experts=num_experts,\n",
    "                expert_capacity=expert_capacity\n",
    "            ) for i in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # 5) ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã¨å‡ºåŠ›å±¤\n",
    "\n",
    "        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸéƒ¨åˆ†ã‚’ä¿ç®¡ã™ã‚‹ãŸã‚ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§ç™»éŒ²\n",
    "        self.register_buffer(\n",
    "            \"mask_token\", \n",
    "            torch.zeros(1, 1, patch_size ** 2 * self.out_channels)\n",
    "        )\n",
    "\n",
    "        # æœ€çµ‚å‡ºåŠ›å±¤ï¼ˆTransformerç‰¹å¾´é‡ -> ç”»åƒãƒ‘ãƒƒãƒï¼‰\n",
    "        self.final_layer = T2IFinalLayer(\n",
    "            dim,\n",
    "            dim,\n",
    "            patch_size,\n",
    "            self.out_channels,\n",
    "            approx_gelu,\n",
    "            create_norm('layernorm', dim, eps=norm_eps)\n",
    "        )\n",
    "\n",
    "        # 6) é‡ã¿ã®åˆæœŸåŒ–\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward_without_cfg(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        mask_ratio: float = 0,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Classifier-Free Guidanceï¼ˆCFGï¼‰ã‚’ä½¿ç”¨ã—ãªã„å ´åˆã®æœ¬æ¥ã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« (batch_size, channels, height, width)\n",
    "            t: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ãƒ³ã‚½ãƒ« (batch_size,)\n",
    "            y: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« (batch_size, 1, seq_len, dim)\n",
    "            mask_ratio: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒã‚¹ã‚¯ã™ã‚‹ãƒ‘ãƒƒãƒã®å‰²åˆï¼ˆ0ã‹ã‚‰1ã®é–“ï¼‰\n",
    "\n",
    "        Returns:\n",
    "            dict: ä»¥ä¸‹ã‚’å«ã‚€è¾æ›¸:\n",
    "                - 'sample': å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« (batch_size, out_channels, height, width)\n",
    "                - 'mask': ãƒã‚¹ã‚­ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸå ´åˆã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«ã€ãã†ã§ãªã‘ã‚Œã°None\n",
    "        \"\"\"\n",
    "        logger.info(f\"DiTã®é †ä¼æ’­ï¼ˆCFGãªã—ï¼‰ {x.shape=} {t.shape=} {y.shape=} {mask_ratio=}\")\n",
    "\n",
    "        # 1) åŸ‹ã‚è¾¼ã¿ã¨å‰å‡¦ç†\n",
    "\n",
    "        self.h = x.shape[-2] // self.patch_size\n",
    "        logger.debug(f\"{self.h=}\")\n",
    "\n",
    "        self.w = x.shape[-1] // self.patch_size\n",
    "        logger.debug(f\"{self.w=}\")\n",
    "\n",
    "        # timm.PatchEmbedã§ç”»åƒã‚’ãƒ‘ãƒƒãƒã«åˆ†å‰²ã—åŸ‹ã‚è¾¼ã¿ã€ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’åŠ ç®—\n",
    "        # æ›´ã«ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’åŠ ç®—\n",
    "        # (num_batch, num_patches, dim)\n",
    "        # num_patches = (H / patch_size) * (W / patch_size)\n",
    "        x = self.x_embedder(x) + self.pos_embed\n",
    "        logger.debug(f\"ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")\n",
    "\n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿\n",
    "        # ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’å…¨ãƒãƒƒãƒã«æ‹¡å¼µã—ã€TimestepEmbedderã‚’é©ç”¨\n",
    "        t = self.t_embedder(t.expand(x.shape[0]))  # (N, D)\n",
    "        logger.debug(f\"ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {t.shape=}\")\n",
    "\n",
    "        # CaptionProjectionã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿\n",
    "        # (N, 1, L, D)\n",
    "        y = self.y_embedder(y)\n",
    "        logger.debug(f\"ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {y.shape=}\")\n",
    "\n",
    "        # AttentionBlockPromptEmbeddingã§ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’å‰å‡¦ç†\n",
    "        # (N, 1, L, D) -> (N, D)\n",
    "        y = self.y_emb_preprocess(y.squeeze(dim=1)).unsqueeze(dim=1)\n",
    "        logger.debug(f\"å‰å‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« {y.shape=}\")  \n",
    "\n",
    "        # ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«åŠ ç®—\n",
    "        y_pooled = self.pooled_y_emb_process(y.mean(dim=-2).squeeze(dim=1))\n",
    "        t = t + y_pooled\n",
    "        logger.debug(f\"ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’åŠ ç®—å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ãƒ³ã‚½ãƒ« {t.shape=}\")\n",
    "\n",
    "        mask = None\n",
    "\n",
    "        # 2) Patch-Mixerã®é©ç”¨\n",
    "\n",
    "        if self.use_patch_mixer:\n",
    "\n",
    "            # å…¥åŠ›ã‚’Patch-Mixerã®æ¬¡å…ƒã«æŠ•å½±\n",
    "            x = self.patch_mixer_map_xin(x)\n",
    "            logger.debug(f\"Patch-Mixerå…¥åŠ›ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")\n",
    "\n",
    "            # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’Patch-Mixerã®æ¬¡å…ƒã«æŠ•å½±\n",
    "            y_mixer = self.patch_mixer_map_y(y)\n",
    "            logger.debug(f\"Patch-Mixerãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {y_mixer.shape=}\")\n",
    "\n",
    "            # Patch-Mixerãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«é©ç”¨\n",
    "            for block in self.patch_mixer:\n",
    "                x = block(x, y_mixer, t)  # (N, T, D_mixer)\n",
    "            logger.debug(f\"Patch-Mixeré©ç”¨å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")\n",
    "\n",
    "        # 3) ãƒã‚¹ã‚­ãƒ³ã‚°\n",
    "        \n",
    "        if mask_ratio > 0:\n",
    "\n",
    "            # ãƒ‘ãƒƒãƒãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ\n",
    "            mask_dict = get_mask(\n",
    "                x.shape[0], x.shape[1], \n",
    "                mask_ratio=mask_ratio, \n",
    "                device=x.device\n",
    "            )\n",
    "            ids_keep = mask_dict['ids_keep']\n",
    "            ids_restore = mask_dict['ids_restore']\n",
    "            mask = mask_dict['mask']\n",
    "\n",
    "            # ãƒã‚¹ã‚¯ã‚¢ã‚¦ãƒˆã‚’é©ç”¨\n",
    "            x = mask_out_token(x, ids_keep)\n",
    "            logger.debug(f\"ãƒã‚¹ã‚¯ã‚¢ã‚¦ãƒˆå¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")    \n",
    "        \n",
    "        if self.use_patch_mixer:\n",
    "            # Patch-Mixerã®å‡ºåŠ›ã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼æ¬¡å…ƒã«æŠ•å½±\n",
    "            # ï¼ˆè¨ˆç®—ã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã«ãƒã‚¹ã‚­ãƒ³ã‚°å¾Œã«å®Ÿè¡Œï¼‰\n",
    "            x = self.patch_mixer_map_xout(x)\n",
    "\n",
    "        # 4) ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®é©ç”¨\n",
    "\n",
    "        # DiTãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«é©ç”¨\n",
    "        for block in self.blocks:\n",
    "            x = block(x, y, t)  # (N, T, D)\n",
    "\n",
    "        # 5) ã‚¢ãƒ³ãƒã‚¹ã‚­ãƒ³ã‚°ã¨ç”»åƒå†æ§‹æˆ\n",
    "\n",
    "        # æœ€çµ‚å‡ºåŠ›å±¤ã‚’é©ç”¨\n",
    "        # (N, T, patch_size ** 2 * out_channels)\n",
    "        x = self.final_layer(x, t)\n",
    "        logger.debug(f\"æœ€çµ‚å‡ºåŠ›å±¤é©ç”¨å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")\n",
    "        \n",
    "        if mask_ratio > 0:\n",
    "            # ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§è£œå®Œã—ã€å…ƒã®é †åºã«å¾©å…ƒ\n",
    "            x = unmask_tokens(x, ids_restore, self.mask_token)\n",
    "\n",
    "        # ãƒ‘ãƒƒãƒã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«å†æ§‹æˆ\n",
    "        # (N, out_channels, H, W)\n",
    "        x = self.unpatchify(x)\n",
    "        logger.debug(f\"ãƒ‘ãƒƒãƒå†æ§‹æˆå¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")\n",
    "\n",
    "        return {'sample': x, 'mask': mask}\n",
    "    \n",
    "    def forward_with_cfg(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        cfg: float = 1.0,\n",
    "        mask_ratio: float = 0,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Clissifier-Free Guidanceï¼ˆCFGï¼‰ã‚’ä½¿ç”¨ã—ãŸé †ä¼æ’­\n",
    "        æ¡ä»¶ä»˜ãç”Ÿæˆã¨ã€ç„¡æ¡ä»¶ã§ã®ç”Ÿæˆã‚’åŒæ™‚ã«è¡Œã„ã€çµæœã‚’çµ„ã¿åˆã‚ã›ã‚‹\n",
    "\n",
    "        Args:\n",
    "            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« (batch_size, channels, height, width)\n",
    "            t: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ãƒ³ã‚½ãƒ« (batch_size,)\n",
    "            y: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« (batch_size, 1\n",
    "            cfg: Classifier-free guidanceã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ1.0ã¯ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãªã—ï¼‰\n",
    "            mask_ratio: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒã‚¹ã‚¯ã™ã‚‹ãƒ‘ãƒƒãƒã®å‰²åˆï¼ˆ0ã‹ã‚‰1ã®é–“ï¼‰\n",
    "        Returns:\n",
    "            dict: ä»¥ä¸‹ã‚’å«ã‚€è¾æ›¸:\n",
    "                - 'sample': å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« (batch_size, out_channels, height, width)\n",
    "        \"\"\"\n",
    "        logger.info(f\"DiTã®é †ä¼æ’­ï¼ˆCFGã‚ã‚Šï¼‰ {x.shape=} {t.shape=} {y.shape=} {cfg=} {mask_ratio=}\")\n",
    "\n",
    "        # 1) ãƒãƒƒãƒã®æ‹¡å¼µ\n",
    "\n",
    "        x = torch.cat([x, x], 0)\n",
    "        logger.debug(f\"CFGç”¨ã«ãƒãƒƒãƒã‚’2å€ã«æ‹¡å¼µ {x.shape=}\")\n",
    "\n",
    "        y = torch.cat([y, torch.zeros_like(y)], 0)\n",
    "        logger.debug(f\"CFGç”¨ã«ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã‚’æ‹¡å¼µ {y.shape=}\")\n",
    "\n",
    "        if len(t) != 1:\n",
    "            t = torch.cat([t, t], 0)\n",
    "            logger.debug(f\"CFGç”¨ã«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ‹¡å¼µ {t.shape=}\")\n",
    "        \n",
    "        # 2) 2å€ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã§é †ä¼æ’­ã‚’å®Ÿè¡Œ\n",
    "\n",
    "        eps = self.forward_without_cfg(x, t, y, mask_ratio, **kwargs)['sample']\n",
    "\n",
    "        # 3) çµæœã®åˆ†å‰²ã¨ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã®é©ç”¨\n",
    "\n",
    "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
    "        logger.debug(f\"æ¡ä»¶ä»˜ãã¨ç„¡æ¡ä»¶ã®å‡ºåŠ›ã«åˆ†å‰² {cond_eps.shape=} {uncond_eps.shape=}\")\n",
    "\n",
    "        # CFGã®å¼ã‚’é©ç”¨\n",
    "        # ç„¡æ¡ä»¶ã®å‡ºåŠ› + ã‚¹ã‚±ãƒ¼ãƒ« * (æ¡ä»¶ä»˜ãã®å‡ºåŠ› - ç„¡æ¡ä»¶ã®å‡ºåŠ›)\n",
    "        eps = uncond_eps + cfg * (cond_eps - uncond_eps)\n",
    "        logger.debug(f\"CFGã‚’é©ç”¨ã—ãŸæœ€çµ‚å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« {eps.shape=}\")\n",
    "\n",
    "        return {'sample': eps}\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        cfg: float = 1.0,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        CFGã®å€¤ã«åŸºã¥ã„ã¦é©åˆ‡ãªé †ä¼æ’­ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« (batch_size, channels, height, width)\n",
    "            t: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ãƒ³ã‚½ãƒ« (batch_size,)\n",
    "            y: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« (batch_size, 1, seq_len, dim)\n",
    "            cfg: Classifier-free guidanceã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ1.0ã¯ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãªã—ï¼‰\n",
    "        Returns:\n",
    "            dict: ä»¥ä¸‹ã‚’å«ã‚€è¾æ›¸:\n",
    "                - 'sample': å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« (batch_size, out_channels, height, width)\n",
    "        \"\"\"\n",
    "\n",
    "        # CFGã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if cfg != 1.0:\n",
    "            return self.forward_with_cfg(x, t, y, cfg, **kwargs)\n",
    "\n",
    "        # ä½¿ç”¨ã—ãªã„å ´åˆ\n",
    "        else:\n",
    "            return self.forward_without_cfg(x, t, y, **kwargs)\n",
    "\n",
    "    def unpatchify(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å…ƒã®ç”»åƒã®å½¢çŠ¶ã«å¾©å…ƒã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): ãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ (N, num_patches, patch_size**2 * out_channels)\n",
    "        Returns:\n",
    "            torch.Tensor: å¾©å…ƒã•ã‚ŒãŸç”»åƒãƒ†ãƒ³ã‚½ãƒ« (N, out_channels, H, W)\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ‘ãƒƒãƒã‚’å…ƒã®ç”»åƒã«å¾©å…ƒ {x.shape=}\")\n",
    "\n",
    "        # 1) æ¬¡å…ƒã®è¨ˆç®—ã¨æ¤œè¨¼\n",
    "\n",
    "        c = self.out_channels\n",
    "        logger.debug(f\"{c=}\")\n",
    "\n",
    "        p = self.x_embedder.patch_size[0]\n",
    "        logger.debug(f\"{p=}\")\n",
    "\n",
    "        h = w = int(x.shape[1]**0.5)\n",
    "        logger.debug(f\"{h=} {w=}\")\n",
    "\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        # 2) ãƒ•ãƒ©ãƒƒãƒˆãªãƒ‘ãƒƒãƒã‚’ç”»åƒã«å¾©å…ƒ\n",
    "\n",
    "        # (num_batch, num_patches, patch_size**2 * out_channels)\n",
    "        # -> (num_batch, h, w, patch_size, patch_size, out_channels)\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "\n",
    "        # 3) æ¬¡å…ƒã®ä¸¦ã³æ›¿ãˆ\n",
    "\n",
    "        # (num_batch, out_channels, h*patch_size, w*patch_size)\n",
    "        # -> (num_batch, out_channels, h, patch_size, w, patch_size)\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        logger.debug(f\"ãƒ‘ãƒƒãƒã‚’æ­£ã—ã„ä½ç½®ã«é…ç½®å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« {x.shape=}\")\n",
    "\n",
    "        # 4) ç”»åƒã¸ã®çµåˆ\n",
    "\n",
    "        # (num_batch, out_channels, h, patch_size, w, patch_size)\n",
    "        # -> (num_batch, out_channels, h*patch_size, h*patch_size)\n",
    "        result = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
    "        logger.debug(f\"å¾©å…ƒã•ã‚ŒãŸç”»åƒãƒ†ãƒ³ã‚½ãƒ« {result.shape=}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(\"é‡ã¿ã®åˆæœŸåŒ–ã‚’å®Ÿè¡Œ\")\n",
    "\n",
    "        # 1) åŸºæœ¬çš„ãªåˆæœŸåŒ–ã‚’é©ç”¨\n",
    "\n",
    "        def zero_bias(m: nn.Module) -> None:\n",
    "            \"\"\"\n",
    "            ãƒã‚¤ã‚¢ã‚¹ã‚’ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "            \"\"\"\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        def _basic_init(module: nn.Module) -> None:\n",
    "            \"\"\"\n",
    "            Xavierï¼ˆã‚·ãƒ“ã‚¢ãƒ¼ï¼‰åˆæœŸåŒ–ã¨ãƒã‚¤ã‚¢ã‚¹ã®ã‚¼ãƒ­åˆæœŸåŒ–ã‚’é©ç”¨\n",
    "            \"\"\"\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                zero_bias(module)  # All bias in the model init to zero\n",
    "\n",
    "        # Baseline init of all parameters\n",
    "        self.apply(_basic_init)\n",
    "\n",
    "        # 2) ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’åˆæœŸåŒ–\n",
    "\n",
    "        # 2æ¬¡å…ƒæ­£å¼¦æ³¢ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—\n",
    "        pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.pos_embed.shape[-1],\n",
    "            int(self.x_embedder.num_patches**0.5),\n",
    "            pos_interp_scale=self.pos_interp_scale,\n",
    "            base_size=self.base_size\n",
    "        )\n",
    "\n",
    "        # å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿ã«ä»£å…¥\n",
    "        self.pos_embed.data.copy_(\n",
    "            torch.from_numpy(pos_embed).float().unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        # 3) ç‰¹å®šã®å±¤ã®æ­£è¦åˆ†å¸ƒåˆæœŸåŒ–\n",
    "\n",
    "        # XavieråˆæœŸåŒ–\n",
    "        w = self.x_embedder.proj.weight.data\n",
    "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # æ¨™æº–åå·®0.02ã®æ­£è¦åˆ†å¸ƒã§åˆæœŸåŒ–\n",
    "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
    "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
    "        nn.init.normal_(self.pooled_y_emb_process.fc1.weight, std=0.02)\n",
    "        nn.init.normal_(self.pooled_y_emb_process.fc2.weight, std=0.02)\n",
    "        nn.init.normal_(self.y_embedder.y_proj.fc1.weight, std=0.02)\n",
    "        nn.init.normal_(self.y_embedder.y_proj.fc2.weight, std=0.02)\n",
    "        \n",
    "        # 4) ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®ã‚«ã‚¹ã‚¿ãƒ åˆæœŸåŒ–\n",
    "\n",
    "        for block in self.blocks:\n",
    "            block.custom_init()\n",
    "\n",
    "        for block in self.patch_mixer:\n",
    "            block.custom_init()\n",
    "\n",
    "        # AdaLNã®å¤‰èª¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "        for block in self.blocks:\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
    "\n",
    "        # AdaLNã®å¤‰èª¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "        for block in self.patch_mixer:\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
    "\n",
    "        self.y_emb_preprocess.custom_init()\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†å±¤ã®ç‰¹å®šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "        nn.init.constant_(self.y_emb_preprocess.attn.proj.weight, 0)\n",
    "        nn.init.constant_(self.y_emb_preprocess.mlp.w3.weight, 0)\n",
    "\n",
    "        # æœ€çµ‚å‡ºåŠ›å±¤ã‚’ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.weight, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MicroDiT_Tiny_2(\n",
    "    caption_channels: int = 1024,\n",
    "    qkv_ratio: List[float] = [0.5, 1.0],\n",
    "    mlp_ratio: List[float] = [0.5, 4.0],\n",
    "    pos_interp_scale: float = 1.0,\n",
    "    input_size: int = 32,\n",
    "    num_experts: int = 8,\n",
    "    expert_capacity: float = 2.0,\n",
    "    experts_every_n: int = 2,\n",
    "    in_channels: int = 4,\n",
    "    **kwargs\n",
    ") -> DiT:\n",
    "    depth = 16\n",
    "    model = DiT(\n",
    "        input_size=input_size,\n",
    "        patch_size=2,\n",
    "        in_channels=in_channels,\n",
    "        dim=512,\n",
    "        depth=depth,\n",
    "        head_dim=32,\n",
    "        multiple_of=256,\n",
    "        caption_channels=caption_channels,\n",
    "        pos_interp_scale=pos_interp_scale,\n",
    "        norm_eps=1e-6,\n",
    "        depth_init=True,\n",
    "        qkv_multipliers=np.linspace(qkv_ratio[0], qkv_ratio[1], num=depth, dtype=float),\n",
    "        ffn_multipliers=np.linspace(mlp_ratio[0], mlp_ratio[1], num=depth, dtype=float),\n",
    "        use_patch_mixer=True,\n",
    "        patch_mixer_depth=4,\n",
    "        patch_mixer_dim=512,  # allocating higher budget to mixer layers\n",
    "        patch_mixer_qkv_ratio=1.0,\n",
    "        patch_mixer_mlp_ratio=4.0,\n",
    "        use_bias=False,\n",
    "        num_experts=num_experts,\n",
    "        expert_capacity=expert_capacity,\n",
    "        experts_every_n=experts_every_n,\n",
    "        **kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MicroDiT_XL_2(\n",
    "    caption_channels: int = 1024,\n",
    "    qkv_ratio: List[float] = [0.5, 1.0],\n",
    "    mlp_ratio: List[float] = [0.5, 4.0],\n",
    "    pos_interp_scale: float = 1.0,\n",
    "    input_size: int = 32,\n",
    "    num_experts: int = 8,\n",
    "    expert_capacity: float = 2.0,\n",
    "    experts_every_n: int = 2,\n",
    "    in_channels: int = 4,\n",
    "    **kwargs\n",
    ") -> DiT:\n",
    "    depth = 28\n",
    "    model = DiT(\n",
    "        input_size=input_size,\n",
    "        patch_size=2,\n",
    "        in_channels=in_channels,\n",
    "        dim=1024,\n",
    "        depth=depth,\n",
    "        head_dim=64,\n",
    "        multiple_of=256,\n",
    "        caption_channels=caption_channels,\n",
    "        pos_interp_scale=pos_interp_scale,\n",
    "        norm_eps=1e-6,\n",
    "        depth_init=True,\n",
    "        qkv_multipliers=np.linspace(qkv_ratio[0], qkv_ratio[1], num=depth, dtype=float),\n",
    "        ffn_multipliers=np.linspace(mlp_ratio[0], mlp_ratio[1], num=depth, dtype=float),\n",
    "        use_patch_mixer=True,\n",
    "        patch_mixer_depth=6,\n",
    "        patch_mixer_dim=768,  # allocating higher budget to mixer layers\n",
    "        patch_mixer_qkv_ratio=1.0,\n",
    "        patch_mixer_mlp_ratio=4.0,\n",
    "        use_bias=False,\n",
    "        num_experts=num_experts,\n",
    "        expert_capacity=expert_capacity,\n",
    "        experts_every_n=experts_every_n,\n",
    "        **kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534db8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_dit_tiny_2 = MicroDiT_Tiny_2()\n",
    "sample_input = torch.randn(1, 4, 32, 32)\n",
    "sample_timestep = torch.tensor([10])\n",
    "sample_caption = torch.randn(1, 1, 16, 1024)\n",
    "output = micro_dit_tiny_2(sample_input, sample_timestep, sample_caption, cfg=1.5, mask_ratio=0.3)\n",
    "logger.info(f\"MicroDiT_Tiny_2ã®å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« {output['sample'].shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3034baa6",
   "metadata": {},
   "source": [
    "## LatentDiffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610757c9",
   "metadata": {},
   "source": [
    "### UniversalTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ae948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_2_hf_tokenizer_wrapper:\n",
    "    \"\"\"\n",
    "    OpenCLIPãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    Hugging Faceã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«æ²¿ã‚ã›ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (Any): OpenCLIP tokenizer instance\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: Any):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokenizer (Any): OpenCLIPã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "        \"\"\"\n",
    "        logger.info(f\"OpenCLIPãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–\")\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Hugging Faceã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæŒã¤å±æ€§ã‚’ãƒ¢ãƒƒã‚¯\n",
    "        self.model_max_length = self.tokenizer.context_length\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        caption: str,\n",
    "        padding: str = 'max_length',\n",
    "        max_length: Optional[int] = None,\n",
    "        truncation: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        logger.info(f\"OpenCLIPã§ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–: {caption}\")\n",
    "\n",
    "        input_ids = self.tokenizer(caption, context_length=max_length)\n",
    "        logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–çµæœ: {input_ids}\")\n",
    "\n",
    "        return { 'input_ids': input_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce76788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder_embedding_format(enc: str) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®åå‰ã‚’å—ã‘å–ã‚Šã€åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®å½¢çŠ¶ã‚’è¿”ã™\n",
    "    ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ã¥ã‘ã§ã€å…¥åŠ›ã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®ã‚µã‚¤ã‚ºã‚’äº‹å‰ã«çŸ¥ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚\n",
    "\n",
    "    Args:\n",
    "        enc (str): ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®åå‰\n",
    "    Returns:\n",
    "        Tuple[int, int]:\n",
    "            - ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§é•·\n",
    "            - ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°\n",
    "    \"\"\"\n",
    "\n",
    "    if enc in [\n",
    "        'stabilityai/stable-diffusion-2-base',\n",
    "        'runwayml/stable-diffusion-v1-5',\n",
    "        'CompVis/stable-diffusion-v1-4'\n",
    "    ]:\n",
    "        return 77, 1024\n",
    "\n",
    "    # OpenCLIPã®å ´åˆ\n",
    "    if enc in ['openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378']:\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§é•·ã¯77ã€åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã¯1024\n",
    "        return 77, 1024\n",
    "\n",
    "    if enc in [\"DeepFloyd/t5-v1_1-xxl\"]:\n",
    "        return 120, 4096\n",
    "\n",
    "    raise ValueError(f'Please specifcy the sequence and embedding size of {enc} encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalTokenizer:\n",
    "    \"\"\"\n",
    "    OpenCLIPã‚„T5ãªã©ã®ç•°ãªã‚‹ç¨®é¡ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«å¯¾å¿œã™ã‚‹æ±ç”¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name (str): ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åå‰ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­\n",
    "                - OpenCLIP\n",
    "                - T5\n",
    "                - CLIP\n",
    "        \"\"\"\n",
    "        logger.info(f\"UniversalTokenizerã‚’åˆæœŸåŒ–: {name=}\")\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        s, d = text_encoder_embedding_format(name)\n",
    "\n",
    "        # OpenCLIPã®å ´åˆ\n",
    "        # True\n",
    "        if self.name.startswith(\"openclip:\"):\n",
    "\n",
    "            # OpenCLIPãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–\n",
    "            self.tokenizer = simple_2_hf_tokenizer_wrapper(\n",
    "                open_clip.get_tokenizer(name.lstrip('openclip:'))\n",
    "            )\n",
    "            assert s == self.tokenizer.model_max_length, \"simply check of text_encoder_embedding_format\"\n",
    "\n",
    "        # T5ã®å ´åˆ\n",
    "        elif self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(name) # for t5 we would use a smaller than max_seq_length\n",
    "\n",
    "        # CLIPã®å ´åˆ\n",
    "        else:\n",
    "            self.tokenizer = CLIPTokenizer.from_pretrained(name, subfolder='tokenizer')\n",
    "            assert s == self.tokenizer.model_max_length, \"simply check of text_encoder_embedding_format\"\n",
    "\n",
    "        self.model_max_length = s\n",
    "        \n",
    "    def tokenize(self, captions: Union[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
    "        logger.info(f\"ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–: {captions=}\")\n",
    "\n",
    "        # T5ã®å ´åˆ\n",
    "        # False\n",
    "        if self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            text_tokens_and_mask = self.tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                max_length=self.model_max_length,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç„¡è¦–ã™ã‚‹ãŸã‚ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ãŒå¿…è¦\n",
    "            return {\n",
    "                'input_ids': text_tokens_and_mask['input_ids'],\n",
    "                'attention_mask': text_tokens_and_mask['attention_mask']\n",
    "            }\n",
    "\n",
    "        # OpenCLIPã¾ãŸã¯CLIPã®å ´åˆ\n",
    "        # True\n",
    "        else:\n",
    "            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’æ˜ç¤ºçš„ã«ä½¿ç”¨ã—ãªã„\n",
    "            tokenized_caption = self.tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )['input_ids']\n",
    "            return {'input_ids': tokenized_caption}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b884a",
   "metadata": {},
   "source": [
    "### UniversalTextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class openclip_text_encoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    OpenCLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    å…¥å‡ºåŠ›å½¢å¼ã‚’ä½¿ã„ã‚„ã™ãã™ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model: Any, dtype: torch.dtype = torch.float32, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip_model (Any): OpenCLIPãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "            dtype (torch.dtype, torch.float32): ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®ãƒ‡ãƒ¼ã‚¿å‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"OpenCLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–: {dtype=}\")\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.device = None\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward_fxn(self, text: torch.Tensor) -> Tuple[torch.Tensor, None]:\n",
    "        \"\"\"\n",
    "        ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®é †ä¼æ¬\n",
    "\n",
    "        Args:\n",
    "            text (torch.Tensor): ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, None]: ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«ã¨None\n",
    "        \"\"\"\n",
    "        logger.info(f\"OpenCLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®é †ä¼æ’­ï¼ˆå†…éƒ¨ï¼‰ {text.shape=}\")\n",
    "\n",
    "        # 1) ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿\n",
    "\n",
    "        # ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—\n",
    "        cast_dtype = self.clip_model.transformer.get_cast_dtype()\n",
    "        logger.debug(f\"ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—: {cast_dtype=}\")\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã—ã€ãƒ‡ãƒ¼ã‚¿å‹ã‚’ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        # (batch_size, n_ctx, d_model)\n",
    "        x = self.clip_model.token_embedding(text).to(cast_dtype)\n",
    "        logger.debug(f\"ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›: {x.shape=}\")\n",
    "\n",
    "        # 2) ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è¿½åŠ \n",
    "\n",
    "        x = x + self.clip_model.positional_embedding.to(cast_dtype)\n",
    "\n",
    "\n",
    "        # 3) Transformerã‚’é©ç”¨\n",
    "\n",
    "        # (batch_size, n_ctx, d_model) -> (n_ctx, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        logger.debug(f\"æ¬¡å…ƒã‚’å…¥ã‚Œæ›¿ãˆ {x.shape=}\")\n",
    "\n",
    "        x = self.clip_model.transformer(x)\n",
    "\n",
    "        # (n_ctx, batch_size, d_model) -> (batch_size, n_ctx, d_model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        logger.debug(f\"æ¬¡å…ƒã‚’å…ƒã«æˆ»ã™ {x.shape=}\")\n",
    "\n",
    "        # 4) ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’é©ç”¨\n",
    "\n",
    "        x = self.clip_model.ln_final(x)\n",
    "\n",
    "        # 5) å‡ºåŠ›ã‚’æ•´å½¢ï¼ˆDiTã®å…¥åŠ›ã‚„Hugging Faceã®ä»•æ§˜ã«åˆã‚ã›ã‚‹ï¼‰\n",
    "\n",
    "        # (batch_size, n_ctx, d_model) -> (batch_size, 1, n_ctx, d_model)\n",
    "        x = x.unsqueeze(dim=1)\n",
    "\n",
    "        return x, None\n",
    "\n",
    "    def forward(self, text: torch.Tensor, **kwargs) -> Tuple[torch.Tensor, None]:\n",
    "        logger.info(f\"OpenCLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®é †ä¼æ’­ {text.shape=}\")\n",
    "\n",
    "        # CUDAã§æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‹ã‚’ä½¿ã£ã¦å†…éƒ¨ã®é †ä¼æ’­ã‚’å®Ÿè¡Œ\n",
    "        with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "            return self.forward_fxn(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fa1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalTextEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    ç•°ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«å¯¾å¿œã™ã‚‹æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    \n",
    "    Args:\n",
    "        name (str): Name/path of the model to load\n",
    "        dtype (str): Data type for model weights\n",
    "        pretrained (bool, True): Whether to load pretrained weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, dtype: str, pretrained: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name (str): ãƒ¢ãƒ‡ãƒ«ã®åå‰ã¾ãŸã¯ãƒ‘ã‚¹\n",
    "            dtype (str): ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®ãƒ‡ãƒ¼ã‚¿å‹\n",
    "            pretrained (bool, True): äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"UniversalTextEncoderã‚’åˆæœŸåŒ–: {name=}, {dtype=}, {pretrained=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        # OpenCLIPã®å ´åˆ\n",
    "        # True\n",
    "        if self.name.startswith(\"openclip:\"):\n",
    "            assert pretrained, 'Load default pretrained model from openclip'\n",
    "\n",
    "            # OpenCLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–\n",
    "            self.encoder = openclip_text_encoder(\n",
    "                open_clip.create_model_and_transforms(name.lstrip('openclip:'))[0],\n",
    "                torch_dtype=DATA_TYPES[dtype]\n",
    "            )\n",
    "\n",
    "        # False\n",
    "        elif self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            self.encoder = T5EncoderModel.from_pretrained(\n",
    "                name,\n",
    "                torch_dtype=DATA_TYPES[dtype],\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "\n",
    "        # False\n",
    "        else:\n",
    "            self.encoder = CLIPTextModel.from_pretrained(\n",
    "                name,\n",
    "                subfolder='text_encoder',\n",
    "                torch_dtype=DATA_TYPES[dtype],\n",
    "                pretrained=pretrained\n",
    "            )\n",
    "\n",
    "    def encode(self, tokenized_caption: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—\n",
    "        Args:\n",
    "            tokenized_caption (torch.Tensor):\n",
    "                ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            attention_mask (Optional[torch.Tensor], None):\n",
    "                ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆT5ç”¨ï¼‰\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "                åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«ã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯\n",
    "        \"\"\"\n",
    "        logger.info(f\"UniversalTextEncoderã§ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰: {tokenized_caption.shape=}, {attention_mask.shape if attention_mask is not None else None=}\")\n",
    "\n",
    "        # T5ã®å ´åˆ\n",
    "        # False\n",
    "        if self.name == \"DeepFloyd/t5-v1_1-xxl\":\n",
    "            out = self.encoder(\n",
    "                tokenized_caption,\n",
    "                attention_mask=attention_mask\n",
    "            )['last_hidden_state']\n",
    "            out = out.unsqueeze(dim=1)\n",
    "            return out, None\n",
    "\n",
    "        # True\n",
    "        else:\n",
    "            # OpenCLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§é †ä¼æ’­\n",
    "            return self.encoder(tokenized_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbf783",
   "metadata": {},
   "source": [
    "### DistLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistLoss(Metric):\n",
    "    \"\"\"\n",
    "    åˆ†æ•£å­¦ç¿’ç’°å¢ƒã§æå¤±ã®å¹³å‡å€¤ã‚’é›†è¨ˆã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    torchmetrics.Metricã‚’ç¶™æ‰¿\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kwargs (Any): è¦ªã‚¯ãƒ©ã‚¹ã«æ¸¡ã•ã‚Œã‚‹è¿½åŠ ã®å¼•æ•°\n",
    "        \"\"\"\n",
    "        logger.info(\"DistLossãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’åˆæœŸåŒ–\")\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # æå¤±ã‚’é›†è¨ˆã™ã‚‹ãŸã‚ã®çŠ¶æ…‹ã‚’è¿½åŠ \n",
    "        self.add_state(\n",
    "            \"loss\", default=torch.tensor(0.), dist_reduce_fx=\"sum\"\n",
    "        )\n",
    "\n",
    "        # ãƒãƒƒãƒæ•°ã‚’é›†è¨ˆã™ã‚‹ãŸã‚ã®çŠ¶æ…‹ã‚’è¿½åŠ \n",
    "        self.add_state(\n",
    "            \"batches\", default=torch.tensor(0), dist_reduce_fx=\"sum\"\n",
    "        )\n",
    "\n",
    "    def update(self, value: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            value (torch.Tensor): ç¾åœ¨ã®ãƒãƒƒãƒã®æå¤±å€¤\n",
    "        \"\"\"\n",
    "        logger.info(f\"DistLossãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’æ›´æ–°: {value=}\")\n",
    "\n",
    "        # æå¤±ã‚’åŠ ç®—\n",
    "        self.loss += value\n",
    "\n",
    "        # ãƒãƒƒãƒæ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ\n",
    "        self.batches += 1\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        é›†è¨ˆã•ã‚ŒãŸæå¤±ã®å¹³å‡å€¤ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: é›†è¨ˆã•ã‚ŒãŸæå¤±ã®å¹³å‡å€¤\n",
    "        \"\"\"\n",
    "        return self.loss.float() / self.batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e94ed",
   "metadata": {},
   "source": [
    "### LatentDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e95056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusion(ComposerModel):\n",
    "    \"\"\"\n",
    "    ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹Latent Diffusionãƒ¢ãƒ‡ãƒ«ï¼ˆLDMï¼‰\n",
    "\n",
    "    - MosaicMLã®ComposerModelã‚’ç¶™æ‰¿ã—ã€Composerã¨ã„ã†å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨çµ±åˆ\n",
    "    - 3ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ\n",
    "        - DiT: ãƒã‚¤ã‚ºé™¤å»ã‚’è¡Œã†ãƒ¡ã‚¤ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’å¯¾è±¡ï¼‰\n",
    "        - VAEï¼ˆAutoencoderKLï¼‰: ç”»åƒã‚’æ½œåœ¨ç©ºé–“ã«åœ§ç¸®ãƒ»å¾©å…ƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆå‡çµï¼‰\n",
    "        - ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆUniversalTextEncoderï¼‰:\n",
    "            ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆå‡çµï¼‰\n",
    "    - EDMï¼ˆElucidated Diffusion Modelï¼‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè£…\n",
    "        - EDMã¯ã€ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹é †ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "    - ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ã¯ã€äº‹å‰è¨ˆç®—ã•ã‚ŒãŸã‚‚ã®ã‚’ä½¿ç”¨å¯èƒ½\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dit: nn.Module,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: UniversalTextEncoder,\n",
    "        tokenizer: UniversalTokenizer,\n",
    "        image_key: str = 'image',\n",
    "        text_key: str = 'captions',\n",
    "        image_latents_key: str = 'image_latents',\n",
    "        text_latents_key: str = 'caption_latents',\n",
    "        precomputed_latents: bool = True,\n",
    "        dtype: str = 'bfloat16',\n",
    "        latent_res: int = 32,\n",
    "        p_mean: float = -0.6,\n",
    "        p_std: float = 1.2,\n",
    "        train_mask_ratio: float = 0.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dit (nn.Module): å­¦ç¿’å¯¾è±¡ã®Diffusion Transformerãƒ¢ãƒ‡ãƒ«\n",
    "            vae (AutoencoderKL): ç”»åƒã‚’æ½œåœ¨ç©ºé–“ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰/ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹VAEãƒ¢ãƒ‡ãƒ«ï¼ˆå‡çµï¼‰\n",
    "            text_encoder (UniversalTextEncoder): ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆå‡çµï¼‰\n",
    "            tokenizer (UniversalTokenizer): ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "            image_key (str, optional): ãƒãƒƒãƒè¾æ›¸å†…ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'image'ã€‚\n",
    "            text_key (str, optional): ãƒãƒƒãƒè¾æ›¸å†…ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã€‚ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'captions'ã€‚\n",
    "            image_latents_key (str, optional): ãƒãƒƒãƒè¾æ›¸å†…ã®äº‹å‰è¨ˆç®—ã•ã‚ŒãŸç”»åƒæ½œåœ¨å¤‰æ•°ã®ã‚­ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'image_latents'ã€‚\n",
    "            text_latents_key (str, optional): ãƒãƒƒãƒè¾æ›¸å†…ã®äº‹å‰è¨ˆç®—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆæ½œåœ¨å¤‰æ•°ã®ã‚­ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'caption_latents'ã€‚\n",
    "            precomputed_latents (bool, optional): äº‹å‰è¨ˆç®—ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Trueã€‚\n",
    "            dtype (str, optional): è¨ˆç®—ç²¾åº¦ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'bfloat16'ã€‚\n",
    "            latent_res (int, optional): VAEã«ã‚ˆã‚‹8å€ã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æƒ³å®šã—ãŸæ½œåœ¨ç©ºé–“ã®è§£åƒåº¦ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯32ã€‚\n",
    "            p_mean (float, optional): EDMã®å¯¾æ•°æ­£è¦ãƒã‚¤ã‚ºã®å¹³å‡ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ -0.6ã€‚\n",
    "            p_std (float, optional): EDMã®å¯¾æ•°æ­£è¦ãƒã‚¤ã‚ºã®æ¨™æº–åå·®ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.2ã€‚\n",
    "            train_mask_ratio (float, optional): å­¦ç¿’ä¸­ã«ç”»åƒã‚’ãƒã‚¹ã‚¯ã™ã‚‹å‰²åˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã€‚\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        logger.info(f\"LatentDiffusionãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–: {dtype=}, {latent_res=}, {precomputed_latents=}, {train_mask_ratio=}\")\n",
    "\n",
    "        # 1) å±æ€§ã®è¨­å®š\n",
    "\n",
    "        self.dit = dit\n",
    "        self.vae = vae\n",
    "        self.image_key = image_key\n",
    "        self.text_key = text_key\n",
    "        self.image_latents_key = image_latents_key\n",
    "        self.text_latents_key = text_latents_key\n",
    "        self.precomputed_latents = precomputed_latents\n",
    "        self.dtype = dtype\n",
    "        self.latent_res = latent_res\n",
    "\n",
    "        # EDMãƒ•ãƒ¬ãƒ¼ãƒ ã«åŸºã¥ã„ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
    "        self.edm_config = EasyDict({\n",
    "            'sigma_min': 0.002, # æœ€å°ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«\n",
    "            'sigma_max': 80, # æœ€å¤§ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«\n",
    "            'P_mean': p_mean, # ãƒã‚¤ã‚ºã®å¯¾æ•°æ­£è¦åˆ†å¸ƒã®å¹³å‡\n",
    "            'P_std': p_std, # ãƒã‚¤ã‚ºã®å¯¾æ•°æ­£è¦åˆ†å¸ƒã®æ¨™æº–åå·®\n",
    "            'sigma_data': 0.9, # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åå·®\n",
    "            'num_steps': 18, # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "            'rho': 7, # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®rhoãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            'S_churn': 0, # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®churnãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            'S_min': 0, # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æœ€å°å€¤\n",
    "            'S_max': float('inf'), # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æœ€å¤§å€¤\n",
    "            'S_noise': 1 # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«\n",
    "        })\n",
    "\n",
    "        self.train_mask_ratio = train_mask_ratio\n",
    "\n",
    "        # è©•ä¾¡ãƒ»ç”Ÿæˆæ™‚ã¯ãƒã‚¹ã‚¯ãªã—\n",
    "        self.eval_mask_ratio = 0.\n",
    "\n",
    "        assert self.train_mask_ratio >= 0, 'Masking ratio must be non-negative!'\n",
    "\n",
    "        self.randn_like = torch.randn_like\n",
    "\n",
    "        # VAEã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã‚’å–å¾—\n",
    "        # æ½œåœ¨ç©ºé–“ãŒæ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ãšã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚\n",
    "        self.latent_scale = self.vae.config.scaling_factor\n",
    "\n",
    "        self.text_encoder = text_encoder\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # 2) ãƒ¢ãƒ‡ãƒ«ã®å‡çµ\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å‡çµ\n",
    "        self.text_encoder.requires_grad_(False)\n",
    "\n",
    "        # VAEã‚’å‡çµ\n",
    "        self.vae.requires_grad_(False)\n",
    "\n",
    "        # FSDPï¼ˆFully Sharded Data Parallelï¼‰ã§ãƒ©ãƒƒãƒ—ã—ãªã„ã‚ˆã†ã«è¨­å®š\n",
    "        # FSDPã¯åˆ†æ•£å­¦ç¿’ã®ãŸã‚ã®æŠ€è¡“\n",
    "        self.text_encoder._fsdp_wrap = False\n",
    "        self.vae._fsdp_wrap = False\n",
    "        self.dit._fsdp_wrap = True\n",
    "\n",
    "    def forward(self, batch: dict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚‚ã—ãã¯äº‹å‰è¨ˆç®—ãƒ‡ãƒ¼ã‚¿ã‚’é †ä¼æ¬ã—ã€æå¤±ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            batch (dict): ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - è¨ˆç®—ã•ã‚ŒãŸæå¤±ãƒ†ãƒ³ã‚½ãƒ«\n",
    "                - ç”»åƒæ½œåœ¨å¤‰æ•°ãƒ†ãƒ³ã‚½ãƒ«\n",
    "                - ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) ç”»åƒã®æ½œåœ¨å¤‰æ•°ã®å–å¾—\n",
    "\n",
    "        # äº‹å‰è¨ˆç®—æ¸ˆã¿ã®å ´åˆ\n",
    "        if self.precomputed_latents and self.image_latents_key in batch:\n",
    "\n",
    "            # ãã®ã¾ã¾ä½¿ç”¨\n",
    "            latents = batch[self.image_latents_key]\n",
    "            logger.debug(f\"äº‹å‰è¨ˆç®—ã•ã‚ŒãŸç”»åƒæ½œåœ¨å¤‰æ•°ã‚’ä½¿ç”¨ {latents.shape=}\")\n",
    "\n",
    "        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ãªå ´åˆ\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # ç”»åƒã‚’å–å¾—\n",
    "                images = batch[self.image_key]\n",
    "\n",
    "                # ç”»åƒã‚’æ½œåœ¨ç©ºé–“ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "                latents = self.vae.encode(\n",
    "                    images.to(DATA_TYPES[self.dtype])\n",
    "                )['latent_dist'].sample().data\n",
    "\n",
    "                # VAEã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã‚’ä¹—ã˜ã‚‹\n",
    "                latents *= self.latent_scale\n",
    "\n",
    "            logger.debug(f\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒæ½œåœ¨å¤‰æ•° {latents.shape=}\")\n",
    "\n",
    "        # 2) ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã®å–å¾—\n",
    "\n",
    "        # äº‹å‰è¨ˆç®—æ¸ˆã¿ã®å ´åˆ\n",
    "        if self.precomputed_latents and self.text_latents_key in batch:\n",
    "\n",
    "            # ãã®ã¾ã¾ä½¿ç”¨\n",
    "            conditioning = batch[self.text_latents_key]\n",
    "\n",
    "        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ãªå ´åˆ\n",
    "        else:\n",
    "\n",
    "            # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—\n",
    "            captions = batch[self.text_key]\n",
    "            logger.debug(f\"ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾— {captions=}\")\n",
    "\n",
    "            # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å¹³å¦åŒ–\n",
    "            captions = captions.view(-1, captions.shape[-1])\n",
    "            logger.debug(f\"ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å¹³å¦åŒ– {captions.shape=}\")\n",
    "\n",
    "            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆï¼ˆT5ç”¨ï¼‰\n",
    "            if 'attention_mask' in batch:\n",
    "                conditioning = self.text_encoder.encode(\n",
    "                    captions,\n",
    "                    attention_mask=batch['attention_mask'].view(-1, captions.shape[-1])\n",
    "                )[0]\n",
    "\n",
    "            # ãªã„å ´åˆï¼ˆOpenCLIPã‚„CLIPç”¨ï¼‰\n",
    "            else:\n",
    "                conditioning = self.text_encoder.encode(captions)[0]\n",
    "                logger.debug(f\"ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’å–å¾— {conditioning.shape=}\") \n",
    "\n",
    "        # drop_caption_maskãŒã‚ã‚‹å ´åˆï¼ˆCFGç”¨ï¼‰\n",
    "        if 'drop_caption_mask' in batch.keys():\n",
    "\n",
    "            # ä¸€å®šç¢ºç‡ã§ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’ã‚¼ãƒ­ã«ã™ã‚‹\n",
    "            conditioning *= batch['drop_caption_mask'].view(\n",
    "                [-1] + [1] * (len(conditioning.shape) - 1)\n",
    "            )\n",
    "\n",
    "        # 3) EDMæå¤±ã®è¨ˆç®—\n",
    "\n",
    "        # DiTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’è¨ˆç®—ã—ã€EDMæå¤±ã‚’è¨ˆç®—\n",
    "        loss = self.edm_loss(\n",
    "            latents.float(),\n",
    "            conditioning.float(),\n",
    "            mask_ratio=self.train_mask_ratio if self.training else self.eval_mask_ratio\n",
    "        )\n",
    "        return (loss, latents, conditioning)\n",
    "\n",
    "    def model_forward_wrapper(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        sigma: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        model_forward_fxn: callable,\n",
    "        mask_ratio: float,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        EDM(Elucidated Diffusion Models)ã«ãŠã‘ã‚‹äº‹å‰æ¡ä»¶ä»˜ã‘ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "        https://github.com/NVlabs/edm/blob/main/training/networks.py#L632\n",
    "\n",
    "        ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå­¦ç¿’ã—ã‚„ã™ã„ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ï¼ˆåˆ†æ•£1ä»˜è¿‘ï¼‰\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            sigma (torch.Tensor): ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            y (torch.Tensor): æ¡ä»¶ä»˜ã‘åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            model_forward_fxn (callable): DiTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­é–¢æ•°\n",
    "            mask_ratio (float): ãƒã‚¹ã‚¯æ¯”ç‡\n",
    "            **kwargs: è¿½åŠ ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "\n",
    "        Returns:\n",
    "            dict: ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›è¾æ›¸\n",
    "        \"\"\"\n",
    "        logger.info(f\"EDMãƒ¢ãƒ‡ãƒ«é †ä¼æ’­ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’å‘¼ã³å‡ºã—: {x.shape=}, {sigma.shape=}, {y.shape=}, {mask_ratio=}\")\n",
    "\n",
    "        # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’é©åˆ‡ãªå½¢çŠ¶ã«å¤‰æ›\n",
    "        sigma = sigma.to(x.dtype).reshape(-1, 1, 1, 1)\n",
    "        logger.debug(f\"ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’å¤‰æ› {sigma.shape=}\")\n",
    "\n",
    "        # å…¥åŠ›ç”»åƒã‚’ãã®ã¾ã¾å‡ºåŠ›ã«è¶³ã—åˆã‚ã›ã‚‹å‰²åˆï¼ˆã‚¹ã‚­ãƒƒãƒ—æ¥ç¶šï¼‰\n",
    "        c_skip = (\n",
    "            self.edm_config.sigma_data ** 2 /\n",
    "            (sigma ** 2 + self.edm_config.sigma_data ** 2)\n",
    "        )\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ä¿‚æ•°\n",
    "        c_out = (\n",
    "            sigma * self.edm_config.sigma_data /\n",
    "            (sigma ** 2 + self.edm_config.sigma_data ** 2).sqrt()\n",
    "        )\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹ç”»åƒã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ä¿‚æ•°\n",
    "        c_in = 1 / (self.edm_config.sigma_data ** 2 + sigma ** 2).sqrt()\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã«ç¾åœ¨ã®ãƒã‚¤ã‚ºã®å¼·ã•ã‚’ä¼ãˆã‚‹ãŸã‚ã®å€¤\n",
    "        c_noise = sigma.log() / 4\n",
    "\n",
    "        # 2) ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ\n",
    "\n",
    "        # DiTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’å‘¼ã³å‡ºã™\n",
    "        out = model_forward_fxn(\n",
    "            (c_in * x).to(x.dtype),\n",
    "            c_noise.flatten(),\n",
    "            y,\n",
    "            mask_ratio=mask_ratio,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿã®å‡ºåŠ›ã‚’å–å¾—\n",
    "        F_x = out['sample']\n",
    "\n",
    "        # 3) æœ€çµ‚çš„ãªäºˆæ¸¬å€¤ã®åˆæˆ\n",
    "\n",
    "        # ã™ã¹ã¦ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "        c_skip = c_skip.to(F_x.device)\n",
    "        x = x.to(F_x.device)\n",
    "        c_out = c_out.to(F_x.device)\n",
    "\n",
    "        # å…ƒã®å…¥åŠ›ã¨ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’åˆæˆã—ã¦æœ€çµ‚çš„ãªäºˆæ¸¬ã‚’å¾—ã‚‹\n",
    "        D_x = c_skip * x + c_out * F_x\n",
    "\n",
    "        out['sample'] = D_x\n",
    "        return out\n",
    "\n",
    "    def edm_loss(self, x: torch.Tensor, y: torch.Tensor, mask_ratio: float = 0, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        EDM(Elucidated Diffusion Models)æå¤±è¨ˆç®—\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): ã‚¯ãƒªãƒ¼ãƒ³ãªç”»åƒãƒ†ãƒ³ã‚½ãƒ«\n",
    "            y (torch.Tensor): æ¡ä»¶ä»˜ã‘åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            mask_ratio (float, optional): ãƒã‚¹ã‚¯æ¯”ç‡ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã€‚\n",
    "        Returns:\n",
    "            torch.Tensor: è¨ˆç®—ã•ã‚ŒãŸæå¤±ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"EDMæå¤±ã‚’è¨ˆç®—: {x.shape=}, {y.shape=}, {mask_ratio=}\")\n",
    "\n",
    "        # 1) ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "\n",
    "        # æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ä¹±æ•°ã‚’ç”Ÿæˆ\n",
    "        rnd_normal = torch.randn([x.shape[0], 1, 1, 1], device=x.device)\n",
    "\n",
    "        # å¯¾æ•°æ­£è¦åˆ†å¸ƒã‹ã‚‰ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        sigma = (rnd_normal * self.edm_config.P_std + self.edm_config.P_mean).exp()\n",
    "\n",
    "        # 2) æå¤±ã®é‡ã¿ä»˜ã‘ã®è¨ˆç®—\n",
    "\n",
    "        # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ã€æå¤±é–¢æ•°ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "        # ãƒã‚¤ã‚ºãŒå¤§ãã„ã»ã©ã€æå¤±ã®é‡ã¿ãŒå°ã•ããªã‚‹\n",
    "        weight = (\n",
    "            (sigma ** 2 + self.edm_config.sigma_data ** 2) /\n",
    "            (sigma * self.edm_config.sigma_data) ** 2\n",
    "        )\n",
    "\n",
    "        # 3) ãƒã‚¤ã‚ºã®è¿½åŠ ã¨ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­\n",
    "\n",
    "        # ãƒã‚¤ã‚ºã‚’ç”Ÿæˆ\n",
    "        n = self.randn_like(x) * sigma\n",
    "\n",
    "        #ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã€ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’å®Ÿè¡Œ\n",
    "        model_out = self.model_forward_wrapper(\n",
    "            x + n, # ãƒã‚¤ã‚ºã‚’è¿½åŠ \n",
    "            sigma,\n",
    "            y,\n",
    "            self.dit,\n",
    "            mask_ratio=mask_ratio,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # ãƒã‚¤ã‚ºãŒé™¤å»ã•ã‚ŒãŸç”»åƒã‚’å–å¾—\n",
    "        D_xn = model_out['sample']\n",
    "\n",
    "        # 4) åŸºæœ¬çš„ãªæå¤±ã®è¨ˆç®—\n",
    "\n",
    "        # äºˆæ¸¬ç”»åƒã¨æ­£è§£ç”»åƒã®äºŒä¹—èª¤å·®ã‚’è¨ˆç®—ã—ã€é‡ã¿ã‚’é©ç”¨\n",
    "        loss = weight * ((D_xn - x) ** 2)  # (N, C, H, W)\n",
    "        logger.debug(f\"åŸºæœ¬çš„ãªæå¤±ã‚’è¨ˆç®— {loss.shape=}\")\n",
    "\n",
    "        # 5) ãƒã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã®æå¤±è¨ˆç®—\n",
    "\n",
    "        # ãƒã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆ\n",
    "        if mask_ratio > 0:\n",
    "            # Masking is not feasible during image generation as it only returns denoised version\n",
    "            # for non-masked patches. Image generation requires all patches to be denoised.\n",
    "            assert (\n",
    "                self.dit.training and 'mask' in model_out\n",
    "            ), 'Masking is only recommended during training'\n",
    "            loss = F.avg_pool2d(loss.mean(dim=1), self.dit.patch_size).flatten(1)\n",
    "\n",
    "            # ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã‚’å–å¾—\n",
    "            unmask = 1 - model_out['mask']\n",
    "            logger.debug(f\"ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã‚’å–å¾— {unmask.shape=}\")\n",
    "\n",
    "            # ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã®æå¤±ã®ã¿ã‚’è¨ˆç®—\n",
    "            loss = (loss * unmask).sum(dim=1) / unmask.sum(dim=1)  # (N,)\n",
    "            logger.debug(f\"ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã®æå¤±ã‚’è¨ˆç®— {loss.shape=}\")\n",
    "\n",
    "        # å¹³å‡æå¤±ã‚’è¿”ã™\n",
    "        return loss.mean()\n",
    "\n",
    "    def loss(self, outputs: tuple, batch: dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Composerç‰¹æœ‰ã®æå¤±é–¢æ•°ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "\n",
    "        Args:\n",
    "            outputs (tuple): ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚¿ãƒ—ãƒ«\n",
    "            batch (dict): ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸\n",
    "        Returns:\n",
    "            torch.Tensor: è¨ˆç®—ã•ã‚ŒãŸæå¤±ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(\"lossé–¢æ•°ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’å‘¼ã³å‡ºã—\")\n",
    "\n",
    "        # forwardãƒ¡ã‚½ãƒƒãƒ‰ã§è¨ˆç®—æ¸ˆã¿ã®æå¤±ã‚’ãã®ã¾ã¾è¿”ã™\n",
    "        return outputs[0]\n",
    "\n",
    "    def eval_forward(self, batch: dict, outputs: Optional[tuple] = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Composerç‰¹æœ‰ã®è©•ä¾¡ç”¨é †ä¼æ’­ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "\n",
    "        Args:\n",
    "            batch (dict): ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸\n",
    "            outputs (Optional[tuple], None): äº‹å‰è¨ˆç®—ã•ã‚ŒãŸå‡ºåŠ›ã‚¿ãƒ—ãƒ«\n",
    "        Returns:\n",
    "            tuple: è©•ä¾¡ç”¨ã®å‡ºåŠ›ã‚¿ãƒ—ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(\"eval_forwardã‚’å‘¼ã³å‡ºã—\")\n",
    "\n",
    "        # è¨ˆç®—æ¸ˆã¿ã®å ´åˆ\n",
    "        if outputs is not None:\n",
    "            # ãã®ã¾ã¾è¿”ã™\n",
    "            return outputs\n",
    "\n",
    "        # è¨ˆç®—ã•ã‚Œã¦ã„ãªã„å ´åˆã¯forwardãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—æå¤±ã‚’è¨ˆç®—\n",
    "        loss, _, _ = self.forward(batch)\n",
    "\n",
    "        return loss, None, None\n",
    "\n",
    "    def get_metrics(self, is_train: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Composerç‰¹æœ‰ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯å–å¾—ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "\n",
    "        Args:\n",
    "            is_train (bool, optional): è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã€‚\n",
    "        Returns:\n",
    "            dict: ãƒ¡ãƒˆãƒªãƒƒã‚¯è¾æ›¸\n",
    "        \"\"\"\n",
    "        logger.info(f\"get_metricsã‚’å‘¼ã³å‡ºã— {is_train=}\")\n",
    "        return {'loss': DistLoss()}\n",
    "\n",
    "    def update_metric(self, batch: dict, outputs: tuple, metric: DistLoss):\n",
    "        \"\"\"\n",
    "        Composerç‰¹æœ‰ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯æ›´æ–°ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "\n",
    "        Args:\n",
    "            batch (dict): ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸\n",
    "            outputs (tuple): ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚¿ãƒ—ãƒ«\n",
    "            metric (DistLoss): æ›´æ–°ã™ã‚‹ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "        \"\"\"\n",
    "        logger.info(\"update_metricã‚’å‘¼ã³å‡ºã—\")\n",
    "        metric.update(outputs[0])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def edm_sampler_loop(\n",
    "        self,\n",
    "        x: torch.Tensor, \n",
    "        y: torch.Tensor, \n",
    "        steps: Optional[int] = None, \n",
    "        cfg: float = 1.0, \n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        EDM(Elucidated Diffusion Models)ã«åŸºã¥ã„ã¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—\n",
    "        Args:\n",
    "            x (torch.Tensor): åˆæœŸãƒã‚¤ã‚ºãƒ†ãƒ³ã‚½ãƒ«\n",
    "            y (torch.Tensor): æ¡ä»¶ä»˜ã‘åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            steps (Optional[int], None): ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€‚Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            cfg (float, optional): Classifier-Free Guidanceã®ã‚¹ã‚±ãƒ¼ãƒ«ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã€‚\n",
    "            **kwargs: è¿½åŠ ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "        Returns:\n",
    "            torch.Tensor: ç”Ÿæˆã•ã‚ŒãŸç”»åƒãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆæœŸåŒ–\n",
    "\n",
    "        # ãƒã‚¹ã‚¯ã®å‰²åˆã¯ã‚¼ãƒ­\n",
    "        mask_ratio = 0\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­é–¢æ•°ã‚’è¨­å®šï¼ˆCFGãŒæœ‰åŠ¹ãªå ´åˆã¯éƒ¨åˆ†é©ç”¨ï¼‰\n",
    "        model_forward_fxn = (\n",
    "            partial(self.dit.forward, cfg=cfg) if cfg > 1.0\n",
    "            else self.dit.forward\n",
    "        )\n",
    "\n",
    "        # 2) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è¨­å®š\n",
    "\n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "        num_steps = self.edm_config.num_steps if steps is None else steps\n",
    "        logger.debug(f\"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨­å®š {num_steps=}\")\n",
    "\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ—ã‚’ä½œæˆ\n",
    "        # [0, 1, 2, ..., num_steps-1]\n",
    "        step_indices = torch.arange(num_steps, dtype=torch.float64, device=x.device)\n",
    "\n",
    "        # EDMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ã„ã¦ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—\n",
    "        t_steps = (\n",
    "            self.edm_config.sigma_max ** (1 / self.edm_config.rho) +\n",
    "            step_indices / (num_steps - 1) *\n",
    "            (self.edm_config.sigma_min ** (1 / self.edm_config.rho) -\n",
    "             self.edm_config.sigma_max ** (1 / self.edm_config.rho))\n",
    "        ) ** self.edm_config.rho\n",
    "\n",
    "        # æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã«ã‚¼ãƒ­ã‚’è¿½åŠ \n",
    "        t_steps = torch.cat([torch.as_tensor(t_steps), torch.zeros_like(t_steps[:1])])\n",
    "\n",
    "        # 3) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ\n",
    "\n",
    "        # åˆæœŸãƒã‚¤ã‚ºã‚’ã€æœ€åˆã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        x_next = x.to(torch.float64) * t_steps[0]\n",
    "\n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "        # t_cur: ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã€€t_next: æ¬¡ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—\n",
    "        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):  # 0, ..., N-1\n",
    "\n",
    "            # ç¾åœ¨ã®ç”»åƒã‚’è¨­å®š\n",
    "            x_cur = x_next\n",
    "\n",
    "            # ç¢ºç‡çš„æ’¹ä¹±ï¼ˆStochastic Churn)\n",
    "            # ãƒã‚¤ã‚ºã‚’ä¸€æ™‚çš„ã«å¢—åŠ ã•ã›ã‚‹\n",
    "            # æ±ºå®šè«–çš„ãªODEã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã€ç¢ºç‡çš„ãªè¦ç´ ï¼ˆSDEï¼‰ã‚’æ··ãœã€ç”»è³ªã‚’å‘ä¸Šã•ã›ã‚‹\n",
    "            gamma = (\n",
    "                min(self.edm_config.S_churn / num_steps, np.sqrt(2) - 1)\n",
    "                if self.edm_config.S_min <= t_cur <= self.edm_config.S_max else 0\n",
    "            )\n",
    "\n",
    "            # ãƒã‚¤ã‚ºã‚’å¢—åŠ ã•ã›ãŸæ–°ã—ã„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—\n",
    "            t_hat = torch.as_tensor(t_cur + gamma * t_cur)\n",
    "\n",
    "            # å®Ÿéš›ã«ç”»åƒã«ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹\n",
    "            x_hat = (\n",
    "                x_cur +\n",
    "                (t_hat ** 2 - t_cur ** 2).sqrt() *\n",
    "                self.edm_config.S_noise *\n",
    "                self.randn_like(x_cur)\n",
    "            )\n",
    "\n",
    "            # 3) ã‚ªã‚¤ãƒ©ãƒ¼æ³•ã«ã‚ˆã‚‹äºˆæ¸¬ã¨ç§»å‹•\n",
    "\n",
    "            # DiTã‚’ä½¿ã£ã¦ã€ãƒã‚¤ã‚ºé™¤å»ã•ã‚ŒãŸç”»åƒã‚’äºˆæ¸¬ã—ã€å€ç²¾åº¦ã«ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "            denoised = self.model_forward_wrapper(\n",
    "                x_hat.to(torch.float32),\n",
    "                t_hat.to(torch.float32),\n",
    "                y,\n",
    "                model_forward_fxn,\n",
    "                mask_ratio=mask_ratio,\n",
    "                **kwargs\n",
    "            )['sample'].to(torch.float64)\n",
    "\n",
    "            # ç¾åœ¨ä½ç½®ã‹ã‚‰ç¶ºéº—ãªç”»åƒã¸ã®å‹¾é…ã‚’è¨ˆç®—\n",
    "            d_cur = (x_hat - denoised) / t_hat\n",
    "\n",
    "            # æ¬¡ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—(t_next)ã¸å‘ã‹ã£ã¦ç§»å‹•\n",
    "            x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "\n",
    "            # 2) ãƒ›ã‚¤ãƒ³æ³•ï¼ˆHeun's methodï¼‰ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š\n",
    "\n",
    "            if i < num_steps - 1:\n",
    "\n",
    "                # ã‚ªã‚¤ãƒ©ãƒ¼æ³•ã§åˆ°é”ã—ãŸä»®ã®ä½ç½®ã§å†åº¦äºˆæ¸¬ã—ã€å€ç²¾åº¦ã«ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "                denoised = self.model_forward_wrapper(\n",
    "                    x_next.to(torch.float32),\n",
    "                    t_next.to(torch.float32),\n",
    "                    y,\n",
    "                    model_forward_fxn,\n",
    "                    mask_ratio=mask_ratio,\n",
    "                    **kwargs\n",
    "                )['sample'].to(torch.float64)\n",
    "\n",
    "                # ä»®ã®åœ°ç‚¹ã‹ã‚‰ç¶ºéº—ãªç”»åƒã¸ã®å‹¾é…ã‚’è¨ˆç®—\n",
    "                d_prime = (x_next - denoised) / t_next\n",
    "\n",
    "                # æœ€åˆã®å‚¾ãã¨ã€ã‚ªã‚¤ãƒ©ãƒ¼æ³•ã§åˆ°é”ã—ãŸå‚¾ãã‚’å¹³å‡åŒ–ã—ã¦ã€ç§»å‹•ä½ç½®ã‚’æ±ºå®š\n",
    "                x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "\n",
    "        return x_next.to(torch.float32)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Optional[list] = None,\n",
    "        tokenized_prompts: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        guidance_scale: Optional[float] = 5.0,\n",
    "        num_inference_steps: Optional[int] = 30,\n",
    "        seed: Optional[int] = None,\n",
    "        return_only_latents: Optional[bool] = False,\n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            prompt (Optional[list], None): ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            tokenized_prompts (Optional[torch.LongTensor], None): ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ã‚½ãƒ«ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            attention_mask (Optional[torch.LongTensor], None): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆT5ç”¨ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            guidance_scale (Optional[float], 5.0): Classifier-Free Guidanceã®ã‚¹ã‚±ãƒ¼ãƒ«ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯5.0ã€‚\n",
    "            num_inference_steps (Optional[int], 30): ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯30ã€‚\n",
    "            seed (Optional[int], None): ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            return_only_latents (Optional[bool], False): æ½œåœ¨å¤‰æ•°ã®ã¿ã‚’è¿”ã™ã‹ã©ã†ã‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã€‚\n",
    "            **kwargs: è¿½åŠ ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "        Returns:\n",
    "            torch.Tensor: ç”Ÿæˆã•ã‚ŒãŸç”»åƒãƒ†ãƒ³ã‚½ãƒ«ã‚‚ã—ãã¯æ½œåœ¨å¤‰æ•°ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ç”»åƒã‚’ç”Ÿæˆ: {prompt=}, {tokenized_prompts is not None=}, {guidance_scale=}, {num_inference_steps=}, {seed=}, {return_only_latents=}\")\n",
    "\n",
    "        # 1) æº–å‚™\n",
    "\n",
    "        # _check_prompt_given(prompt, tokenized_prompts, prompt_embeds=None)\n",
    "        assert prompt or tokenized_prompts, \"Must provide either prompt or tokenized prompts\"\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\n",
    "        device = self.vae.device\n",
    "\n",
    "        # ä¹±æ•°ç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–\n",
    "        rng_generator = torch.Generator(device=device)\n",
    "\n",
    "        # ã‚·ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        if seed:\n",
    "            # ä¹±æ•°ç”Ÿæˆå™¨ã«ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "            rng_generator = rng_generator.manual_seed(seed)\n",
    "\n",
    "        # 2) ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã®å–å¾—\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        if tokenized_prompts is None:\n",
    "\n",
    "            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "            out = self.tokenizer.tokenize(prompt)\n",
    "\n",
    "            # ãƒˆãƒ¼ã‚¯ãƒ³IDã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å–å¾—\n",
    "            tokenized_prompts = out['input_ids']\n",
    "            logger.debug(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ– {tokenized_prompts.shape=}\")\n",
    "\n",
    "            # T5ç”¨ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "            attention_mask = (\n",
    "                out['attention_mask'] if 'attention_mask' in out else None\n",
    "            )\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "        text_embeddings = self.text_encoder.encode(\n",
    "            tokenized_prompts.to(device),\n",
    "            attention_mask=attention_mask.to(device) if attention_mask is not None else None\n",
    "        )[0]\n",
    "        logger.debug(f\"ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’å–å¾— {text_embeddings.shape=}\")\n",
    "\n",
    "        # 3) åˆæœŸãƒã‚¤ã‚ºã‚’ç”Ÿæˆ\n",
    "\n",
    "        # ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¤ã‚ºã‚’ä½œæˆï¼ˆæ½œåœ¨å¤‰æ•°ï¼‰\n",
    "        latents = torch.randn(\n",
    "            (len(text_embeddings), self.dit.in_channels, self.latent_res, self.latent_res),\n",
    "            device=device,\n",
    "            generator=rng_generator,\n",
    "        )\n",
    "        logger.debug(f\"åˆæœŸãƒã‚¤ã‚ºã‚’ç”Ÿæˆ {latents.shape=}\")\n",
    "\n",
    "        # 4) EDMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã§ç”»åƒã‚’ç”Ÿæˆ\n",
    "\n",
    "        # ãƒã‚¤ã‚ºé™¤å»ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ\n",
    "        latents = self.edm_sampler_loop(\n",
    "            latents,\n",
    "            text_embeddings,\n",
    "            num_inference_steps,\n",
    "            cfg=guidance_scale\n",
    "        )\n",
    "\n",
    "        # æ½œåœ¨å¤‰æ•°ã®ã¿ã‚’è¿”ã™å ´åˆ\n",
    "        if return_only_latents:\n",
    "            return latents\n",
    "\n",
    "        # 5) ç”»åƒã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦è¿”ã™\n",
    "\n",
    "        # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æˆ»ã™\n",
    "        latents = 1 / self.latent_scale * latents\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—\n",
    "        torch_dtype = DATA_TYPES[self.dtype]\n",
    "        logger.debug(f\"ãƒ‡ã‚³ãƒ¼ãƒ‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾— {torch_dtype=}\")\n",
    "\n",
    "        # VAEã§æ½œåœ¨å¤‰æ•°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ç”»åƒã«å¤‰æ›\n",
    "        image = self.vae.decode(latents.to(torch_dtype)).sample\n",
    "\n",
    "        # ç”»åƒã®ãƒ”ã‚¯ã‚»ãƒ«å€¤ã‚’[0, 1]ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "        # æµ®å‹•å°æ•°ç‚¹å‹ã«å¤‰æ›ã—ã¦è¿”ã™\n",
    "        image = image.float().detach()\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0720cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latent_diffusion(\n",
    "    vae_name: str = 'stabilityai/stable-diffusion-xl-base-1.0',\n",
    "    text_encoder_name: str = 'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378', \n",
    "    dit_arch: str = 'MicroDiT_XL_2',\n",
    "    latent_res: int = 32,\n",
    "    in_channels: int = 4,\n",
    "    pos_interp_scale: float = 1.0,\n",
    "    dtype: str = 'bfloat16',\n",
    "    precomputed_latents: bool = True,\n",
    "    p_mean: float = -0.6,\n",
    "    p_std: float = 1.2,\n",
    "    train_mask_ratio: float = 0.\n",
    ") -> LatentDiffusion:\n",
    "    \"\"\"\n",
    "    Latent Diffusionãƒ¢ãƒ‡ãƒ«ï¼ˆLDMï¼‰ã‚’ä½œæˆã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        vae_name (str, optional): ä½¿ç”¨ã™ã‚‹VAEãƒ¢ãƒ‡ãƒ«ã®åå‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'stabilityai/stable-diffusion-xl-base-1.0'ã€‚\n",
    "        text_encoder_name (str, optional): ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®åå‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'ã€‚\n",
    "        dit_arch (str, optional): ä½¿ç”¨ã™ã‚‹DiTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åå‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'MicroDiT_XL_2'ã€‚\n",
    "        latent_res (int, optional): æ½œåœ¨ç©ºé–“ã®è§£åƒåº¦ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯32ã€‚\n",
    "        in_channels (int, optional): DiTãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4ã€‚\n",
    "        pos_interp_scale (float, optional): ä½ç½®åŸ‹ã‚è¾¼ã¿ã®è£œé–“ã‚¹ã‚±ãƒ¼ãƒ«ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã€‚\n",
    "        dtype (str, optional): è¨ˆç®—ç²¾åº¦ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'bfloat16'ã€‚\n",
    "        precomputed_latents (bool, optional): äº‹å‰è¨ˆç®—ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Trueã€‚\n",
    "        p_mean (float, optional): EDMã®å¯¾æ•°æ­£è¦ãƒã‚¤ã‚ºã®å¹³å‡ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ -0.6ã€‚\n",
    "        p_std (float, optional): EDMã®å¯¾æ•°æ­£è¦ãƒã‚¤ã‚ºã®æ¨™æº–åå·®ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.2ã€‚\n",
    "        train_mask_ratio (float, optional): å­¦ç¿’ä¸­ã«ç”»åƒã‚’ãƒã‚¹ã‚¯ã™ã‚‹å‰²åˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã€‚\n",
    "    Returns:\n",
    "        LatentDiffusion: ä½œæˆã•ã‚ŒãŸLatent Diffusionãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "    \"\"\"\n",
    "    logger.info(f\"Latent Diffusionãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ: {vae_name=}, {text_encoder_name=}, {dit_arch=}, {latent_res=}, {in_channels=}, {pos_interp_scale=}, {dtype=}, {precomputed_latents=}, {p_mean=}, {p_std=}, {train_mask_ratio=}\")\n",
    "\n",
    "    # 1) ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®æƒ…å ±ã‚’æŠ½å‡º\n",
    "\n",
    "    # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·(s)ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ(d)ã‚’å–å¾—\n",
    "    s, d = text_encoder_embedding_format(text_encoder_name)\n",
    "\n",
    "    # 2) DiTã®åˆæœŸåŒ–\n",
    "\n",
    "    # Tinyã®å ´åˆ\n",
    "    if dit_arch == 'MicroDiT_Tiny_2':\n",
    "        dit = MicroDiT_Tiny_2(\n",
    "            input_size=latent_res,\n",
    "            caption_channels=d,\n",
    "            pos_interp_scale=pos_interp_scale,\n",
    "            in_channels=in_channels\n",
    "        )\n",
    "\n",
    "    # XLã®å ´åˆ\n",
    "    elif dit_arch == 'MicroDiT_XL_2':\n",
    "        dit = MicroDiT_XL_2(\n",
    "            input_size=latent_res,\n",
    "            caption_channels=d,\n",
    "            pos_interp_scale=pos_interp_scale,\n",
    "            in_channels=in_channels\n",
    "        )\n",
    "    # ãã®ä»–\n",
    "    else:\n",
    "        raise ValueError(f'Unknown DiT architecture: {dit_arch}')\n",
    "\n",
    "    # 3) VAEã®åˆæœŸåŒ–\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        vae_name,\n",
    "        subfolder=None if vae_name=='ostris/vae-kl-f8-d16' else 'vae',\n",
    "        torch_dtype=DATA_TYPES[dtype],\n",
    "        pretrained=True\n",
    "    )\n",
    "\n",
    "    # 4) ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™\n",
    "\n",
    "    text_encoder = UniversalTextEncoder(\n",
    "        text_encoder_name,\n",
    "        dtype=dtype,\n",
    "        pretrained=True\n",
    "    )\n",
    "\n",
    "    # 5) ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™\n",
    "\n",
    "    tokenizer = UniversalTokenizer(text_encoder_name)\n",
    "\n",
    "    # 6) Latent Diffusionãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "\n",
    "    model = LatentDiffusion(\n",
    "        dit=dit,\n",
    "        vae=vae,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        precomputed_latents=precomputed_latents,\n",
    "        dtype=dtype,\n",
    "        latent_res=latent_res,\n",
    "        p_mean=p_mean,\n",
    "        p_std=p_std,\n",
    "        train_mask_ratio=train_mask_ratio\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70324359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_latent_diffusion(\n",
    "    vae_name='stabilityai/stable-diffusion-xl-base-1.0',\n",
    "    text_encoder_name='openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378',\n",
    "    # dit_arch='MicroDiT_XL_2',\n",
    "    dit_arch='MicroDiT_Tiny_2',\n",
    "    latent_res=32,\n",
    "    in_channels=4,\n",
    "    pos_interp_scale=1.0,\n",
    "    dtype='bfloat16',\n",
    "    precomputed_latents=True,\n",
    "    p_mean=-0.6,\n",
    "    p_std=1.2,\n",
    "    train_mask_ratio=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada8eda",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2228d7",
   "metadata": {},
   "source": [
    "### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_metadata():\n",
    "    # Only using a single process for downloading metadata\n",
    "    metadata_files = [\n",
    "        ('data/train', 'train_anno.jsonl.tgz'),\n",
    "        ('data/train', 'train_anno_realease_repath.jsonl.tgz'),\n",
    "        ('data/valid', 'valid_anno_repath.jsonl.tgz'),\n",
    "        ('data/test', 'test_questions.jsonl.tgz'),\n",
    "        ('data/test', 'imgs.tgz'),\n",
    "    ]\n",
    "\n",
    "    for subfolder, filename in metadata_files:\n",
    "        hf_hub_download(\n",
    "            repo_id=\"JourneyDB/JourneyDB\",\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=subfolder,\n",
    "            filename=filename,\n",
    "            local_dir=COMPRESSED_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "\n",
    "    metadata_tars = [\n",
    "        os.path.join(dir, fname) for (dir, fname) in metadata_files\n",
    "    ]\n",
    "\n",
    "    for tar_file in metadata_tars:\n",
    "        subprocess.call(\n",
    "            f'tar -xvzf {os.path.join(COMPRESSED_DIR, tar_file)} '\n",
    "            f'-C {os.path.join(COMPRESSED_DIR, os.path.dirname(tar_file))}',\n",
    "            shell=True,\n",
    "        )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/train/train_anno_realease_repath.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"train/train_anno_realease_repath.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/valid/valid_anno_repath.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"valid/valid_anno_repath.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.copy(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/test/test_questions.jsonl\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"test/test_questions.jsonl\")}',\n",
    "    )\n",
    "\n",
    "    shutil.move(\n",
    "        f'{os.path.join(COMPRESSED_DIR, \"data/test/imgs\")}',\n",
    "        f'{os.path.join(RAW_DIR, \"test/\")}',\n",
    "    )\n",
    "\n",
    "COMPRESSED_DIR = os.path.join(DATA_DIR, 'compressed')\n",
    "logger.info(f\"{COMPRESSED_DIR=}\")\n",
    "\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "logger.info(f\"{RAW_DIR=}\")\n",
    "\n",
    "if not os.path.exists(COMPRESSED_DIR):\n",
    "    os.makedirs(COMPRESSED_DIR, exist_ok=True)\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "    download_and_process_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737dbd3",
   "metadata": {},
   "source": [
    "### è§£å‡ã¨ãƒªã‚µã‚¤ã‚º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df247d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_uncompress_resize(\n",
    "    valid_ids: list,\n",
    "    max_image_size: int,\n",
    "    min_image_size: int,\n",
    "    split: str,\n",
    "    idx: int,\n",
    "):\n",
    "    \"\"\"Download, uncompress, and resize images for a given archive index.\"\"\"\n",
    "    assert split in ('train', 'valid')\n",
    "    assert idx in valid_ids\n",
    "\n",
    "    print(f\"Downloading idx: {idx}\")\n",
    "    if not os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/'):\n",
    "        hf_hub_download(\n",
    "            repo_id=\"JourneyDB/JourneyDB\",\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=f'data/{split}/imgs',\n",
    "            filename=f'{idx:>03}.tgz',\n",
    "            local_dir=COMPRESSED_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "    print(f\"Downloaded idx: {idx}\")\n",
    "\n",
    "    print(f\"Extracting idx: {idx}\")\n",
    "    if not os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/'):\n",
    "        subprocess.call(\n",
    "            f'tar -xzf {COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz '\n",
    "            f'-C {COMPRESSED_DIR}/data/{split}/imgs/',\n",
    "            shell=True,\n",
    "        )\n",
    "    print(f\"Extracted idx: {idx}\")\n",
    "\n",
    "    print(f\"Removing idx: {idx}\")\n",
    "    if os.path.exists(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz'):\n",
    "        os.remove(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}.tgz')\n",
    "    print(f\"Removed idx: {idx}\")\n",
    "\n",
    "    # add bicubic downsize\n",
    "    downsize = transforms.Resize(\n",
    "        max_image_size,\n",
    "        antialias=True,\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    )\n",
    "\n",
    "    print(f\"Downsizing idx: {idx}\")\n",
    "    os.makedirs(\n",
    "        f'{RAW_DIR}/{split}/imgs/{idx:>03}/',\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    for f in iglob(f'{COMPRESSED_DIR}/data/{split}/imgs/{idx:>03}/*'):\n",
    "        save_path = f'{RAW_DIR}/{split}/imgs/{idx:>03}/{os.path.basename(f)}'\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "            try:\n",
    "                img = Image.open(f)\n",
    "                w, h = img.size\n",
    "                if min(w, h) > max_image_size:\n",
    "                    img = downsize(img)\n",
    "                if min(w, h) < min_image_size:\n",
    "                    print(\n",
    "                        f'Skipping image with resolution ({h}, {w}) - '\n",
    "                        f'Since at least one side has resolution below {min_image_size}'\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                img.save(save_path)\n",
    "                os.remove(f)\n",
    "            except (UnidentifiedImageError, OSError) as e:\n",
    "                print(f\"Error {e}, File: {f}\")\n",
    "    print(f'Downsized idx: {idx}')\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(RAW_DIR, 'train/imgs')):\n",
    "\n",
    "    valid_ids = list(np.arange(1))\n",
    "    pool_args = [('train', i) for i in valid_ids] + [('valid', i) for i in valid_ids]\n",
    "    max_image_size = 512\n",
    "    min_image_size = 64\n",
    "\n",
    "    with Pool(processes=4) as pool:\n",
    "        pool.starmap(\n",
    "            download_uncompress_resize,\n",
    "            [(\n",
    "                valid_ids,\n",
    "                max_image_size,\n",
    "                min_image_size,\n",
    "                split,\n",
    "                idx\n",
    "            ) for split, idx in pool_args]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221dffcd",
   "metadata": {},
   "source": [
    "### MDSå½¢å¼ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50319bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mds(\n",
    "    images_dir: str,\n",
    "    captions_jsonl: str,\n",
    "    local_mds_dir: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    JourneyDBã‚’MDSå½¢å¼ã«å¤‰æ›ã™ã‚‹\n",
    "    MDSï¼ˆMosaic Data Storageï¼‰ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®é«˜é€Ÿã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "\n",
    "    Args:\n",
    "        images_dir: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "        captions_jsonl: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "        local_mds_dir: å‡ºåŠ›MDSãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    columns = {\n",
    "        'width': 'int32',\n",
    "        'height': 'int32',\n",
    "        'jpg': 'jpeg',\n",
    "        'caption': 'str',\n",
    "    }\n",
    "    \n",
    "    writer = MDSWriter(\n",
    "        out=local_mds_dir,\n",
    "        columns=columns,\n",
    "        compression=None,\n",
    "        size_limit=256 * (2**20),\n",
    "        max_workers=64,\n",
    "    )\n",
    "    \n",
    "    # Retrieving achieve indies, in case only a subset of the data is downloaded\n",
    "    valid_archieve_idx = [\n",
    "        os.path.basename(p) for p in glob(os.path.join(images_dir, '*'))\n",
    "    ]\n",
    "    \n",
    "    metadata = list(open(captions_jsonl, 'r'))\n",
    "\n",
    "    for f in tqdm(metadata):\n",
    "        d = json.loads(f)\n",
    "        cap, p = d['prompt'], d['img_path'].strip('./')\n",
    "        \n",
    "        if os.path.dirname(p) not in valid_archieve_idx:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            img = Image.open(os.path.join(images_dir, p))\n",
    "            w, h = img.size\n",
    "            mds_sample = {\n",
    "                'jpg': img,\n",
    "                'caption': cap,\n",
    "                'width': w,\n",
    "                'height': h,\n",
    "            }\n",
    "            writer.write(mds_sample)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Something went wrong in reading caption, \"\n",
    "                f\"skipping writing this sample in mds. Error: {e}\"\n",
    "            )\n",
    "\n",
    "    writer.finish()\n",
    "\n",
    "MDS_DIR = os.path.join(DATA_DIR, 'jdb', 'mds')\n",
    "logger.info(f\"{MDS_DIR=}\")\n",
    "\n",
    "if not os.path.exists(MDS_DIR):\n",
    "    convert_to_mds(\n",
    "        images_dir=os.path.join(RAW_DIR, 'train', 'imgs'),\n",
    "        captions_jsonl=os.path.join(RAW_DIR, 'train', 'train_anno_realease_repath.jsonl'),\n",
    "        local_mds_dir=os.path.join(MDS_DIR, 'train'),\n",
    "    )\n",
    "\n",
    "    convert_to_mds(\n",
    "        images_dir=os.path.join(RAW_DIR, 'valid', 'imgs'),\n",
    "        captions_jsonl=os.path.join(RAW_DIR, 'valid', 'valid_anno_repath.jsonl'),\n",
    "        local_mds_dir=os.path.join(MDS_DIR, 'valid'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912aa78",
   "metadata": {},
   "source": [
    "### å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingJdbDatasetForPreCompute(StreamingDataset):\n",
    "    \"\"\"Streaming dataset that resizes images to user-provided resolutions and tokenizes captions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        streams: Sequence[Stream],\n",
    "        transforms_list: List[Callable],\n",
    "        batch_size: int,\n",
    "        tokenizer_name: str,\n",
    "        shuffle: bool = False,\n",
    "        caption_key: str = 'caption',\n",
    "    ):\n",
    "        super().__init__(\n",
    "            streams=streams,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        self.transforms_list = transforms_list\n",
    "        self.caption_key = caption_key\n",
    "        self.tokenizer = UniversalTokenizer(tokenizer_name)\n",
    "        print(\"Created tokenizer: \", tokenizer_name)\n",
    "        assert self.transforms_list is not None, 'Must provide transforms to resize and center crop images'\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict:\n",
    "        sample = super().__getitem__(index)\n",
    "        ret = {}\n",
    "\n",
    "        out = self.tokenizer.tokenize(sample[self.caption_key])\n",
    "        ret[self.caption_key] = out['input_ids'].clone().detach()\n",
    "        if 'attention_mask' in out:\n",
    "            ret[f'{self.caption_key}_attention_mask'] = out['attention_mask'].clone().detach()\n",
    "\n",
    "        for i, tr in enumerate(self.transforms_list):\n",
    "            img = sample['jpg']\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img = tr(img)\n",
    "            ret[f'image_{i}'] = img\n",
    "\n",
    "        ret['sample'] = sample\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_streaming_jdb_precompute_dataloader(\n",
    "    datadir: Union[List[str], str],\n",
    "    batch_size: int,\n",
    "    resize_sizes: Optional[List[int]] = None,\n",
    "    drop_last: bool = False,\n",
    "    shuffle: bool = True,\n",
    "    caption_key: Optional[str] = None,\n",
    "    tokenizer_name: Optional[str] = None,\n",
    "    **dataloader_kwargs,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Builds a streaming mds dataloader returning multiple image sizes and text captions.\"\"\"\n",
    "    assert resize_sizes is not None, 'Must provide target resolution for image resizing'\n",
    "    datadir = [datadir] if isinstance(datadir, str) else datadir\n",
    "    streams = [Stream(remote=None, local=l) for l in datadir]\n",
    "\n",
    "    transforms_list = []\n",
    "    for resize in resize_sizes:\n",
    "        transforms_list.append(\n",
    "            transforms.Compose([\n",
    "                transforms.Resize(\n",
    "                    resize,\n",
    "                    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "                ),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    dataset = StreamingJdbDatasetForPreCompute(\n",
    "        streams=streams,\n",
    "        shuffle=shuffle,\n",
    "        transforms_list=transforms_list,\n",
    "        batch_size=batch_size,\n",
    "        caption_key=caption_key,\n",
    "        tokenizer_name=tokenizer_name,\n",
    "    )\n",
    "\n",
    "    def custom_collate(list_of_dict: List[Dict]) -> Dict:\n",
    "        out = {k: [] for k in list_of_dict[0].keys()}\n",
    "        for d in list_of_dict:\n",
    "            for k, v in d.items():\n",
    "                out[k].append(v)\n",
    "        return out\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=custom_collate,\n",
    "        **dataloader_kwargs,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute(\n",
    "    datadir: str,\n",
    "    savedir: str = \"\",\n",
    "    image_resolutions: list = [256, 512],\n",
    "    save_images: bool = False,\n",
    "    model_dtype: str = \"bfloat16\",\n",
    "    save_dtype: str = \"float16\",\n",
    "    vae: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    text_encoder: str = \"openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\",\n",
    "    batch_size: int = 32,\n",
    "    seed: int = 2024,\n",
    "):\n",
    "    \"\"\"Precompute image and text latents and store them in MDS format.\n",
    "\n",
    "    By default, we only save the image latents for 256x256 and 512x512 image\n",
    "    resolutions (using center crop).\n",
    "\n",
    "    Note that the image latents will be scaled by the vae_scaling_factor.\n",
    "    \"\"\"\n",
    "    cap_key = 'caption'  # Hardcoding the image caption key to 'caption' in MDS dataset\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    device_idx = int(accelerator.process_index)\n",
    "\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(device_idx + seed)\n",
    "    torch.cuda.manual_seed(device_idx + seed)\n",
    "    np.random.seed(device_idx + seed)\n",
    "\n",
    "    dataloader = build_streaming_jdb_precompute_dataloader(\n",
    "        datadir=[datadir],\n",
    "        batch_size=batch_size,\n",
    "        resize_sizes=image_resolutions,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        caption_key=cap_key,\n",
    "        tokenizer_name=text_encoder,\n",
    "        # prefetch_factor=2,\n",
    "        # num_workers=2,\n",
    "        # persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f'Device: {device_idx}, Dataloader sample count: {len(dataloader.dataset)}')\n",
    "\n",
    "    # print(\n",
    "    #     f\"MP variable -> world size: {os.environ['WORLD_SIZE']}, \"\n",
    "    #     f\"RANK: {os.environ['RANK']}, {device}\"\n",
    "    # )\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        vae,\n",
    "        subfolder='vae',  # Change subfolder to appropriate one in hf_hub, if needed\n",
    "        torch_dtype=DATA_TYPES[model_dtype],\n",
    "    )\n",
    "    print(\"Created VAE: \", vae)\n",
    "    assert isinstance(vae, AutoencoderKL)\n",
    "\n",
    "    text_encoder = UniversalTextEncoder(\n",
    "        text_encoder,\n",
    "        dtype=model_dtype,\n",
    "        pretrained=True,\n",
    "    )\n",
    "    print(\"Created text encoder: \", text_encoder)\n",
    "\n",
    "    vae = vae.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "    columns = {\n",
    "        cap_key: 'str',\n",
    "        f'{cap_key}_latents': 'bytes',\n",
    "        'latents_256': 'bytes',\n",
    "        'latents_512': 'bytes',\n",
    "    }\n",
    "    if save_images:\n",
    "        columns['jpg'] = 'jpeg'\n",
    "\n",
    "    remote_upload = os.path.join(savedir, str(accelerator.process_index))\n",
    "\n",
    "    writer = MDSWriter(\n",
    "        out=remote_upload,\n",
    "        columns=columns,\n",
    "        compression=None,\n",
    "        size_limit=256 * (2**20),\n",
    "        max_workers=64,\n",
    "    )\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        image_256 = torch.stack(batch['image_0']).to(device)\n",
    "        image_512 = torch.stack(batch['image_1']).to(device)\n",
    "        captions = torch.stack(batch[cap_key]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type='cuda', dtype=DATA_TYPES[model_dtype]):\n",
    "                latent_dist_256 = vae.encode(image_256)\n",
    "                assert isinstance(latent_dist_256, AutoencoderKLOutput)\n",
    "                latents_256 = (\n",
    "                    latent_dist_256['latent_dist'].sample().data * vae.config.scaling_factor\n",
    "                ).to(DATA_TYPES[save_dtype])\n",
    "\n",
    "                latent_dist_512 = vae.encode(image_512)\n",
    "                assert isinstance(latent_dist_512, AutoencoderKLOutput)\n",
    "                latents_512 = (\n",
    "                    latent_dist_512['latent_dist'].sample().data * vae.config.scaling_factor\n",
    "                ).to(DATA_TYPES[save_dtype])\n",
    "\n",
    "                attention_mask = None\n",
    "\n",
    "                if f'{cap_key}_attention_mask' in batch:\n",
    "                    attention_mask = torch.stack(\n",
    "                        batch[f'{cap_key}_attention_mask']\n",
    "                    ).to(device)\n",
    "\n",
    "                conditioning = text_encoder.encode(\n",
    "                    captions.view(-1, captions.shape[-1]),\n",
    "                    attention_mask=attention_mask,\n",
    "                )[0].to(DATA_TYPES[save_dtype])\n",
    "\n",
    "        try:\n",
    "            if isinstance(latents_256, torch.Tensor) and isinstance(\n",
    "                latents_512, torch.Tensor\n",
    "            ):\n",
    "                latents_256 = latents_256.detach().cpu().numpy()\n",
    "                latents_512 = latents_512.detach().cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if isinstance(conditioning, torch.Tensor):\n",
    "                conditioning = conditioning.detach().cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Write the batch to the MDS file\n",
    "            for i in range(latents_256.shape[0]):\n",
    "                mds_sample = {\n",
    "                    cap_key: batch['sample'][i][cap_key],\n",
    "                    f'{cap_key}_latents': np.reshape(conditioning[i], -1).tobytes(),\n",
    "                    'latents_256': latents_256[i].tobytes(),\n",
    "                    'latents_512': latents_512[i].tobytes(),\n",
    "                }\n",
    "                if save_images:\n",
    "                    mds_sample['jpg'] = batch['sample'][i]['jpg']\n",
    "                writer.write(mds_sample)\n",
    "        except RuntimeError:\n",
    "            print('Runtime error CUDA, skipping this batch')\n",
    "\n",
    "    writer.finish()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    accelerator.wait_for_everyone()\n",
    "    print(f'Process {accelerator.process_index} finished')\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Merge the mds shards created by each device (only do on main process)\n",
    "    if accelerator.is_main_process:\n",
    "        shards_metadata = [\n",
    "            os.path.join(savedir, str(i), 'index.json')\n",
    "            for i in range(accelerator.num_processes)\n",
    "        ]\n",
    "        merge_index(shards_metadata, out=savedir, keep_local=True)\n",
    "\n",
    "PRECOMPUTE_DIR = os.path.join(DATA_DIR, 'mds_latents_sdxl1_dfnclipH14',)\n",
    "logger.info(f\"{PRECOMPUTE_DIR=}\")\n",
    "\n",
    "if not os.path.exists(PRECOMPUTE_DIR):\n",
    "    precompute(\n",
    "        datadir=os.path.join(DATA_DIR, 'jdb', 'mds', 'train'),\n",
    "        savedir=os.path.join(PRECOMPUTE_DIR, 'train'),\n",
    "        image_resolutions=[256, 512],\n",
    "        save_images=False,\n",
    "        model_dtype=\"bfloat16\",\n",
    "        save_dtype=\"float16\",\n",
    "        vae=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        text_encoder=\"openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378\",\n",
    "        batch_size=16,\n",
    "        seed=2024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22573ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(PRECOMPUTE_DIR, 'train', 'index.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3c39e",
   "metadata": {},
   "source": [
    "## è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b423a0",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—ï¼‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f14a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer with special handling for MoE parameters\n",
    "\n",
    "moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "\n",
    "if len(moe_params) > 0:\n",
    "    print('Reducing learning rate of MoE parameters by 1/2')\n",
    "    opt_dict = dict(cfg.optimizer)\n",
    "    opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "    del opt_dict['_target_']\n",
    "    optimizer = getattr(torch.optim, opt_name)(\n",
    "        params=[{'params': rest_params}, {'params': moe_params, 'lr': cfg.optimizer.lr / 2}], **opt_dict)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.dit.parameters(),\n",
    "        lr=2.4e-4,\n",
    "        weight_decay=0.1,\n",
    "        eps=1.0e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "for p in optimizer.param_groups:\n",
    "    p['betas'] = list(p['betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27062f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_seq_size, cap_emb_dim = text_encoder_embedding_format(\n",
    "    'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n",
    ")\n",
    "logger.info(f\"{cap_seq_size=}, {cap_emb_dim=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingLatentsDataset(StreamingDataset):\n",
    "    \"\"\"Dataset class for loading precomputed latents from mds format.\n",
    "    \n",
    "    Args:\n",
    "        streams: List of individual streams (in our case streams of individual datasets)\n",
    "        shuffle: Whether to shuffle the dataset\n",
    "        image_size: Size of images (256 or 512)\n",
    "        cap_seq_size: Context length of text-encoder\n",
    "        cap_emb_dim: Dimension of caption embeddings\n",
    "        cap_drop_prob: Probability of using all zeros caption embedding (classifier-free guidance)\n",
    "        batch_size: Batch size for streaming\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        streams: Optional[List[Stream]] = None,\n",
    "        shuffle: bool = False,\n",
    "        image_size: Optional[int] = None,\n",
    "        cap_seq_size: Optional[int] = None,\n",
    "        cap_emb_dim: Optional[int] = None,\n",
    "        cap_drop_prob: float = 0.0,\n",
    "        batch_size: Optional[int] = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            streams=streams,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.cap_seq_size = cap_seq_size\n",
    "        self.cap_emb_dim = cap_emb_dim\n",
    "        self.cap_drop_prob = cap_drop_prob\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Union[torch.Tensor, str, float]]:\n",
    "        sample = super().__getitem__(index)\n",
    "        out = {}\n",
    "\n",
    "        # Mask for zero'ed out captions in classifier-free guidance (cfg) training.\n",
    "        # We replace caption embeddings with a zero vector in cfg guidance.\n",
    "        out['drop_caption_mask'] = (\n",
    "            0. if torch.rand(1) < self.cap_drop_prob else 1.\n",
    "        )\n",
    "        out['caption_latents'] = torch.from_numpy(\n",
    "            np.frombuffer(sample['caption_latents'], dtype=np.float16)\n",
    "            .copy()\n",
    "        ).reshape(1, self.cap_seq_size, self.cap_emb_dim)\n",
    "\n",
    "        if self.image_size == 256 and 'latents_256' in sample:\n",
    "            out['image_latents'] = torch.from_numpy(\n",
    "                np.frombuffer(sample['latents_256'], dtype=np.float16)\n",
    "                .copy()\n",
    "            ).reshape(-1, 32, 32)\n",
    "\n",
    "        if self.image_size == 512 and 'latents_512' in sample:\n",
    "            out['image_latents'] = torch.from_numpy(\n",
    "                np.frombuffer(sample['latents_512'], dtype=np.float16)\n",
    "                .copy()\n",
    "            ).reshape(-1, 64, 64)\n",
    "\n",
    "        # out['caption'] = sample['caption']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_streaming_latents_dataloader(\n",
    "    datadir: Union[str, List[str]],\n",
    "    batch_size: int,\n",
    "    image_size: int = 256,\n",
    "    cap_seq_size: int = 77,\n",
    "    cap_emb_dim: int = 1024,\n",
    "    cap_drop_prob: float = 0.0,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = True,\n",
    "    **dataloader_kwargs\n",
    ") -> DataLoader:\n",
    "    \"\"\"Creates a DataLoader for streaming latents dataset.\"\"\"\n",
    "    if isinstance(datadir, str):\n",
    "        datadir = [datadir]\n",
    "\n",
    "    streams = [Stream(remote=None, local=d) for d in datadir]\n",
    "\n",
    "    dataset = StreamingLatentsDataset(\n",
    "        streams=streams,\n",
    "        shuffle=shuffle,\n",
    "        image_size=image_size,\n",
    "        cap_seq_size=cap_seq_size,\n",
    "        cap_emb_dim=cap_emb_dim,\n",
    "        cap_drop_prob=cap_drop_prob,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=None,\n",
    "        drop_last=drop_last,\n",
    "        **dataloader_kwargs,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84934954",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_streaming_latents_dataloader(\n",
    "    datadir=['/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/'],\n",
    "    image_size=256,\n",
    "    batch_size=128 // dist.get_world_size(), # 2048\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    cap_drop_prob=0.1,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "num_train_dataset_images = len(train_loader.dataset) * dist.get_world_size()\n",
    "logger.info(f\"{num_train_dataset_images=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db877f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = build_streaming_latents_dataloader(\n",
    "    datadir='/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/',\n",
    "    image_size=256,\n",
    "    batch_size=1024 // dist.get_world_size(),\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "num_eval_dataset_images = len(eval_loader.dataset) * dist.get_world_size()\n",
    "logger.info(f\"{num_eval_dataset_images=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725deb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "loggers.append(TensorboardLogger())\n",
    "\n",
    "exp_name = 'MicroDiTXL_mask_75_res_256_pretrain'\n",
    "wandb_logger = WandBLogger(\n",
    "    init_kwargs={\n",
    "        'name': f'{exp_name}',\n",
    "        'project': 'microdit_training',  # insert wandb project name\n",
    "        'group': f'{exp_name}'\n",
    "    }\n",
    ")\n",
    "# loggers.append(wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_low_precision_layernorm(\n",
    "    model=model.dit,\n",
    "    precision=Precision('amp_bf16'),\n",
    "    optimizers=optimizer\n",
    ")\n",
    "\n",
    "algorithms = []\n",
    "algorithms.append(\n",
    "    GradientClipping(clipping_type='norm', clipping_threshold=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca63d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(SpeedMonitor(window_size=3))\n",
    "callbacks.append(LRMonitor())\n",
    "callbacks.append(RuntimeEstimator())\n",
    "callbacks.append(OptimizerMonitor())\n",
    "callbacks.append(\n",
    "    LogDiffusionImages(\n",
    "        prompts=[\n",
    "            \"a photograph of an astronaut riding a horse\",\n",
    "            \"An astronaut riding a pig, highly realistic dslr photo, cinematic shot\",\n",
    "            \"Panda mad scientist mixing sparkling chemicals, artstation\",\n",
    "            \"a close-up of a fire spitting dragon, cinematic shot.\",\n",
    "            \"A small cactus with a happy face in the Sahara desert\",\n",
    "            \"Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\",\n",
    "            \"A dog that has been meditating all the time\",\n",
    "            \"A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style\",\n",
    "            \"A worker that looks like a mixture of cow and horse is working hard to type code\",\n",
    "            \"A capybara made of lego sitting in a realistic, natural field\",\n",
    "            \"A grand piano with a white bench.\",\n",
    "            \"In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and clothing texture.\",\n",
    "            \"A illustration from a graphic novel. A bustling city street under the shine of a full moon. The sidewalks bustling with pedestrians enjoying the nightlife. At the corner stall, a young woman with fiery red hair, dressed in a signature velvet cloak, is haggling with the grumpy old vendor. the grumpy vendor, a tall, sophisticated man is wearing a sharp suit, sports a noteworthy moustache is animatedly conversing on his steampunk telephone.\",\n",
    "            \"A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.\",\n",
    "            \"A giant cobra snake made from corn\",\n",
    "            \"A green sign that says 'Very Deep Learning' and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\"\n",
    "        ],\n",
    "        guidance_scale=5,\n",
    "        sampling_steps=30,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "callbacks.append(NaNCatcher())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = CosineAnnealingWithWarmupScheduler(\n",
    "    t_warmup='2500ba',\n",
    "    alpha_f=0.33 # decay to 0.8e-4 after 256x256 masked pre-training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887aca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.compileã«ã‚ˆã‚Š15%ç¨‹åº¦é«˜é€ŸåŒ–\n",
    "compile = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    loggers=loggers,\n",
    "    algorithms=algorithms,\n",
    "    callbacks=callbacks,\n",
    "    device='gpu',\n",
    "    # max_duration='250000ba',\n",
    "    max_duration='1ba',\n",
    "    eval_interval='2500ba' if compile else 0,\n",
    "    save_interval='2500ba',\n",
    "    save_num_checkpoints_to_keep=1,\n",
    "    device_train_microbatch_size=256,\n",
    "    run_name='microdit_experiment',\n",
    "    seed=42,\n",
    "    save_folder='./trained_models/microdit_experiment/',\n",
    "    save_overwrite=True,\n",
    "    autoresume=False,\n",
    "    parallelism_config=None,\n",
    "    precision='amp_bf16',  # if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "    python_log_level='debug',\n",
    "    compile_config={} if compile else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Ensure models are on correct device\n",
    "device = next(model.dit.parameters()).device\n",
    "model.vae.to(device)\n",
    "model.text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a89870",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.WARNING)  # to reduce streaming dataset info logs\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11922547",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—ï¼’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb97edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer with special handling for MoE parameters\n",
    "moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "\n",
    "if len(moe_params) > 0:\n",
    "    print('Reducing learning rate of MoE parameters by 1/2')\n",
    "    opt_dict = dict(cfg.optimizer)\n",
    "    opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "    del opt_dict['_target_']\n",
    "    optimizer = getattr(torch.optim, opt_name)(\n",
    "        params=[\n",
    "            {'params': rest_params},\n",
    "            {'params': moe_params, 'lr': 8e-5 / 2}], **opt_dict)\n",
    "else:\n",
    "    # optimizer = hydra.utils.instantiate(cfg.optimizer, params=model.dit.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.dit.parameters(),\n",
    "        lr=8e-5,\n",
    "        weight_decay=0.1,\n",
    "        eps=1.0e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "for p in optimizer.param_groups:\n",
    "    p['betas'] = list(p['betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acfd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "\n",
    "cap_seq_size, cap_emb_dim = text_encoder_embedding_format(\n",
    "    'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n",
    ")\n",
    "\n",
    "train_loader = build_streaming_latents_dataloader(\n",
    "    datadir=[\n",
    "        '/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/'\n",
    "    ],\n",
    "    image_size=256,\n",
    "    batch_size=128 // dist.get_world_size(), # 2048\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    cap_drop_prob=0.1,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_loader = build_streaming_latents_dataloader(\n",
    "    datadir='/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/',\n",
    "    image_size=256,\n",
    "    batch_size=1024 // dist.get_world_size(),\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d83dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "loggers.append(TensorboardLogger())\n",
    "\n",
    "exp_name = 'MicroDiTXL_mask_0_res_256_finetune'\n",
    "wandb_logger = WandBLogger(\n",
    "    init_kwargs={\n",
    "        'name': f'{exp_name}',\n",
    "        'project': 'microdit_training',  # insert wandb project name\n",
    "        'group': f'{exp_name}'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e88c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = []\n",
    "\n",
    "apply_low_precision_layernorm(\n",
    "    model=model.dit,\n",
    "    precision=Precision('amp_bf16'),\n",
    "    optimizers=optimizer\n",
    ")\n",
    "\n",
    "algorithms.append(\n",
    "    GradientClipping(clipping_type='norm', clipping_threshold=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a509a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(SpeedMonitor(window_size=3))\n",
    "callbacks.append(LRMonitor())\n",
    "callbacks.append(RuntimeEstimator())\n",
    "callbacks.append(OptimizerMonitor())\n",
    "callbacks.append(\n",
    "    LogDiffusionImages(\n",
    "        prompts=[\n",
    "            \"a photograph of an astronaut riding a horse\",\n",
    "            \"An astronaut riding a pig, highly realistic dslr photo, cinematic shot\",\n",
    "            \"Panda mad scientist mixing sparkling chemicals, artstation\",\n",
    "            \"a close-up of a fire spitting dragon, cinematic shot.\",\n",
    "            \"A small cactus with a happy face in the Sahara desert\",\n",
    "            \"Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\",\n",
    "            \"A dog that has been meditating all the time\",\n",
    "            \"A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style\",\n",
    "            \"A worker that looks like a mixture of cow and horse is working hard to type code\",\n",
    "            \"A capybara made of lego sitting in a realistic, natural field\",\n",
    "            \"A grand piano with a white bench.\",\n",
    "            \"In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and clothing texture.\",\n",
    "            \"A illustration from a graphic novel. A bustling city street under the shine of a full moon. The sidewalks bustling with pedestrians enjoying the nightlife. At the corner stall, a young woman with fiery red hair, dressed in a signature velvet cloak, is haggling with the grumpy old vendor. the grumpy vendor, a tall, sophisticated man is wearing a sharp suit, sports a noteworthy moustache is animatedly conversing on his steampunk telephone.\",\n",
    "            \"A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.\",\n",
    "            \"A giant cobra snake made from corn\",\n",
    "            \"A green sign that says 'Very Deep Learning' and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\"\n",
    "        ],\n",
    "        guidance_scale=5,\n",
    "        sampling_steps=30,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "callbacks.append(NaNCatcher())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33979c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ConstantScheduler(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147483ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile = False\n",
    "\n",
    "fsdp_config = {\n",
    "    'sharding_strategy': 'SHARD_GRAD_OP',\n",
    "}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    loggers=loggers,\n",
    "    algorithms=algorithms,\n",
    "    callbacks=callbacks,\n",
    "    device='gpu',\n",
    "    # max_duration='280000ba',\n",
    "    max_duration='1ba',\n",
    "    eval_interval='2500ba',\n",
    "    save_interval='2500ba',\n",
    "    save_num_checkpoints_to_keep=1,\n",
    "    device_train_microbatch_size=64,\n",
    "    run_name='microdit_experiment',\n",
    "    seed=18,\n",
    "    save_folder='./trained_models/microdit_experiment/',\n",
    "    # load_path='./trained_models/MicroDiTXL_mask_75_res_256_pretrain/latest-rank0.pt',\n",
    "    # load_ignore_keys=[\n",
    "    #     \"state/optimizers/AdamW/param_groups/initial_lr\",\n",
    "    #     \"state/optimizers/AdamW/param_groups/lr\",\n",
    "    #     \"state/schedulers/LambdaLR/base_lrs\",\n",
    "    #     \"state/schedulers/LambdaLR/_last_lr\"\n",
    "    # ],\n",
    "    save_overwrite=True,\n",
    "    autoresume=False,\n",
    "    parallelism_config=None,\n",
    "    precision='amp_bf16',  # if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "    python_log_level='debug',\n",
    "    compile_config={} if compile else None,\n",
    "    # fsdp_config=fsdp_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.dit.parameters()).device\n",
    "model.vae.to(device)\n",
    "model.text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070a0ac",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—ï¼“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer with special handling for MoE parameters\n",
    "\n",
    "moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "\n",
    "if len(moe_params) > 0:\n",
    "    print('Reducing learning rate of MoE parameters by 1/2')\n",
    "    opt_dict = dict(cfg.optimizer)\n",
    "    opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "    del opt_dict['_target_']\n",
    "    optimizer = getattr(torch.optim, opt_name)(\n",
    "        params=[\n",
    "            {'params': rest_params},\n",
    "            {'params': moe_params, 'lr': 8e-5 / 2}], **opt_dict)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.dit.parameters(),\n",
    "        lr=8e-5,\n",
    "        weight_decay=0.1,\n",
    "        eps=1.0e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d56753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "for p in optimizer.param_groups:\n",
    "    p['betas'] = list(p['betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "# cap_seq_size, cap_emb_dim = text_encoder_embedding_format(cfg.model.text_encoder_name)\n",
    "cap_seq_size, cap_emb_dim = text_encoder_embedding_format(\n",
    "    'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n",
    ")\n",
    "\n",
    "train_loader = build_streaming_latents_dataloader(\n",
    "    datadir=[\n",
    "        '/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/'\n",
    "    ],\n",
    "    image_size=256,\n",
    "    batch_size=128 // dist.get_world_size(), # 2048\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    cap_drop_prob=0.0,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# print(f\"Found {len(train_loader.dataset)*dist.get_world_size()} images in the training dataset\")\n",
    "# time.sleep(3)\n",
    "\n",
    "eval_loader = build_streaming_latents_dataloader(\n",
    "    datadir='/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/',\n",
    "    image_size=256,\n",
    "    batch_size=1024 // dist.get_world_size(),\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473782e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "loggers.append(TensorboardLogger())\n",
    "exp_name = 'MicroDiTXL_mask_0_res_256_finetune_final'\n",
    "wandb_logger = WandBLogger(\n",
    "    init_kwargs={\n",
    "        'name': f'{exp_name}',\n",
    "        'project': 'microdit_training',  # insert wandb project name\n",
    "        'group': f'{exp_name}'\n",
    "    }\n",
    ")\n",
    "# loggers.append(wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = []\n",
    "apply_low_precision_layernorm(\n",
    "    model=model.dit,\n",
    "    precision=Precision('amp_bf16'),\n",
    "    optimizers=optimizer\n",
    ")\n",
    "algorithms.append(\n",
    "    GradientClipping(clipping_type='norm', clipping_threshold=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca32d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(SpeedMonitor(window_size=3))\n",
    "callbacks.append(LRMonitor())\n",
    "callbacks.append(RuntimeEstimator())\n",
    "callbacks.append(OptimizerMonitor())\n",
    "callbacks.append(\n",
    "    LogDiffusionImages(\n",
    "        prompts=[\n",
    "            \"a photograph of an astronaut riding a horse\",\n",
    "            \"An astronaut riding a pig, highly realistic dslr photo, cinematic shot\",\n",
    "            \"Panda mad scientist mixing sparkling chemicals, artstation\",\n",
    "            \"a close-up of a fire spitting dragon, cinematic shot.\",\n",
    "            \"A small cactus with a happy face in the Sahara desert\",\n",
    "            \"Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\",\n",
    "            \"A dog that has been meditating all the time\",\n",
    "            \"A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style\",\n",
    "            \"A worker that looks like a mixture of cow and horse is working hard to type code\",\n",
    "            \"A capybara made of lego sitting in a realistic, natural field\",\n",
    "            \"A grand piano with a white bench.\",\n",
    "            \"In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and clothing texture.\",\n",
    "            \"A illustration from a graphic novel. A bustling city street under the shine of a full moon. The sidewalks bustling with pedestrians enjoying the nightlife. At the corner stall, a young woman with fiery red hair, dressed in a signature velvet cloak, is haggling with the grumpy old vendor. the grumpy vendor, a tall, sophisticated man is wearing a sharp suit, sports a noteworthy moustache is animatedly conversing on his steampunk telephone.\",\n",
    "            \"A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.\",\n",
    "            \"A giant cobra snake made from corn\",\n",
    "            \"A green sign that says 'Very Deep Learning' and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\"\n",
    "        ],\n",
    "        guidance_scale=5,\n",
    "        sampling_steps=30,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "callbacks.append(NaNCatcher())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5738a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ConstantScheduler(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    loggers=loggers,\n",
    "    algorithms=algorithms,\n",
    "    callbacks=callbacks,\n",
    "    device='gpu',\n",
    "    # max_duration='50000ba',\n",
    "    max_duration='1ba',\n",
    "    eval_interval='2500ba',\n",
    "    save_interval='2500ba',\n",
    "    save_num_checkpoints_to_keep=1,\n",
    "    device_train_microbatch_size=64,\n",
    "    run_name='microdit_experiment',\n",
    "    seed=18,\n",
    "    save_folder='./trained_models/microdit_experiment/',\n",
    "    load_path='./trained_models/MicroDiTXL_mask_0_res_256_finetune/latest-rank0.pt',\n",
    "    load_weights_only=True,\n",
    "    load_strict_model_weights=False,\n",
    "    load_ignore_keys=[\"state/model/dit.pos_embed\"],\n",
    "    save_overwrite=True,\n",
    "    autoresume=False,\n",
    "    parallelism_config=None,\n",
    "    precision='amp_bf16',  # if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "    python_log_level='debug',\n",
    "    compile_config={} if compile else None,\n",
    "    # fsdp_config=fsdp_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.dit.parameters()).device\n",
    "model.vae.to(device)\n",
    "model.text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0007779",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8565a4e",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—ï¼”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8931ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer with special handling for MoE parameters\n",
    "moe_params = [p[1] for p in model.dit.named_parameters() if 'moe' in p[0].lower()]\n",
    "rest_params = [p[1] for p in model.dit.named_parameters() if 'moe' not in p[0].lower()]\n",
    "if len(moe_params) > 0:\n",
    "    print('Reducing learning rate of MoE parameters by 1/2')\n",
    "    opt_dict = dict(cfg.optimizer)\n",
    "    opt_name = opt_dict['_target_'].split('.')[-1]\n",
    "    del opt_dict['_target_']\n",
    "    optimizer = getattr(torch.optim, opt_name)(\n",
    "        params=[{'params': rest_params}, {'params': moe_params, 'lr': cfg.optimizer.lr / 2}], **opt_dict)\n",
    "else:\n",
    "    # optimizer = hydra.utils.instantiate(cfg.optimizer, params=model.dit.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.dit.parameters(),\n",
    "        lr=8e-5,\n",
    "        weight_decay=0.1,\n",
    "        eps=1.0e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "# Convert ListConfig betas to native list to avoid ValueError when saving optimizer state\n",
    "for p in optimizer.param_groups:\n",
    "    p['betas'] = list(p['betas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_seq_size, cap_emb_dim = text_encoder_embedding_format(\n",
    "    'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378'\n",
    ")\n",
    "\n",
    "train_loader = build_streaming_latents_dataloader(\n",
    "    datadir=[\n",
    "        '/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/'\n",
    "    ],\n",
    "    image_size=512,\n",
    "    batch_size=128 // dist.get_world_size(), # 2048\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    cap_drop_prob=0.0,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_loader = build_streaming_latents_dataloader(\n",
    "    datadir='/root/.cache/micro_diffusion/data/mds_latents_sdxl1_dfnclipH14/train/',\n",
    "    image_size=512,\n",
    "    batch_size=1024 // dist.get_world_size(),\n",
    "    cap_seq_size=cap_seq_size,\n",
    "    cap_emb_dim=cap_emb_dim,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "\n",
    "loggers.append(TensorboardLogger())\n",
    "exp_name = 'MicroDiTXL_mask_0_res_512_finetune_final'\n",
    "wandb_logger = WandBLogger(\n",
    "    init_kwargs={\n",
    "        'name': f'{exp_name}',\n",
    "        'project': 'microdit_training',  # insert wandb project name\n",
    "        'group': f'{exp_name}'\n",
    "    }\n",
    ")\n",
    "# loggers.append(wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61067727",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = []\n",
    "apply_low_precision_layernorm(\n",
    "    model=model.dit,\n",
    "    precision=Precision('amp_bf16'),\n",
    "    optimizers=optimizer\n",
    ")\n",
    "algorithms.append(\n",
    "    GradientClipping(clipping_type='norm', clipping_threshold=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae206a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(SpeedMonitor(window_size=3))\n",
    "callbacks.append(LRMonitor())\n",
    "callbacks.append(RuntimeEstimator())\n",
    "callbacks.append(OptimizerMonitor())\n",
    "callbacks.append(\n",
    "    LogDiffusionImages(\n",
    "        prompts=[\n",
    "            \"a photograph of an astronaut riding a horse\",\n",
    "            \"An astronaut riding a pig, highly realistic dslr photo, cinematic shot\",\n",
    "            \"Panda mad scientist mixing sparkling chemicals, artstation\",\n",
    "            \"a close-up of a fire spitting dragon, cinematic shot.\",\n",
    "            \"A small cactus with a happy face in the Sahara desert\",\n",
    "            \"Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\",\n",
    "            \"A dog that has been meditating all the time\",\n",
    "            \"A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style\",\n",
    "            \"A worker that looks like a mixture of cow and horse is working hard to type code\",\n",
    "            \"A capybara made of lego sitting in a realistic, natural field\",\n",
    "            \"A grand piano with a white bench.\",\n",
    "            \"In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and clothing texture.\",\n",
    "            \"A illustration from a graphic novel. A bustling city street under the shine of a full moon. The sidewalks bustling with pedestrians enjoying the nightlife. At the corner stall, a young woman with fiery red hair, dressed in a signature velvet cloak, is haggling with the grumpy old vendor. the grumpy vendor, a tall, sophisticated man is wearing a sharp suit, sports a noteworthy moustache is animatedly conversing on his steampunk telephone.\",\n",
    "            \"A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.\",\n",
    "            \"A giant cobra snake made from corn\",\n",
    "            \"A green sign that says 'Very Deep Learning' and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\"\n",
    "        ],\n",
    "        guidance_scale=5,\n",
    "        sampling_steps=30,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "callbacks.append(NaNCatcher())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ConstantScheduler(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    loggers=loggers,\n",
    "    algorithms=algorithms,\n",
    "    callbacks=callbacks,\n",
    "    device='gpu',\n",
    "    # max_duration='55000ba',\n",
    "    max_duration='1ba',\n",
    "    eval_interval='500ba',\n",
    "    save_interval='500ba',\n",
    "    save_num_checkpoints_to_keep=1,\n",
    "    device_train_microbatch_size=64,\n",
    "    run_name='microdit_experiment',\n",
    "    seed=18,\n",
    "    save_folder='./trained_models/microdit_experiment/',\n",
    "    load_path='./trained_models/MicroDiTXL_mask_0_res_256_finetune/latest-rank0.pt',\n",
    "    load_weights_only=True,\n",
    "    load_strict_model_weights=False,\n",
    "    load_ignore_keys=[\"state/optimizers/AdamW/param_groups/initial_lr\", \"state/optimizers/AdamW/param_groups/lr\", \"state/schedulers/LambdaLR/base_lrs\", \"state/schedulers/LambdaLR/_last_lr\"],\n",
    "    save_overwrite=True,\n",
    "    autoresume=False,\n",
    "    parallelism_config=None,\n",
    "    precision='amp_bf16',  # if cfg.model['dtype'] == 'bfloat16' else 'amp_fp16',  # fp16 by default\n",
    "    python_log_level='debug',\n",
    "    compile_config={} if compile else None,\n",
    "    # fsdp_config=fsdp_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.dit.parameters()).device\n",
    "model.vae.to(device)\n",
    "model.text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c41188",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
